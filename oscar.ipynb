{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "oscar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQteP3oFzFRh6KsA2604nh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IKACE/OscarPlus/blob/main/oscar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEDqnYIMuAQp"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsewaoP1pLkG"
      },
      "source": [
        "Make sure that modules and datasets are properly mounted from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_8Y6CLSaFq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "722e7c00-43f7-48b5-ac4c-33ed181dd38d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMiNxOBauO6G",
        "outputId": "f45a8930-03a3-4358-d4cf-223e13c80b7b"
      },
      "source": [
        "!ls drive/MyDrive/OscarPlus/coco_caption"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotations\t\t__init__.py  pycocoevalcap  results\n",
            "cocoEvalCapDemo.ipynb\tlicense.txt  pycocotools\n",
            "get_stanford_models.sh\t__pycache__  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSi-WnNx5nfR"
      },
      "source": [
        "Install python requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_od9neO5p-4",
        "outputId": "092a0a33-bace-4ba9-9020-ffaec0cc194f"
      },
      "source": [
        "!pip install -r /content/drive/MyDrive/OscarPlus/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 1)) (4.62.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 2)) (3.13)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (0.18.3)\n",
            "Collecting anytree\n",
            "  Downloading anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▉                        | 10 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 20 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 30 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 40 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 340 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 7)) (2019.12.20)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.8-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (2.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (2.6.3)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting botocore<1.24.0,>=1.23.8\n",
            "  Downloading botocore-1.23.8-py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 42.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 48.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, jmespath, botocore, s3transfer, boto3, anytree\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed anytree-2.8.0 boto3-1.20.8 botocore-1.23.8 jmespath-0.10.0 s3transfer-0.5.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-UehtJCpXVw"
      },
      "source": [
        "Add drive directory to system path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49Ryure3unWA"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/OscarPlus\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miKiUiLn1DrN"
      },
      "source": [
        "Make sure submodules can be imported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKF7ggoyu1aX"
      },
      "source": [
        "from coco_caption.pycocotools.coco import COCO"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EitKYqG4yHih"
      },
      "source": [
        "# Image Caption"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "uONXz2DAyNNa",
        "outputId": "f8fa827b-8075-4e6f-ce2f-5440598afd89"
      },
      "source": [
        "# Copyright (c) 2021 Microsoft Corporation. Licensed under the MIT license.\n",
        "\n",
        "import argparse\n",
        "import base64\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path as op\n",
        "import random, time, json\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "from oscar.utils.logger import setup_logger\n",
        "from oscar.utils.tsv_file import TSVFile\n",
        "from oscar.utils.tsv_file_ops import (tsv_writer, concat_tsv_files,\n",
        "        delete_tsv_files, reorder_tsv_keys)\n",
        "from oscar.utils.misc import (mkdir, set_seed, \n",
        "        load_from_yaml_file, find_file_path_in_yaml)\n",
        "from oscar.utils.caption_evaluate import (evaluate_on_coco_caption,\n",
        "        ScstRewardCriterion)\n",
        "from oscar.utils.cbs import ConstraintFilter, ConstraintBoxesReader\n",
        "from oscar.utils.cbs import FiniteStateMachineBuilder\n",
        "from oscar.modeling.modeling_bert import BertForImageCaptioning\n",
        "from transformers.pytorch_transformers import BertTokenizer, BertConfig\n",
        "from transformers.pytorch_transformers import AdamW, WarmupLinearSchedule, WarmupConstantSchedule\n",
        "\n",
        "\n",
        "class CaptionTSVDataset(Dataset):\n",
        "    def __init__(self, yaml_file, tokenizer=None, add_od_labels=True,\n",
        "            max_img_seq_length=50, max_seq_length=70, max_seq_a_length=40, \n",
        "            is_train=True, mask_prob=0.15, max_masked_tokens=3, overfit=False, **kwargs):\n",
        "        \"\"\"Constructor.\n",
        "        Args:\n",
        "            yaml file with all required data (image feature, caption, labels, etc)\n",
        "            tokenizer: tokenizer for text processing.\n",
        "            add_od_labels: whether to add labels from yaml file to BERT. \n",
        "            max_img_seq_length: max image sequence length.\n",
        "            max_seq_length: max text sequence length.\n",
        "            max_seq_a_length: max caption sequence length.\n",
        "            is_train: train or test mode.\n",
        "            mask_prob: probability to mask a input token.\n",
        "            max_masked_tokens: maximum number of tokens to be masked in one sentence.\n",
        "            overfit: provide a small dataset to test pipeline\n",
        "            kwargs: other arguments.\n",
        "        \"\"\"\n",
        "        self.yaml_file = yaml_file\n",
        "        self.cfg = load_from_yaml_file(yaml_file)\n",
        "        self.root = op.dirname(yaml_file)\n",
        "        self.label_file = find_file_path_in_yaml(self.cfg['label'], self.root)\n",
        "        self.feat_file = find_file_path_in_yaml(self.cfg['feature'], self.root)\n",
        "        self.caption_file = find_file_path_in_yaml(self.cfg.get('caption'), self.root)\n",
        "\n",
        "        self.overfit = overfit\n",
        "\n",
        "        assert op.isfile(self.feat_file)\n",
        "        if add_od_labels: assert op.isfile(self.label_file)\n",
        "        if is_train: assert op.isfile(self.caption_file) and tokenizer is not None\n",
        "\n",
        "        self.label_tsv = None if not self.label_file else TSVFile(self.label_file)\n",
        "        self.feat_tsv = TSVFile(self.feat_file)\n",
        "        \n",
        "        self.captions = []\n",
        "        if self.caption_file and op.isfile(self.caption_file):\n",
        "            if self.overfit == False:\n",
        "                with open(self.caption_file, 'r') as f:\n",
        "                    self.captions = json.load(f)\n",
        "            else:\n",
        "                with open(\"/content/drive/MyDrive/OscarPlus/datasets/coco_caption/test_caption_overfit.json\", 'r') as f:\n",
        "                    self.captions = json.load(f)                \n",
        "        # if self.overfit == True:\n",
        "        #     self.captions = self.captions[0:100]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tensorizer = CaptionTensorizer(self.tokenizer, max_img_seq_length,\n",
        "                max_seq_length, max_seq_a_length, mask_prob, max_masked_tokens,\n",
        "                is_train=is_train)\n",
        "        self.add_od_labels = add_od_labels\n",
        "        self.is_train = is_train\n",
        "        self.kwargs = kwargs\n",
        "        self.image_keys = self.prepare_image_keys()\n",
        "        self.key2index = self.prepare_image_key_to_index()\n",
        "        self.key2captions = self.prepare_image_key_to_captions()\n",
        "\n",
        "    def get_valid_tsv(self):\n",
        "        # based on the order of file size\n",
        "        if self.label_tsv:\n",
        "            return self.label_tsv\n",
        "        if self.feat_tsv:\n",
        "            return self.feat_tsv\n",
        "\n",
        "    def prepare_image_keys(self):\n",
        "        tsv = self.get_valid_tsv()\n",
        "        if self.overfit == True:\n",
        "            try:\n",
        "                return [tsv.seek(i)[0] for i in range(100)]\n",
        "            except Exception as e:\n",
        "                print(\"Original dataset smaller than 100!\")\n",
        "                raise\n",
        "        else:\n",
        "            return [tsv.seek(i)[0] for i in range(tsv.num_rows())]\n",
        "\n",
        "    def prepare_image_key_to_index(self):\n",
        "        tsv = self.get_valid_tsv()\n",
        "        if self.overfit == True:\n",
        "            try:\n",
        "                return {tsv.seek(i)[0] : i for i in range(100)}\n",
        "            except Exception as e:\n",
        "                print(\"Original dataset smaller than 100!\")\n",
        "                raise\n",
        "        else:\n",
        "            return {tsv.seek(i)[0] : i for i in range(tsv.num_rows())}\n",
        "\n",
        "    def prepare_image_key_to_captions(self):\n",
        "        if self.captions:\n",
        "            key2captions = {key: [] for key in self.image_keys}\n",
        "            for cap in self.captions:\n",
        "                key2captions[cap['image_id']].append(cap['caption'])\n",
        "            # for key in self.image_keys:\n",
        "            #     key2captions[key].append(cap['caption'])\n",
        "            return key2captions\n",
        "\n",
        "    def get_image_index(self, idx):\n",
        "        if self.is_train:\n",
        "            img_cap_pair = self.captions[idx]\n",
        "            img_key = img_cap_pair['image_id']\n",
        "            return self.key2index[img_key]\n",
        "        return idx\n",
        "\n",
        "    def get_image_key(self, idx):\n",
        "        img_idx = self.get_image_index(idx)\n",
        "        return self.image_keys[img_idx]\n",
        "\n",
        "    def get_image_features(self, img_idx):\n",
        "        feat_info = json.loads(self.feat_tsv.seek(img_idx)[1])\n",
        "        num_boxes = feat_info['num_boxes']\n",
        "        features = np.frombuffer(base64.b64decode(feat_info['features']), np.float32\n",
        "                ).reshape((num_boxes, -1))\n",
        "        return torch.Tensor(features)\n",
        "\n",
        "    def get_caption(self, idx):\n",
        "        if self.is_train:\n",
        "            img_cap_pair = self.captions[idx]\n",
        "            return img_cap_pair['caption']\n",
        "        return \"\"\n",
        "\n",
        "    def get_od_labels(self, img_idx):\n",
        "        od_labels = None\n",
        "        if self.add_od_labels:\n",
        "            label_info = json.loads(self.label_tsv.seek(img_idx)[1])\n",
        "            od_labels = \" \".join([l['class'] for l in label_info])\n",
        "        return od_labels\n",
        "\n",
        "    def get_caption_file_in_coco_format(self):\n",
        "        cap_file = op.splitext(self.caption_file)[0] + '_coco_format.json'\n",
        "        return cap_file\n",
        "\n",
        "    def get_captions_by_key(self, key):\n",
        "        return self.key2captions[key]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_idx = self.get_image_index(idx)\n",
        "        img_key = self.image_keys[img_idx]\n",
        "        features = self.get_image_features(img_idx)\n",
        "        caption = self.get_caption(idx)\n",
        "        od_labels = self.get_od_labels(img_idx)\n",
        "        example = self.tensorizer.tensorize_example(caption, features, text_b=od_labels)\n",
        "        return img_key, example\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.is_train or self.overfit:\n",
        "            return len(self.captions)\n",
        "        return self.get_valid_tsv().num_rows()\n",
        "\n",
        "\n",
        "class CaptionTSVDatasetWithConstraints(CaptionTSVDataset):\n",
        "    r\"\"\"\n",
        "    Providing inputs for inference with Constraint Beam Search\n",
        "\n",
        "    nms_threshold: float, optional (default = 0.85)\n",
        "        NMS threshold for suppressing generic object class names during constraint filtering,\n",
        "        for two boxes with IoU higher than this threshold, \"dog\" suppresses \"animal\".\n",
        "    max_given_constraints: int, optional (default = 3)\n",
        "        Maximum number of constraints which can be specified for CBS decoding. Constraints are\n",
        "        selected based on the prediction confidence score of their corresponding bounding boxes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, yaml_file,\n",
        "        nms_threshold=0.85,\n",
        "        max_given_constraints=3, **kwargs\n",
        "    ):\n",
        "        super().__init__(yaml_file, **kwargs)\n",
        "        boxes_tsvpath = find_file_path_in_yaml(self.cfg['cbs_box'], self.root)\n",
        "        constraint2tokens_tsvpath = find_file_path_in_yaml(self.cfg['cbs_constraint'], self.root)\n",
        "        tokenforms_tsvpath = find_file_path_in_yaml(self.cfg['cbs_tokenforms'], self.root)\n",
        "        hierarchy_jsonpath = find_file_path_in_yaml(self.cfg['cbs_hierarchy'], self.root)\n",
        "\n",
        "        self._boxes_reader = ConstraintBoxesReader(boxes_tsvpath)\n",
        "        self._constraint_filter = ConstraintFilter(\n",
        "            hierarchy_jsonpath, nms_threshold, max_given_constraints\n",
        "        )\n",
        "        self._fsm_builder = FiniteStateMachineBuilder(self.tokenizer,\n",
        "                constraint2tokens_tsvpath, tokenforms_tsvpath,\n",
        "                max_given_constraints)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_key, example = super().__getitem__(index)\n",
        "\n",
        "        # Apply constraint filtering to object class names.\n",
        "        constraint_boxes = self._boxes_reader[img_key]\n",
        "\n",
        "        candidates = self._constraint_filter(\n",
        "            constraint_boxes[\"boxes\"], constraint_boxes[\"class_names\"], constraint_boxes[\"scores\"]\n",
        "        )\n",
        "        num_constraints = len(candidates)\n",
        "        fsm, nstates = self._fsm_builder.build(candidates)\n",
        "\n",
        "        return img_key, example + (fsm, num_constraints, )\n",
        "\n",
        "\n",
        "class CaptionTensorizer(object):\n",
        "    def __init__(self, tokenizer, max_img_seq_length=50, max_seq_length=70, \n",
        "            max_seq_a_length=40, mask_prob=0.15, max_masked_tokens=3,\n",
        "            is_train=True):\n",
        "        \"\"\"Constructor.\n",
        "        Args:\n",
        "            tokenizer: tokenizer for text processing.\n",
        "            max_img_seq_length: max image sequence length.\n",
        "            max_seq_length: max text sequence length.\n",
        "            max_seq_a_length: max caption sequence length.\n",
        "            is_train: train or test mode.\n",
        "            mask_prob: probability to mask a input token.\n",
        "            max_masked_tokens: maximum number of tokens to be masked in one sentence.\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.is_train = is_train\n",
        "        self.max_img_seq_len = max_img_seq_length\n",
        "        self.max_seq_len = max_seq_length\n",
        "        self.max_seq_a_len = max_seq_a_length\n",
        "        self.mask_prob = mask_prob\n",
        "        self.max_masked_tokens = max_masked_tokens\n",
        "        self._triangle_mask = torch.tril(torch.ones((self.max_seq_len, \n",
        "            self.max_seq_len), dtype=torch.long))\n",
        "\n",
        "    def tensorize_example(self, text_a, img_feat, text_b=None,\n",
        "            cls_token_segment_id=0, pad_token_segment_id=0,\n",
        "            sequence_a_segment_id=0, sequence_b_segment_id=1):\n",
        "        if self.is_train:\n",
        "            tokens_a = self.tokenizer.tokenize(text_a)\n",
        "        else:\n",
        "            # fake tokens to generate masks\n",
        "            tokens_a = [self.tokenizer.mask_token] * (self.max_seq_a_len - 2)\n",
        "        if len(tokens_a) > self.max_seq_a_len - 2:\n",
        "            tokens_a = tokens_a[:(self.max_seq_a_len - 2)]\n",
        "\n",
        "        tokens = [self.tokenizer.cls_token] + tokens_a + [self.tokenizer.sep_token]\n",
        "        segment_ids = [cls_token_segment_id] + [sequence_a_segment_id] * (len(tokens) - 1)\n",
        "        seq_a_len = len(tokens)\n",
        "        if text_b:\n",
        "            # pad text_a to keep it in fixed length for better inference.\n",
        "            padding_a_len = self.max_seq_a_len - seq_a_len\n",
        "            tokens += [self.tokenizer.pad_token] * padding_a_len\n",
        "            segment_ids += ([pad_token_segment_id] * padding_a_len)\n",
        "\n",
        "            tokens_b = self.tokenizer.tokenize(text_b)\n",
        "            if len(tokens_b) > self.max_seq_len - len(tokens) - 1:\n",
        "                tokens_b = tokens_b[: (self.max_seq_len - len(tokens) - 1)]\n",
        "            tokens += tokens_b + [self.tokenizer.sep_token]\n",
        "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "        seq_len = len(tokens)\n",
        "        if self.is_train:\n",
        "            masked_pos = torch.zeros(self.max_seq_len, dtype=torch.int)\n",
        "            # randomly mask words for prediction, ignore [CLS]\n",
        "            candidate_masked_idx = list(range(1, seq_a_len)) # only mask text_a\n",
        "            random.shuffle(candidate_masked_idx)\n",
        "            num_masked = min(max(round(self.mask_prob * seq_a_len), 1), self.max_masked_tokens)\n",
        "            num_masked = int(num_masked)\n",
        "            masked_idx = candidate_masked_idx[:num_masked]\n",
        "            masked_idx = sorted(masked_idx)\n",
        "            masked_token = [tokens[i] for i in masked_idx]\n",
        "            for pos in masked_idx:\n",
        "                if random.random() <= 0.8:\n",
        "                    # 80% chance to be a ['MASK'] token\n",
        "                    tokens[pos] = self.tokenizer.mask_token\n",
        "                elif random.random() <= 0.5:\n",
        "                    # 10% chance to be a random word ((1-0.8)*0.5)\n",
        "                    from random import randint\n",
        "                    i = randint(0, len(self.tokenizer.vocab))\n",
        "                    self.tokenizer._convert_id_to_token(i)\n",
        "                    tokens[pos] = self.tokenizer._convert_id_to_token(i)\n",
        "                else:\n",
        "                    # 10% chance to remain the same (1-0.8-0.1)\n",
        "                    pass\n",
        "\n",
        "            masked_pos[masked_idx] = 1 \n",
        "            # pad masked tokens to the same length\n",
        "            if num_masked < self.max_masked_tokens:\n",
        "                masked_token = masked_token + ([self.tokenizer.pad_token] *\n",
        "                        (self.max_masked_tokens - num_masked))\n",
        "            masked_ids = self.tokenizer.convert_tokens_to_ids(masked_token)\n",
        "        else:\n",
        "            masked_pos = torch.ones(self.max_seq_len, dtype=torch.int)\n",
        "\n",
        "        # pad on the right for image captioning\n",
        "        padding_len = self.max_seq_len - seq_len\n",
        "        tokens = tokens + ([self.tokenizer.pad_token] * padding_len)\n",
        "        segment_ids += ([pad_token_segment_id] * padding_len)\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # image features\n",
        "        img_len = img_feat.shape[0]\n",
        "        if img_len > self.max_img_seq_len:\n",
        "            img_feat = img_feat[0 : self.max_img_seq_len, ]\n",
        "            img_len = img_feat.shape[0]\n",
        "        else:\n",
        "            padding_matrix = torch.zeros((self.max_img_seq_len - img_len,\n",
        "                                          img_feat.shape[1]))\n",
        "            img_feat = torch.cat((img_feat, padding_matrix), 0)\n",
        "\n",
        "        # prepare attention mask:\n",
        "        # note that there is no attention from caption to image\n",
        "        # because otherwise it will violate the triangle attention \n",
        "        # for caption as caption will have full attention on image. \n",
        "        max_len = self.max_seq_len + self.max_img_seq_len\n",
        "        attention_mask = torch.zeros((max_len, max_len), dtype=torch.long)\n",
        "        # C: caption, L: label, R: image region\n",
        "        c_start, c_end = 0, seq_a_len\n",
        "        l_start, l_end = self.max_seq_a_len, seq_len\n",
        "        r_start, r_end = self.max_seq_len, self.max_seq_len + img_len\n",
        "        # triangle mask for caption to caption\n",
        "        attention_mask[c_start : c_end, c_start : c_end].copy_(self._triangle_mask[0 : seq_a_len, 0 : seq_a_len])\n",
        "        # full attention for L-L, R-R\n",
        "        attention_mask[l_start : l_end, l_start : l_end] = 1\n",
        "        attention_mask[r_start : r_end, r_start : r_end] = 1\n",
        "        # full attention for C-L, C-R\n",
        "        attention_mask[c_start : c_end, l_start : l_end] = 1\n",
        "        attention_mask[c_start : c_end, r_start : r_end] = 1\n",
        "        # full attention for L-R:\n",
        "        attention_mask[l_start : l_end, r_start : r_end] = 1\n",
        "        attention_mask[r_start : r_end, l_start : l_end] = 1\n",
        "\n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "        segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
        "\n",
        "        if self.is_train:\n",
        "            masked_ids = torch.tensor(masked_ids, dtype=torch.long)\n",
        "            return (input_ids, attention_mask, segment_ids, img_feat, masked_pos, masked_ids)\n",
        "        return (input_ids, attention_mask, segment_ids, img_feat, masked_pos)\n",
        "\n",
        "\n",
        "def build_dataset(yaml_file, tokenizer, args, is_train=True):\n",
        "    if not op.isfile(yaml_file):\n",
        "        yaml_file = op.join(args.data_dir, yaml_file)\n",
        "        assert op.isfile(yaml_file)\n",
        "\n",
        "    if is_train:\n",
        "        return CaptionTSVDataset(yaml_file, tokenizer=tokenizer,\n",
        "            add_od_labels=args.add_od_labels, max_img_seq_length=args.max_img_seq_length,\n",
        "            max_seq_length=args.max_seq_length, max_seq_a_length=args.max_seq_a_length,\n",
        "            is_train=True, mask_prob=args.mask_prob, max_masked_tokens=args.max_masked_tokens)\n",
        "    if args.use_cbs:\n",
        "        dataset_class = CaptionTSVDatasetWithConstraints\n",
        "    else:\n",
        "        dataset_class = CaptionTSVDataset\n",
        "    return dataset_class(yaml_file, tokenizer=tokenizer,\n",
        "            add_od_labels=args.add_od_labels, max_img_seq_length=args.max_img_seq_length,\n",
        "            max_seq_length=args.max_seq_length, max_seq_a_length=args.max_gen_length,\n",
        "            is_train=False, overfit=args.overfit)\n",
        "\n",
        "\n",
        "def make_data_sampler(dataset, shuffle, distributed):\n",
        "    if distributed:\n",
        "        return torch.utils.data.distributed.DistributedSampler(dataset, shuffle=shuffle)\n",
        "    if shuffle:\n",
        "        sampler = torch.utils.data.sampler.RandomSampler(dataset)\n",
        "    else:\n",
        "        sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n",
        "    return sampler\n",
        "\n",
        "\n",
        "def make_data_loader(args, yaml_file, tokenizer, is_distributed=True, \n",
        "        is_train=True):\n",
        "    dataset = build_dataset(yaml_file, tokenizer, args, \n",
        "        is_train=(is_train and not args.scst))\n",
        "    if is_train:\n",
        "        shuffle = True\n",
        "        images_per_gpu = args.per_gpu_train_batch_size\n",
        "        images_per_batch = images_per_gpu * get_world_size()\n",
        "        iters_per_batch = len(dataset) // images_per_batch\n",
        "        num_iters = iters_per_batch * args.num_train_epochs\n",
        "        logger.info(\"Train with {} images per GPU.\".format(images_per_gpu))\n",
        "        logger.info(\"Total batch size {}\".format(images_per_batch))\n",
        "        logger.info(\"Total training steps {}\".format(num_iters))\n",
        "    else:\n",
        "        shuffle = False\n",
        "        images_per_gpu = args.per_gpu_eval_batch_size\n",
        "\n",
        "    sampler = make_data_sampler(dataset, shuffle, is_distributed)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, num_workers=args.num_workers, sampler=sampler,\n",
        "        batch_size=images_per_gpu,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "def save_checkpoint(model, tokenizer, args, epoch, iteration, num_trial=10):\n",
        "    checkpoint_dir = op.join(args.output_dir, 'checkpoint-{}-{}'.format(\n",
        "        epoch, iteration))\n",
        "    if not is_main_process():\n",
        "        return checkpoint_dir\n",
        "    mkdir(checkpoint_dir)\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    for i in range(num_trial):\n",
        "        try:\n",
        "            model_to_save.save_pretrained(checkpoint_dir)\n",
        "            torch.save(args, op.join(checkpoint_dir, 'training_args.bin'))\n",
        "            tokenizer.save_pretrained(checkpoint_dir)\n",
        "            logger.info(\"Save checkpoint to {}\".format(checkpoint_dir))\n",
        "            break\n",
        "        except:\n",
        "            pass\n",
        "    else:\n",
        "        logger.info(\"Failed to save checkpoint after {} trails.\".format(num_trial))\n",
        "    return checkpoint_dir\n",
        "\n",
        "\n",
        "def compute_score_with_logits(logits, labels):\n",
        "    logits = torch.max(logits, -1)[1].data # argmax\n",
        "    scores = logits == labels \n",
        "    return scores\n",
        "\n",
        "\n",
        "def train(args, train_dataloader, val_dataset, model, tokenizer):\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], \n",
        "            output_device=args.local_rank,\n",
        "            find_unused_parameters=True,\n",
        "        )\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // \\\n",
        "                args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps \\\n",
        "                * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and scheduler\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not \\\n",
        "            any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if \\\n",
        "            any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    if args.scheduler == \"constant\":\n",
        "        scheduler = WarmupConstantSchedule(\n",
        "                optimizer, warmup_steps=args.warmup_steps)\n",
        "    elif args.scheduler == \"linear\":\n",
        "        scheduler = WarmupLinearSchedule(\n",
        "                optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown scheduler type: {}\".format(args.scheduler))\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\"  Total train batch size (w. parallel, & accumulation) = %d\",\n",
        "                   args.per_gpu_train_batch_size * get_world_size() * args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    if args.scst:\n",
        "        scst_criterion = ScstRewardCriterion(\n",
        "            cider_cached_tokens=op.join(args.data_dir, args.cider_cached_tokens),\n",
        "            baseline_type=args.sc_baseline_type,\n",
        "        )\n",
        "        logger.info(\"  SCST training...\")\n",
        "\n",
        "\n",
        "    global_step, global_loss, global_acc =0,  0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    eval_log = []\n",
        "    best_score = 0\n",
        "    for epoch in range(int(args.num_train_epochs)):\n",
        "        for step, (img_keys, batch) in enumerate(train_dataloader):\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            if not args.scst:\n",
        "                model.train()\n",
        "                inputs = {'input_ids': batch[0], 'attention_mask': batch[1],\n",
        "                    'token_type_ids': batch[2], 'img_feats': batch[3], \n",
        "                    'masked_pos': batch[4], 'masked_ids': batch[5]\n",
        "                }\n",
        "                outputs = model(**inputs)\n",
        "                loss, logits = outputs[:2]\n",
        "                masked_ids = inputs['masked_ids']\n",
        "                masked_ids = masked_ids[masked_ids != 0]\n",
        "                batch_score = compute_score_with_logits(logits, masked_ids)\n",
        "                batch_acc = torch.sum(batch_score.float()) / torch.sum(inputs['masked_pos'])\n",
        "            else:\n",
        "                loss = scst_train_iter(args, train_dataloader, model, scst_criterion, img_keys, batch, tokenizer)\n",
        "                batch_acc = scst_criterion.get_score()\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "            global_loss += loss.item()\n",
        "            global_acc += batch_acc\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                global_step += 1\n",
        "                scheduler.step()\n",
        "                optimizer.step()\n",
        "                model.zero_grad()\n",
        "                if global_step % args.logging_steps == 0:\n",
        "                    logger.info(\"Epoch: {}, global_step: {}, lr: {:.6f}, loss: {:.4f} ({:.4f}), \" \\\n",
        "                        \"score: {:.4f} ({:.4f})\".format(epoch, global_step, \n",
        "                        optimizer.param_groups[0][\"lr\"], loss, global_loss / global_step, \n",
        "                        batch_acc, global_acc / global_step)\n",
        "                    )\n",
        "\n",
        "                if (args.save_steps > 0 and global_step % args.save_steps == 0) or \\\n",
        "                        global_step == t_total:\n",
        "                    checkpoint_dir = save_checkpoint(model, tokenizer, args, epoch, global_step) \n",
        "                    # evaluation\n",
        "                    if args.evaluate_during_training: \n",
        "                        logger.info(\"Perform evaluation at step: %d\" % (global_step))\n",
        "                        evaluate_file = evaluate(args, val_dataset, model, tokenizer,\n",
        "                                checkpoint_dir)\n",
        "                        with open(evaluate_file, 'r') as f:\n",
        "                            res = json.load(f)\n",
        "                        best_score = max(best_score, res['CIDEr'])\n",
        "                        res['epoch'] = epoch\n",
        "                        res['global_step'] = step\n",
        "                        res['best_CIDEr'] = best_score\n",
        "                        eval_log.append(res)\n",
        "                        with open(args.output_dir + '/eval_logs.json', 'w') as f:\n",
        "                            json.dump(eval_log, f)\n",
        "    return checkpoint_dir\n",
        "\n",
        "\n",
        "def scst_train_iter(args, train_dataloader, model, scst_criterion, \n",
        "        img_keys, batch, tokenizer):\n",
        "    cls_token_id, sep_token_id, pad_token_id, mask_token_id = \\\n",
        "        tokenizer.convert_tokens_to_ids([tokenizer.cls_token, \n",
        "        tokenizer.sep_token, tokenizer.pad_token, tokenizer.mask_token]\n",
        "    )\n",
        "    inputs = {'is_decode': True,\n",
        "        'input_ids': batch[0], 'attention_mask': batch[1],\n",
        "        'token_type_ids': batch[2], 'img_feats': batch[3],\n",
        "        'masked_pos': batch[4],\n",
        "        'do_sample': False,\n",
        "        'bos_token_id': cls_token_id,\n",
        "        'pad_token_id': pad_token_id,\n",
        "        'eos_token_ids': [sep_token_id],\n",
        "        'mask_token_id': mask_token_id,\n",
        "        # for adding od labels\n",
        "        'add_od_labels': args.add_od_labels, 'od_labels_start_posid': args.max_seq_a_length,\n",
        "        # hyperparameters of beam search\n",
        "        'max_length': args.max_gen_length,\n",
        "        'num_beams': args.sc_beam_size,\n",
        "        \"temperature\": args.temperature,\n",
        "        \"top_k\": args.top_k,\n",
        "        \"top_p\": args.top_p,\n",
        "        \"repetition_penalty\": args.repetition_penalty,\n",
        "        \"length_penalty\": args.length_penalty,\n",
        "        \"num_return_sequences\": 1,\n",
        "        \"num_keep_best\": 1,\n",
        "    }\n",
        "\n",
        "    def _ids_to_captions(all_ids):\n",
        "        captions = []\n",
        "        for ids in all_ids:\n",
        "            c = tokenizer.decode(ids.tolist(), skip_special_tokens=True)\n",
        "            captions.append(c)\n",
        "        return captions\n",
        "\n",
        "    if args.sc_baseline_type == 'greedy':\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            greedy_res_raw, _ = model(**inputs)\n",
        "            greedy_res_raw.squeeze_(1)  # batch_size * max_len\n",
        "        greedy_res = _ids_to_captions(greedy_res_raw)\n",
        "    else:\n",
        "        greedy_res = None\n",
        "\n",
        "    model.train()\n",
        "    inputs['do_sample'] = True\n",
        "    inputs['num_return_sequences'] = args.sc_train_sample_n\n",
        "    sample_res_raw, sample_logprobs = model(**inputs)\n",
        "    sample_res_raw.squeeze_(1)\n",
        "    sample_logprobs.squeeze_(1)\n",
        "    assert sample_logprobs.requires_grad == True\n",
        "    assert sample_res_raw.requires_grad == False\n",
        "    sample_res = _ids_to_captions(sample_res_raw)\n",
        "\n",
        "    gt_res = [train_dataloader.dataset.get_captions_by_key(k) for k in img_keys]\n",
        "    loss = scst_criterion(gt_res, greedy_res, sample_res, sample_logprobs)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def  get_predict_file(output_dir, yaml_file, args):\n",
        "    cc = ['pred']\n",
        "    # make sure it works with/without / in end of the path.\n",
        "    data = op.basename(op.join(args.data_dir, '')[:-1])\n",
        "    split = op.basename(yaml_file)\n",
        "    assert split.endswith('.yaml')\n",
        "    split = split[:-5]\n",
        "    cc.append(data)\n",
        "    cc.append(split)\n",
        "    cc.append('beam{}'.format(args.num_beams))\n",
        "    cc.append('max{}'.format(args.max_gen_length))\n",
        "    if args.add_od_labels:\n",
        "        cc.append('odlabels')\n",
        "    if args.num_keep_best != 1:\n",
        "        cc.append('best{}'.format(args.num_keep_best))\n",
        "    if args.use_cbs:\n",
        "        cc.append('cbs{}'.format(args.min_constraints_to_satisfy))\n",
        "    if args.output_hidden_states:\n",
        "        cc.append('hidden')\n",
        "    return op.join(output_dir, '{}.tsv'.format('.'.join(cc)))\n",
        "\n",
        "\n",
        "def get_evaluate_file(predict_file):\n",
        "    assert predict_file.endswith('.tsv')\n",
        "    fpath = op.splitext(predict_file)[0]\n",
        "    return fpath + '.eval.json'\n",
        "\n",
        "\n",
        "def get_evaluate_method(predict_file):\n",
        "    if 'nocaps' in op.basename(predict_file):\n",
        "        return 'nocaps'\n",
        "    else:\n",
        "        return 'coco'\n",
        "\n",
        "\n",
        "def evaluate(args, val_dataloader, model, tokenizer, output_dir):\n",
        "    predict_file = get_predict_file(output_dir,\n",
        "            val_dataloader.dataset.yaml_file, args)\n",
        "    test(args, val_dataloader, model, tokenizer, predict_file)\n",
        "\n",
        "    if get_world_size() > 1:\n",
        "        torch.distributed.barrier()\n",
        "    evaluate_file = get_evaluate_file(predict_file)\n",
        "    if is_main_process():\n",
        "        caption_file = val_dataloader.dataset.get_caption_file_in_coco_format()\n",
        "        data = val_dataloader.dataset.yaml_file.split('/')[-2]\n",
        "        if 'nocaps' not in data:\n",
        "            result = evaluate_on_coco_caption(predict_file, caption_file, outfile=evaluate_file)\n",
        "            logger.info('evaluation result: {}'.format(str(result)))\n",
        "            logger.info('evaluation result saved to {}'.format(evaluate_file))\n",
        "    if get_world_size() > 1:\n",
        "        torch.distributed.barrier()\n",
        "    return evaluate_file\n",
        "\n",
        "\n",
        "def test(args, test_dataloader, model, tokenizer, predict_file):\n",
        "    cls_token_id, sep_token_id, pad_token_id, mask_token_id, period_token_id = \\\n",
        "        tokenizer.convert_tokens_to_ids([tokenizer.cls_token, tokenizer.sep_token, \n",
        "        tokenizer.pad_token, tokenizer.mask_token, '.'])\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        cache_file = predict_file\n",
        "    else:\n",
        "        cache_file = op.splitext(predict_file)[0] + '_{}_{}'.format(get_rank(), \n",
        "                world_size) + op.splitext(predict_file)[1]\n",
        "\n",
        "    model.eval()\n",
        "    inputs_param = {'is_decode': True,\n",
        "        'do_sample': False,\n",
        "        'bos_token_id': cls_token_id,\n",
        "        'pad_token_id': pad_token_id,\n",
        "        'eos_token_ids': [sep_token_id],\n",
        "        'mask_token_id': mask_token_id,\n",
        "        # for adding od labels\n",
        "        'add_od_labels': args.add_od_labels, 'od_labels_start_posid': args.max_seq_a_length,\n",
        "\n",
        "        # hyperparameters of beam search\n",
        "        'max_length': args.max_gen_length,\n",
        "        'num_beams': args.num_beams,\n",
        "        \"temperature\": args.temperature,\n",
        "        \"top_k\": args.top_k,\n",
        "        \"top_p\": args.top_p,\n",
        "        \"repetition_penalty\": args.repetition_penalty,\n",
        "        \"length_penalty\": args.length_penalty,\n",
        "        \"num_return_sequences\": args.num_return_sequences,\n",
        "        \"num_keep_best\": args.num_keep_best,\n",
        "    }\n",
        "    if args.use_cbs:\n",
        "        inputs_param.update({'use_cbs': True,\n",
        "            'min_constraints_to_satisfy': args.min_constraints_to_satisfy,\n",
        "        })\n",
        "    def gen_rows():\n",
        "        time_meter = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step, (img_keys, batch) in tqdm(enumerate(test_dataloader)):\n",
        "                batch = tuple(t.to(args.device) for t in batch)\n",
        "                inputs = {\n",
        "                    'input_ids': batch[0], 'attention_mask': batch[1],\n",
        "                    'token_type_ids': batch[2], 'img_feats': batch[3],\n",
        "                    'masked_pos': batch[4],\n",
        "                }\n",
        "                if args.use_cbs:\n",
        "                    inputs.update({\n",
        "                        'fsm': batch[5],\n",
        "                        'num_constraints': batch[6],\n",
        "                    })\n",
        "                inputs.update(inputs_param)\n",
        "                tic = time.time()\n",
        "                # captions, logprobs\n",
        "                outputs = model(**inputs)\n",
        "                time_meter += time.time() - tic\n",
        "                all_caps = outputs[0]  # batch_size * num_keep_best * max_len\n",
        "                all_confs = torch.exp(outputs[1])\n",
        "\n",
        "                for img_key, caps, confs in zip(img_keys, all_caps, all_confs):\n",
        "                    res = []\n",
        "                    for cap, conf in zip(caps, confs):\n",
        "                        cap = tokenizer.decode(cap.tolist(), skip_special_tokens=True)\n",
        "                        res.append({'caption': cap, 'conf': conf.item()})\n",
        "                    if isinstance(img_key, torch.Tensor):\n",
        "                        img_key = img_key.item()\n",
        "                    yield img_key, json.dumps(res)\n",
        "\n",
        "        logger.info(\"Inference model computing time: {} seconds per batch\".format(time_meter / (step+1)))\n",
        "\n",
        "    tsv_writer(gen_rows(), cache_file)\n",
        "    if world_size > 1:\n",
        "        torch.distributed.barrier()\n",
        "    if world_size > 1 and is_main_process():\n",
        "        cache_files = [op.splitext(predict_file)[0] + '_{}_{}'.format(i, world_size) + \\\n",
        "            op.splitext(predict_file)[1] for i in range(world_size)]\n",
        "        concat_tsv_files(cache_files, predict_file)\n",
        "        delete_tsv_files(cache_files)\n",
        "        reorder_tsv_keys(predict_file, test_dataloader.dataset.image_keys, predict_file)\n",
        "    if world_size > 1:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "\n",
        "def restore_training_settings(args):\n",
        "    if args.do_train:\n",
        "        if not args.scst:\n",
        "            return args\n",
        "        checkpoint = args.model_name_or_path\n",
        "    else:\n",
        "        assert args.do_test or args.do_eval\n",
        "        checkpoint = args.eval_model_dir\n",
        "    # restore training settings, check hasattr for backward compatibility\n",
        "    print(op.join(checkpoint, 'training_args.bin'))\n",
        "    train_args = torch.load(op.join(checkpoint, 'training_args.bin'))\n",
        "    if hasattr(train_args, 'max_seq_a_length'):\n",
        "        if hasattr(train_args, 'scst') and train_args.scst:\n",
        "            max_od_labels_len = train_args.max_seq_length - train_args.max_gen_length\n",
        "        else:\n",
        "            max_od_labels_len = train_args.max_seq_length - train_args.max_seq_a_length\n",
        "        max_seq_length = args.max_gen_length + max_od_labels_len\n",
        "        args.max_seq_length = max_seq_length\n",
        "        logger.warning('Override max_seq_length to {} = max_gen_length:{} + od_labels_len:{}'.format(\n",
        "                max_seq_length, args.max_gen_length, max_od_labels_len))\n",
        "\n",
        "\n",
        "    override_params = ['max_seq_a_length', 'do_lower_case', 'add_od_labels',\n",
        "            'max_img_seq_length']\n",
        "    for param in override_params:\n",
        "        if hasattr(train_args, param):\n",
        "            train_v = getattr(train_args, param)\n",
        "            test_v = getattr(args, param)\n",
        "            if train_v != test_v:\n",
        "                logger.warning('Override {} with train args: {} -> {}'.format(param,\n",
        "                    test_v, train_v))\n",
        "                setattr(args, param, train_v)\n",
        "    return args\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not dist.is_available():\n",
        "        return 1\n",
        "    if not dist.is_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not dist.is_available():\n",
        "        return 0\n",
        "    if not dist.is_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def synchronize():\n",
        "    \"\"\"\n",
        "    Helper function to synchronize (barrier) among all processes when\n",
        "    using distributed training\n",
        "    \"\"\"\n",
        "    if not dist.is_available():\n",
        "        return\n",
        "    if not dist.is_initialized():\n",
        "        return\n",
        "    world_size = dist.get_world_size()\n",
        "    if world_size == 1:\n",
        "        return\n",
        "    dist.barrier()\n",
        "\n",
        "\n",
        "def ensure_init_process_group(local_rank=None, port=12345):\n",
        "    # init with env\n",
        "    world_size = int(os.environ['WORLD_SIZE']) if 'WORLD_SIZE' in os.environ else 1\n",
        "    if world_size > 1 and not dist.is_initialized():\n",
        "        assert local_rank is not None\n",
        "        print(\"Init distributed training on local rank {}\".format(local_rank))\n",
        "        torch.cuda.set_device(local_rank)\n",
        "        dist.init_process_group(\n",
        "            backend='nccl', init_method='env://'\n",
        "        )\n",
        "    return local_rank\n",
        "\n",
        "class Arguments(object):\n",
        "  pass\n",
        "\n",
        "def main():\n",
        "    args = Arguments()\n",
        "    args.data_dir = 'datasets/coco_caption' # The input data dir with all required files.\n",
        "    args.train_yaml = 'train.yaml' # yaml file for training.\n",
        "    args.test_yaml = 'test.yaml' # yaml file for testing.\n",
        "    args.val_yaml = 'val.yaml' # yaml file used for validation during training.\n",
        "    args.model_name_or_path = None # Path to pre-trained model or model type.\n",
        "    args.output_dir = 'output/'# The output directory to save checkpoint and test results.\n",
        "    args.loss_type = 'sfmx'# Loss function types: support kl, x2, sfmx\n",
        "    args.config_name = \"\", # Pretrained config name or path if not the same as model_name.\n",
        "    args.tokenizer_name = \"\" # Pretrained tokenizer name or path if not the same as model_name.\n",
        "    args.max_seq_length = 70 # The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
        "    args.max_seq_a_length = 40 #The maximum sequence length for caption.\n",
        "    args.do_train = False # Whether to run training.\n",
        "    args.do_test = False # Whether to run inference.\n",
        "    args.do_eval = False # Whether to run evaluation\n",
        "    args.do_lower_case = False # Set this flag if you are using an uncased model.\n",
        "    args.mask_prob = 0.15 # Probability to mask input sentence during training\n",
        "    args.max_masked_tokens = 3 # The max number of masked tokens per sentence.\n",
        "    args.add_od_labels = False # Whether to add object detection labels or not\n",
        "    args.drop_out = 0.1 # Drop out in BERT\n",
        "\n",
        "    args.max_img_seq_length = 50 # The maximum total input image sequence length.\")\n",
        "    args.img_feature_dim = 2054 # The Image Feature Dimension.\")\n",
        "    args.img_feature_type= 'frcnn' #Image feature type.\")\n",
        "    args.tie_weights = False # Whether to tie decoding weights to that of encoding\")\n",
        "    args.freeze_embedding = False # Whether to freeze word embeddings in Bert\")\n",
        "    args.label_smoothing = 0\n",
        "    args.drop_worst_ratio = 0\n",
        "    args.drop_worst_after = 0\n",
        "    args.per_gpu_train_batch_size = 64 # Batch size per GPU/CPU for training.\")\n",
        "    args.per_gpu_eval_batch_size = 64 # Batch size per GPU/CPU for evaluation.\")\n",
        "    args.output_mode = 'classification' # output mode, support classification or regression.\")\n",
        "    args.num_labels = 2 # num_labels is 2 for classification and 1 for regression.\")\n",
        "    args.gradient_accumulation_steps = 1 # Number of updates steps to accumulate before backward.\")\n",
        "    args.learning_rate = 3e-5 # The initial lr.\")\n",
        "    args.weight_decay = 0.05 # Weight deay.\")\n",
        "    args.adam_epsilon = 1e-8 # Epsilon for Adam.\")\n",
        "    args.max_grad_norm = 1.0 # Max gradient norm.\")\n",
        "    args.warmup_steps = 0 # Linear warmup.\")\n",
        "    args.scheduler ='linear' # constant or linear or\")\n",
        "    args.num_workers = 4 # Workers in dataloader.\")\n",
        "    args.num_train_epochs = 40 # Total number of training epochs to perform.\")\n",
        "\n",
        "    args.max_steps = 5 # \"Total number of training steps. Override num_train_epochs.\")\n",
        "    args.logging_steps = 1 # \"Log every X steps.\")\n",
        "\n",
        "    args.save_steps = -1 # \"Save checkpoint every X steps. Will also perform evaluatin.\")\n",
        "    args.evaluate_during_training = False # Run evaluation during training at each save_steps.\")\n",
        "    args.no_cuda = False # Avoid using CUDA.\")\n",
        "    args.local_rank = 0 # For distributed training.\")\n",
        "    args.seed = 88 # random seed for initialization.\")\n",
        "    # for self-critical sequence training\n",
        "    args.scst = False # Self-critical sequence training')\n",
        "    args.sc_train_sample_n = 5 # \"number of sampled captions for sc training\")\n",
        "    args.sc_baseline_type = 'greedy' # \"baseline tyep of REINFORCE algorithm\")\n",
        "    args.sc_beam_size = 1 # beam size for scst training\")\n",
        "    args.cider_cached_tokens = 'coco-train-words.p' #path to cached cPickle file used to calculate CIDEr scores\")\n",
        "    # for generation\n",
        "    args.eval_model_dir = '' # \"Model directory for evaluation.\")\n",
        "    args.max_gen_length = 20 # \"max length of generated sentences\")\n",
        "    args.output_hidden_states = False # \"Turn on for fast decoding\")\n",
        "    args.num_return_sequences = 1 # repeating times per image\")\n",
        "    args.num_beams = 1 # beam search width\")\n",
        "    args.num_keep_best = 1 # number of hypotheses to keep in beam search\")\n",
        "    args.temperature = 1 # \"temperature in softmax for sampling\")\n",
        "    args.top_k = 0 # \"filter distribution for sampling\")\n",
        "    args.top_p = 1 # filter distribution for sampling\")\n",
        "    args.repetition_penalty = 1 # \"repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)\")\n",
        "    args.length_penalty = 1 # beam search length penalty\")\n",
        "    # for Constrained Beam Search\n",
        "    args.use_cbs = False # Use constrained beam search for decoding')\n",
        "    args.min_constraints_to_satisfy = 2 # minimum number of constraints to satisfy\")\n",
        "\n",
        "    args.overfit = False\n",
        "\n",
        "\n",
        "    global logger\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    local_rank = ensure_init_process_group(local_rank=args.local_rank)\n",
        "    args.local_rank = local_rank\n",
        "    args.num_gpus = get_world_size()\n",
        "    args.distributed = args.num_gpus > 1\n",
        "    args.device = torch.device('cuda')\n",
        "    args.data_dir = \"/content/drive/MyDrive/OscarPlus/datasets/coco_caption\"\n",
        "\n",
        "    # Setup custom arguments at here, this is for python notebook compatability\n",
        "    args.do_test = True\n",
        "    args.do_eval = True\n",
        "    args.test_yaml = \"test.yaml\"\n",
        "    args.num_beams = 5\n",
        "    args.max_gen_length = 20\n",
        "    args.eval_model_dir = \"/content/drive/MyDrive/OscarPlus/weights/checkpoint0\"\n",
        "    args.overfit = True\n",
        "    synchronize()\n",
        "\n",
        "    output_dir = args.output_dir\n",
        "    mkdir(output_dir)\n",
        "\n",
        "    logger = setup_logger(\"vlpretrain\", output_dir, args.local_rank)\n",
        "    logger.warning(\"Device: %s, n_gpu: %s\", args.device, args.num_gpus)\n",
        "    set_seed(args.seed, args.num_gpus)\n",
        "    args = restore_training_settings(args)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config_class, model_class, tokenizer_class = BertConfig, BertForImageCaptioning, BertTokenizer\n",
        "    if args.do_train:\n",
        "        assert args.model_name_or_path is not None\n",
        "        config = config_class.from_pretrained(args.config_name if args.config_name else \\\n",
        "                args.model_name_or_path, num_labels=args.num_labels, finetuning_task='image_captioning')\n",
        "        if args.scst:\n",
        "            # avoid using too much memory\n",
        "            config.output_hidden_states = True\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name \\\n",
        "                else args.model_name_or_path, do_lower_case=args.do_lower_case)\n",
        "        config.img_feature_dim = args.img_feature_dim\n",
        "        config.img_feature_type = args.img_feature_type\n",
        "        config.hidden_dropout_prob = args.drop_out\n",
        "        config.loss_type = args.loss_type\n",
        "        config.tie_weights = args.tie_weights\n",
        "        config.freeze_embedding = args.freeze_embedding\n",
        "        config.label_smoothing = args.label_smoothing\n",
        "        config.drop_worst_ratio = args.drop_worst_ratio\n",
        "        config.drop_worst_after = args.drop_worst_after\n",
        "        model = model_class.from_pretrained(args.model_name_or_path,\n",
        "                from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n",
        "    else:\n",
        "        checkpoint = args.eval_model_dir\n",
        "        assert op.isdir(checkpoint)\n",
        "        config = config_class.from_pretrained(checkpoint)\n",
        "        config.output_hidden_states = args.output_hidden_states\n",
        "        tokenizer = tokenizer_class.from_pretrained(checkpoint)\n",
        "        logger.info(\"Evaluate the following checkpoint: %s\", checkpoint)\n",
        "        model = model_class.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "    if args.no_cuda:\n",
        "        pass\n",
        "    else:\n",
        "        model.to(args.device)\n",
        "        \n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "    if args.do_train:\n",
        "        train_dataloader = make_data_loader(args, args.train_yaml, tokenizer,\n",
        "            args.distributed, is_train=True)\n",
        "        val_dataloader = None\n",
        "        if args.evaluate_during_training:\n",
        "            val_dataloader = make_data_loader(args, args.val_yaml, tokenizer,\n",
        "                args.distributed, is_train=False)\n",
        "        last_checkpoint = train(args, train_dataloader, val_dataloader, model, tokenizer)\n",
        "\n",
        "        # test the last checkpoint after training\n",
        "        if args.do_test:\n",
        "            logger.info(\"Evaluate on dataset: \" + args.test_yaml)\n",
        "            test_dataloader = make_data_loader(args, args.test_yaml, \n",
        "                tokenizer, args.distributed, is_train=False)\n",
        "            evaluate(args, test_dataloader, model, tokenizer, last_checkpoint)\n",
        "\n",
        "    # inference and evaluation\n",
        "    elif args.do_test or args.do_eval:\n",
        "        logger.info(\"Evaluate on dataset: \" + args.test_yaml)\n",
        "        test_dataloader = make_data_loader(args, args.test_yaml,\n",
        "            tokenizer, args.distributed, is_train=False)\n",
        "\n",
        "        if not args.do_eval:\n",
        "            predict_file = get_predict_file(checkpoint, test_dataloader.dataset.yaml_file, args)\n",
        "            test(args, test_dataloader, model, tokenizer, predict_file)\n",
        "            logger.info(\"Prediction results saved to: {}\".format(predict_file))\n",
        "        else:\n",
        "            evaluate_file = evaluate(args, test_dataloader, model, tokenizer,\n",
        "                    checkpoint)\n",
        "            logger.info(\"Evaluation results saved to: {}\".format(evaluate_file))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-18 02:23:45,188 vlpretrain WARNING: Device: cuda, n_gpu: 1\n",
            "/content/drive/MyDrive/OscarPlus/weights/checkpoint0/training_args.bin\n",
            "2021-11-18 02:23:45,937 vlpretrain WARNING: Override max_seq_length to 50 = max_gen_length:20 + od_labels_len:30\n",
            "2021-11-18 02:23:45,940 vlpretrain WARNING: Override do_lower_case with train args: False -> True\n",
            "2021-11-18 02:23:45,943 vlpretrain WARNING: Override add_od_labels with train args: False -> True\n",
            "2021-11-18 02:23:47,072 vlpretrain INFO: Evaluate the following checkpoint: /content/drive/MyDrive/OscarPlus/weights/checkpoint0\n",
            "2021-11-18 02:24:19,479 vlpretrain INFO: Training/evaluation parameters <__main__.Arguments object at 0x7f623b1fac10>\n",
            "2021-11-18 02:24:19,488 vlpretrain INFO: Evaluate on dataset: test.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/content/drive/MyDrive/OscarPlus/oscar/modeling/modeling_utils.py:506: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beam_id = idx // vocab_size\n",
            "2it [02:04, 62.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-18 02:26:30,119 vlpretrain INFO: Inference model computing time: 59.924986362457275 seconds per batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "INFO:vlpretrain:Inference model computing time: 59.924986362457275 seconds per batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "0:00:00.670935\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...     \n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "tokenization...\n",
            "setting up scorers...\n",
            "computing Bleu score...\n",
            "{'testlen': 1468, 'reflen': 1215, 'guess': [1468, 1368, 1268, 1168], 'correct': [0, 0, 0, 0]}\n",
            "ratio: 1.2082304526739027\n",
            "Bleu_1: 0.000\n",
            "Bleu_2: 0.000\n",
            "Bleu_3: 0.000\n",
            "Bleu_4: 0.000\n",
            "computing METEOR score...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BrokenPipeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b439021f759b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-b439021f759b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             evaluate_file = evaluate(args, test_dataloader, model, tokenizer,\n\u001b[0;32m--> 992\u001b[0;31m                     checkpoint)\n\u001b[0m\u001b[1;32m    993\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluation results saved to: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b439021f759b>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(args, val_dataloader, model, tokenizer, output_dir)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'nocaps'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_on_coco_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluation result: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluation result saved to {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/OscarPlus/oscar/utils/caption_evaluate.py\u001b[0m in \u001b[0;36mevaluate_on_coco_caption\u001b[0;34m(res_file, label_file, outfile)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# evaluate results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# SPICE will take a few minutes the first time, but speeds up due to caching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcocoEval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcocoEval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/OscarPlus/coco_caption/pycocoevalcap/eval.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'computing %s score...'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/OscarPlus/coco_caption/pycocoevalcap/meteor/meteor.py\u001b[0m in \u001b[0;36mcompute_score\u001b[0;34m(self, gts, res)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimgIds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0meval_line\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' ||| {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/OscarPlus/coco_caption/pycocoevalcap/meteor/meteor.py\u001b[0m in \u001b[0;36m_stat\u001b[0;34m(self, hypothesis_str, reference_list)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mscore_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' ||| '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SCORE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' ||| '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeteor_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeteor_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeteor_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ]
    }
  ]
}