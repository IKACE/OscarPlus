{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "oscar_pretrain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IKACE/OscarPlus/blob/main/oscar_pretrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEDqnYIMuAQp"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsewaoP1pLkG"
      },
      "source": [
        "Make sure that modules and datasets are properly mounted from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_8Y6CLSaFq7",
        "outputId": "f3ef0a0b-9900-44b7-f3dd-2e3a815e44fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Corpus lineidx file to google drive"
      ],
      "metadata": {
        "id": "85AegIjlg5tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./azcopy_linux_amd64_10.13.0/azcopy copy https://biglmdiag.blob.core.windows.net/vinvl/pretrain_corpus/coco_flickr30k_gqa.lineidx /content/drive/MyDrive/OscarPlus/datasets/pretrain "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B9n4u6Mg-h1",
        "outputId": "1a41ff47-8e27-4b65-8b97-0d6a1fcbd084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: ./azcopy_linux_amd64_10.13.0/azcopy: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download COCO image labels to google drive"
      ],
      "metadata": {
        "id": "5ivvYXNiYVm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./azcopy_linux_amd64_10.13.0/azcopy copy https://biglmdiag.blcore.windows.net/vinvl/pretrain_corpus/X152C4_frcnnbig2_exp168model_0060000model.roi_heads.nm_filter_2_model.roi_heads.score_thresh_0.2 /content/drive/MyDrive/OscarPlus/datasets/pretrain --recursive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SgsX08gYUwc",
        "outputId": "8c39f2f3-65ed-404e-bf12-5f984a02ff0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: ./azcopy_linux_amd64_10.13.0/azcopy: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download COCO image features to google drive"
      ],
      "metadata": {
        "id": "utb2nvtygBfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --recursive  \"https://biglmdiag.blob.core.windows.net/vinvl/image_features/coco_X152C4_frcnnbig2_exp168model_0060000model.roi_heads.nm_filter_2_model.roi_heads.score_thresh_0.2/model_0060000/\" -P \"/content/drive/MyDrive/OscarPlus/datasets/pretrain\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_HDOmHWgAhT",
        "outputId": "88a4eb20-ba17-4054-c4c4-081fba3fc923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-09 23:33:13--  https://biglmdiag.blob.core.windows.net/vinvl/image_features/coco_X152C4_frcnnbig2_exp168model_0060000model.roi_heads.nm_filter_2_model.roi_heads.score_thresh_0.2/model_0060000/\n",
            "Resolving biglmdiag.blob.core.windows.net (biglmdiag.blob.core.windows.net)... 52.240.48.36\n",
            "Connecting to biglmdiag.blob.core.windows.net (biglmdiag.blob.core.windows.net)|52.240.48.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 The specified blob does not exist.\n",
            "2021-12-09 23:33:13 ERROR 404: The specified blob does not exist..\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./azcopy_linux_amd64_10.13.0/azcopy copy https://biglmdiag.blob.core.windows.net/vinvl/image_features/coco_X152C4_frcnnbig2_exp168model_0060000model.roi_heads.nm_filter_2_model.roi_heads.score_thresh_0.2/model_0060000/ /content/drive/MyDrive/OscarPlus/datasets/pretrain --recursive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm_MQb9ejatx",
        "outputId": "8eeb1b4c-21e9-49c7-90d5-7bc89ab1928a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Scanning...\n",
            "INFO: Any empty folders will not be processed, because source and/or destination doesn't have full folder support\n",
            "\n",
            "Job 0067ba47-2357-5f44-72ea-907715586a52 has started\n",
            "Log file is located at: /root/.azcopy/0067ba47-2357-5f44-72ea-907715586a52.log\n",
            "\n",
            "100.0 %, 14 Done, 0 Failed, 1 Pending, 0 Skipped, 15 Total,                              \n",
            "\n",
            "\n",
            "Job 0067ba47-2357-5f44-72ea-907715586a52 summary\n",
            "Elapsed Time (Minutes): 14.3871\n",
            "Number of File Transfers: 15\n",
            "Number of Folder Property Transfers: 0\n",
            "Total Number of Transfers: 15\n",
            "Number of Transfers Completed: 15\n",
            "Number of Transfers Failed: 0\n",
            "Number of Transfers Skipped: 0\n",
            "TotalBytesTransferred: 104635654631\n",
            "Final Job Status: Completed\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/OscarPlus/datasets/pretrain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMxRZuDWh_wp",
        "outputId": "8e55e69a-9d98-4e5a-edb4-bb719b62aae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coco.lineidx  coco.tsv\tcoco.yaml  labels  model_0060000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMiNxOBauO6G",
        "outputId": "64601885-35a7-4f5c-d6fa-2cfeb5422007"
      },
      "source": [
        "!ls drive/MyDrive/OscarPlus/coco_caption"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotations\t\t__init__.py  pycocoevalcap  results\n",
            "cocoEvalCapDemo.ipynb\tlicense.txt  pycocotools\n",
            "get_stanford_models.sh\t__pycache__  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-9VGiTmPyfP",
        "outputId": "4511f42b-694b-4979-8c1d-da2801f57abf"
      },
      "source": [
        "!ls /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 281project4\n",
            " 450DR2.drawio\n",
            " 450DR2multimodal.drawio\n",
            "'Colab Notebooks'\n",
            " Notability\n",
            " OscarPlus\n",
            " pred.coco_caption.test.beam5.max20.odlabels.tsv.tmp\n",
            " ps_9_sshoouri.ipynb\n",
            " sshoouri.ipynb\n",
            " VE450\n",
            "'VE450 Design Review #1 - 条形图 1.gsheet'\n",
            "'VE450 Design Review #1 - 柱形图 1.gsheet'\n",
            "'VE450 Design Review #1 - 柱形图 2.gsheet'\n",
            " Yile_Gu_Final_Essay_517370910109.docx\n",
            "'大三DD学生转学分P F或letter grade意向调查表.gform'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSi-WnNx5nfR"
      },
      "source": [
        "Install python requirements"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVf0dNpPbuS5",
        "outputId": "298f70dd-b9ec-4952-c096-a7f8526c836c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardx\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx) (1.15.0)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_od9neO5p-4",
        "outputId": "cdd2e023-08ca-4b01-8074-1527de32b6e5"
      },
      "source": [
        "!pip install -r /content/drive/MyDrive/OscarPlus/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 1)) (4.62.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 2)) (3.13)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (0.18.3)\n",
            "Collecting anytree\n",
            "  Downloading anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▉                        | 10 kB 34.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 20 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 30 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 40 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 429 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 7)) (2019.12.20)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.23-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 13.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (2021.11.2)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.24.0,>=1.23.23\n",
            "  Downloading botocore-1.23.23-py3-none-any.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 40.1 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 70.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, jmespath, botocore, s3transfer, boto3, anytree\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed anytree-2.8.0 boto3-1.20.23 botocore-1.23.23 jmespath-0.10.0 s3transfer-0.5.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-UehtJCpXVw"
      },
      "source": [
        "Add drive directory to system path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49Ryure3unWA"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/OscarPlus\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miKiUiLn1DrN"
      },
      "source": [
        "Make sure submodules can be imported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKF7ggoyu1aX"
      },
      "source": [
        "from coco_caption.pycocotools.coco import COCO"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EitKYqG4yHih"
      },
      "source": [
        "# Image Caption"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Bert class"
      ],
      "metadata": {
        "id": "rxKxhP8l4U2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from transformers.pytorch_transformers.modeling_bert import (BertEmbeddings, \n",
        "        BertSelfAttention, BertAttention, BertEncoder, BertLayer, \n",
        "        BertSelfOutput, BertIntermediate, BertOutput,\n",
        "        BertPooler, BertLayerNorm, BertPreTrainedModel,\n",
        "\t\tBertPredictionHeadTransform, BertOnlyMLMHead, BertLMPredictionHead,\n",
        "        BertConfig, BERT_PRETRAINED_MODEL_ARCHIVE_MAP,\n",
        "        load_tf_weights_in_bert)\n",
        "from oscar.modeling.modeling_utils import CaptionPreTrainedModel, ImgPreTrainedModel\n",
        "from oscar.modeling.modeling_bert import BertImgModel\n",
        "from oscar.utils.cbs import ConstrainedBeamSearch, select_best_beam_with_constraints\n",
        "\n",
        "class BertCaptioningLoss(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.label_smoothing = getattr(config, 'label_smoothing', 0)\n",
        "        self.drop_worst_ratio = getattr(config, 'drop_worst_ratio', 0)\n",
        "        self.drop_worst_after = getattr(config, 'drop_worst_after', 0)\n",
        "        self.log_soft = nn.LogSoftmax(dim=1)\n",
        "        self.kl = nn.KLDivLoss(reduction='none')\n",
        "        self.iter = 0\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        self.iter += 1\n",
        "        eps = self.label_smoothing\n",
        "        n_class = logits.size(1)\n",
        "        one_hot = torch.zeros_like(logits).scatter(1, target.view(-1, 1), 1)\n",
        "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
        "        log_prb = self.log_soft(logits)\n",
        "        loss = self.kl(log_prb, one_hot).sum(1)\n",
        "\n",
        "        if self.drop_worst_ratio > 0 and self.iter > self.drop_worst_after:\n",
        "            loss, _ = torch.topk(loss,\n",
        "                    k=int(loss.shape[0] * (1-self.drop_worst_ratio)),\n",
        "                    largest=False)\n",
        "\n",
        "        loss = loss.mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "class BertForImageCaptioning(CaptionPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Bert for Image Captioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForImageCaptioning, self).__init__(config)\n",
        "        self.config = config\n",
        "        self.bert = BertImgModel(config)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "        self.loss = BertCaptioningLoss(config)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        if hasattr(self.config, 'tie_weights') and self.config.tie_weights:\n",
        "            self._tie_or_clone_weights(self.cls.predictions.decoder,\n",
        "                                       self.bert.embeddings.word_embeddings)\n",
        "        freeze = False\n",
        "        if hasattr(self.config, 'freeze_embedding'):\n",
        "            freeze = self.config.freeze_embedding\n",
        "        self.bert.embeddings.word_embeddings.weight.requires_grad = not freeze\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        is_decode = kwargs.get('is_decode', False)\n",
        "        if is_decode:\n",
        "            return self.generate(*args, **kwargs)\n",
        "        else:\n",
        "            return self.encode_forward(*args, **kwargs)\n",
        "\n",
        "    def encode_forward(self, input_ids, img_feats, attention_mask, masked_pos, masked_ids=None, \n",
        "            token_type_ids=None, position_ids=None, head_mask=None,\n",
        "            is_training=True, encoder_history_states=None):\n",
        "        outputs = self.bert(input_ids, img_feats=img_feats, attention_mask=attention_mask, \n",
        "                position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                head_mask=head_mask,\n",
        "                encoder_history_states=encoder_history_states)\n",
        "        sequence_output = outputs[0][:, :masked_pos.shape[-1], :]\n",
        "\n",
        "        if is_training:\n",
        "            sequence_output = outputs[0][:, :masked_pos.shape[-1], :]\n",
        "            # num_masks_in_batch * hidden_size\n",
        "            sequence_output_masked = sequence_output[masked_pos==1, :]\n",
        "            class_logits = self.cls(sequence_output_masked)\n",
        "            masked_ids = masked_ids[masked_ids != 0]   # remove padding masks\n",
        "            masked_loss = self.loss(class_logits.float(), masked_ids)\n",
        "            outputs = (masked_loss, class_logits,) + outputs[2:]\n",
        "        else:\n",
        "            sequence_output = outputs[0][:, :input_ids.shape[-1], :]\n",
        "            class_logits = self.cls(sequence_output)\n",
        "            outputs = (class_logits,) + outputs[2:]\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def prepare_inputs_for_generation(self, curr_ids, past=None):\n",
        "        # NOTE: if attention is on, it should be the token used to mask words in training\n",
        "        mask_token_id = self.mask_token_id\n",
        "        batch_size = curr_ids.shape[0]\n",
        "        mask_ids = torch.full(\n",
        "            (batch_size, 1), mask_token_id, dtype=torch.long, device=curr_ids.device\n",
        "        )\n",
        "\n",
        "        def _slice(t, start, end):\n",
        "            if t is None:\n",
        "                return t\n",
        "            assert t.shape == (batch_size, self.max_seq_len + self.od_labels_len)\n",
        "            return t[:, start: end]\n",
        "\n",
        "        def _remove_elements(t, start, end):\n",
        "            if t is None:\n",
        "                return t\n",
        "            assert t.shape == (batch_size, self.max_seq_len + self.od_labels_len)\n",
        "            return torch.cat([t[:, :start], t[:, end:]], dim=1)\n",
        "\n",
        "        if past is None:\n",
        "            input_ids = torch.cat([curr_ids, mask_ids], dim=1)\n",
        "\n",
        "            curr_len = input_ids.shape[1]\n",
        "            full_len = self.max_seq_len + self.od_labels_len + self.img_seq_len\n",
        "            assert self.full_attention_mask.shape == (batch_size,\n",
        "                    full_len, full_len)\n",
        "\n",
        "            def _remove_rows_cols(t, row_start, row_end, col_start, col_end):\n",
        "                t00 = t[:, :row_start, :col_start]\n",
        "                t01 = t[:, :row_start, col_end:]\n",
        "                t10 = t[:, row_end:, :col_start]\n",
        "                t11 = t[:, row_end:, col_end:]\n",
        "                res = torch.cat([torch.cat([t00, t01], dim=2), torch.cat([t10, t11],\n",
        "                            dim=2)], dim=1)\n",
        "                assert res.shape == (t.shape[0], t.shape[1]-row_end+row_start,\n",
        "                        t.shape[2]-col_end+col_start)\n",
        "                return res\n",
        "\n",
        "            seq_start = curr_len\n",
        "            seq_end = self.max_seq_len\n",
        "            attention_mask = _remove_rows_cols(self.full_attention_mask, seq_start,\n",
        "                    seq_end, seq_start, seq_end)\n",
        "\n",
        "            masked_pos = _remove_elements(self.full_masked_pos, seq_start, seq_end)\n",
        "            token_type_ids = _remove_elements(self.full_token_type_ids, seq_start, seq_end)\n",
        "            position_ids = _remove_elements(self.full_position_ids, seq_start, seq_end)\n",
        "            img_feats = self.img_feats\n",
        "\n",
        "            if self.add_od_labels:\n",
        "                assert self.od_label_ids.shape[1] == self.od_labels_len\n",
        "                input_ids = torch.cat([input_ids, self.od_label_ids], dim=1)\n",
        "        else:\n",
        "            last_token = curr_ids[:, -1:]\n",
        "            # The representation of last token should be re-computed, because\n",
        "            # it depends on both self-attention context and input tensor\n",
        "            input_ids = torch.cat([last_token, mask_ids], dim=1)\n",
        "            start_pos = curr_ids.shape[1] - 1\n",
        "            end_pos = start_pos + input_ids.shape[1]\n",
        "            masked_pos = _slice(self.full_masked_pos, start_pos, end_pos)\n",
        "            token_type_ids = _slice(self.full_token_type_ids, start_pos, end_pos)\n",
        "            position_ids = _slice(self.full_position_ids, start_pos, end_pos)\n",
        "\n",
        "            img_feats = None\n",
        "            assert past[0].shape[0] == batch_size\n",
        "            if self.prev_encoded_layers is None:\n",
        "                assert start_pos == 1  # the first token after BOS\n",
        "                assert past[0].shape[1] == 2 + self.od_labels_len + self.img_seq_len\n",
        "                # reorder to [od_labels, img_feats, sentence]\n",
        "                self.prev_encoded_layers = [\n",
        "                        torch.cat([x[:, 2:, :], x[:, :start_pos,:]], dim=1)\n",
        "                        for x in past]\n",
        "                s2s = self.full_attention_mask[:, :self.max_seq_len,\n",
        "                        :self.max_seq_len]\n",
        "                s2i = self.full_attention_mask[:, :self.max_seq_len,\n",
        "                        self.max_seq_len:]\n",
        "                i2s = self.full_attention_mask[:, self.max_seq_len:,\n",
        "                        :self.max_seq_len]\n",
        "                i2i = self.full_attention_mask[:, self.max_seq_len:,\n",
        "                        self.max_seq_len:]\n",
        "                self.full_attention_mask = torch.cat(\n",
        "                        [torch.cat([i2i, i2s], dim=2),\n",
        "                        torch.cat([s2i, s2s], dim=2)],\n",
        "                        dim=1)\n",
        "            else:\n",
        "                assert start_pos > 1\n",
        "                assert past[0].shape[1] == 2\n",
        "                self.prev_encoded_layers = [torch.cat([x, p[:, :-1, :]], dim=1)\n",
        "                        for x, p in zip(self.prev_encoded_layers, past)]\n",
        "\n",
        "            attention_mask = self.full_attention_mask[:,\n",
        "                self.od_labels_len+self.img_seq_len+start_pos: self.od_labels_len+self.img_seq_len+end_pos,\n",
        "                :self.od_labels_len+self.img_seq_len+end_pos]\n",
        "\n",
        "        return {'input_ids': input_ids, 'img_feats': img_feats,\n",
        "            'masked_pos': masked_pos, 'attention_mask': attention_mask,\n",
        "            'token_type_ids': token_type_ids, 'position_ids': position_ids,\n",
        "            'is_training': False,\n",
        "            'encoder_history_states': self.prev_encoded_layers}\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.decoder\n",
        "\n",
        "    def generate(self, img_feats, attention_mask, masked_pos, token_type_ids=None,\n",
        "            position_ids=None, head_mask=None, input_ids=None, max_length=None,\n",
        "            do_sample=None, num_beams=None, temperature=None, top_k=None, top_p=None,\n",
        "            repetition_penalty=None, bos_token_id=None, pad_token_id=None,\n",
        "            eos_token_ids=None, mask_token_id=None, length_penalty=None,\n",
        "            num_return_sequences=None,\n",
        "            num_keep_best=1, is_decode=None,\n",
        "            add_od_labels=False, od_labels_start_posid=None,\n",
        "            use_cbs=False, fsm=None, num_constraints=None,\n",
        "            min_constraints_to_satisfy=None, use_hypo=False,\n",
        "            decoding_constraint_flag=None, bad_ending_ids=None,\n",
        "            ):\n",
        "        \"\"\" Generates captions given image features\n",
        "        \"\"\"\n",
        "        assert is_decode\n",
        "        batch_size = img_feats.shape[0]\n",
        "        self.img_seq_len = img_feats.shape[1]\n",
        "        self.max_seq_len = max_length\n",
        "        self.mask_token_id = mask_token_id\n",
        "        self.prev_encoded_layers = None\n",
        "        # NOTE: num_keep_best is not equavilant to num_return_sequences\n",
        "        # num_keep_best is the number of hypotheses to keep in beam search\n",
        "        # num_return_sequences is the repeating times of input, coupled with\n",
        "        # do_sample=True can generate more than one samples per image\n",
        "        self.num_keep_best = num_keep_best\n",
        "\n",
        "        vocab_size = self.config.vocab_size\n",
        "        if not use_cbs:\n",
        "            num_fsm_states = 1\n",
        "        else:\n",
        "            b, num_fsm_states, f1, v = fsm.shape\n",
        "            assert b==batch_size and v==vocab_size and f1==num_fsm_states\n",
        "\n",
        "        self.add_od_labels = add_od_labels\n",
        "        # avoid position_ids collision of caption and od labels\n",
        "        self.od_labels_start_posid = max(od_labels_start_posid, self.max_seq_len)\n",
        "        if self.add_od_labels:\n",
        "            # get od labels part from input_ids\n",
        "            assert input_ids.shape[0] == batch_size\n",
        "            od_label_ids = input_ids[:, self.max_seq_len:]\n",
        "            self.od_labels_len = input_ids.shape[1] - self.max_seq_len\n",
        "            input_ids = None\n",
        "        else:\n",
        "            self.od_labels_len = 0\n",
        "            od_label_ids = None\n",
        "            print(input_ids.shape)\n",
        "            print((batch_size, self.max_seq_len))\n",
        "            # assert input_ids.shape == (batch_size, self.max_seq_len)\n",
        "\n",
        "            input_ids = None\n",
        "\n",
        "        if input_ids is None:\n",
        "            input_ids = torch.full(\n",
        "                (batch_size, 1), bos_token_id, dtype=torch.long, device=img_feats.device\n",
        "            )\n",
        "        else:\n",
        "            assert input_ids.dim() == 2, \"Input prompt should be of shape (batch_size, sequence length).\"\n",
        "            assert input_ids.shape[0] == batch_size, \"Input batch size must match image features\"\n",
        "\n",
        "        cur_len = input_ids.shape[1]\n",
        "        if  num_return_sequences != 1:\n",
        "            # Expand input to num return sequences\n",
        "            input_ids = self._expand_for_beams(input_ids, num_return_sequences)\n",
        "            effective_batch_size = batch_size * num_return_sequences\n",
        "        else:\n",
        "            effective_batch_size = batch_size\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = torch.arange(self.max_seq_len, dtype=torch.long, device=input_ids.device)\n",
        "            posids_len = self.max_seq_len\n",
        "            if self.add_od_labels:\n",
        "                od_labels_posids = torch.arange(\n",
        "                        self.od_labels_start_posid,\n",
        "                        self.od_labels_start_posid + self.od_labels_len, dtype=torch.long, device=input_ids.device)\n",
        "                position_ids = torch.cat([position_ids, od_labels_posids])\n",
        "                posids_len += self.od_labels_len\n",
        "            position_ids = position_ids.unsqueeze(0).expand([batch_size, posids_len])\n",
        "\n",
        "        num_expand = num_beams * num_fsm_states * num_return_sequences\n",
        "        self.od_label_ids = self._expand_for_beams(od_label_ids, num_expand)\n",
        "        self.img_feats = self._expand_for_beams(img_feats, num_expand)\n",
        "        self.full_attention_mask = self._expand_for_beams(attention_mask, num_expand)\n",
        "        self.full_masked_pos = self._expand_for_beams(masked_pos, num_expand)\n",
        "        self.full_token_type_ids = self._expand_for_beams(token_type_ids, num_expand)\n",
        "        self.full_position_ids = self._expand_for_beams(position_ids, num_expand)\n",
        "        self.full_head_mask = self._expand_for_beams(head_mask, num_expand)\n",
        "\n",
        "        if not use_cbs:\n",
        "            if num_beams > 1:\n",
        "                output = self._generate_beam_search(\n",
        "                    input_ids,\n",
        "                    cur_len,\n",
        "                    max_length,\n",
        "                    do_sample,\n",
        "                    temperature,\n",
        "                    top_k,\n",
        "                    top_p,\n",
        "                    repetition_penalty,\n",
        "                    pad_token_id,\n",
        "                    eos_token_ids,\n",
        "                    effective_batch_size,\n",
        "                    length_penalty,\n",
        "                    num_beams,\n",
        "                    vocab_size,\n",
        "                )\n",
        "            else:\n",
        "                output = self._generate_no_beam_search(\n",
        "                    input_ids,\n",
        "                    cur_len,\n",
        "                    max_length,\n",
        "                    do_sample,\n",
        "                    temperature,\n",
        "                    top_k,\n",
        "                    top_p,\n",
        "                    repetition_penalty,\n",
        "                    pad_token_id,\n",
        "                    eos_token_ids,\n",
        "                    effective_batch_size,\n",
        "                )\n",
        "        else:\n",
        "            assert self.num_keep_best == 1, 'not supported n_best > 1 for CBS'\n",
        "            searcher = ConstrainedBeamSearch(eos_token_ids, max_length,\n",
        "                    num_beams)\n",
        "            curr_ids, sum_logprobs = searcher.search(\n",
        "                    input_ids,\n",
        "                    None,\n",
        "                    self._decode_step,\n",
        "                    fsm,\n",
        "            )\n",
        "            curr_ids, logprobs = select_best_beam_with_constraints(\n",
        "                curr_ids,\n",
        "                sum_logprobs,\n",
        "                num_constraints,\n",
        "                min_constraints_to_satisfy,\n",
        "                eos_token_ids,\n",
        "            )\n",
        "            # (batch_size, n_best, max_len), (batch_size, n_best)\n",
        "            output = (curr_ids.unsqueeze(1), logprobs.unsqueeze(1))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _expand_for_beams(self, x, num_expand):\n",
        "        if x is None or num_expand == 1:\n",
        "            return x\n",
        "\n",
        "        input_shape = list(x.shape)\n",
        "        expanded_shape = input_shape[:1] + [num_expand] + input_shape[1:]\n",
        "        x = x.unsqueeze(1).expand(expanded_shape)\n",
        "        # (batch_size * num_expand, ...)\n",
        "        x = x.contiguous().view([input_shape[0] * num_expand] + input_shape[1:])\n",
        "        return x\n",
        "\n",
        "    def _do_output_past(self, outputs):\n",
        "        return len(outputs) > 1"
      ],
      "metadata": {
        "id": "b0pCRvzy4Q9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rLaa4Izd4o0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Driver for training"
      ],
      "metadata": {
        "id": "cFvdqUMW4pP5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uONXz2DAyNNa",
        "outputId": "fcf15029-839b-4cb5-e841-6d946f0ef3b0"
      },
      "source": [
        "# Copyright (c) 2021 Microsoft Corporation. Licensed under the MIT license.\n",
        "\n",
        "import argparse\n",
        "import base64\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path as op\n",
        "import random, time, json\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "from oscar.utils.logger import setup_logger\n",
        "from oscar.utils.tsv_file import TSVFile\n",
        "from oscar.utils.tsv_file_ops import (tsv_writer, concat_tsv_files,\n",
        "        delete_tsv_files, reorder_tsv_keys)\n",
        "from oscar.utils.misc import (mkdir, set_seed, \n",
        "        load_from_yaml_file, find_file_path_in_yaml)\n",
        "from oscar.utils.caption_evaluate import (evaluate_on_coco_caption,\n",
        "        ScstRewardCriterion)\n",
        "from oscar.utils.cbs import ConstraintFilter, ConstraintBoxesReader\n",
        "from oscar.utils.cbs import FiniteStateMachineBuilder\n",
        "# from oscar.modeling.modeling_bert import BertForImageCaptioning\n",
        "from transformers.pytorch_transformers import BertTokenizer, BertConfig\n",
        "from transformers.pytorch_transformers import AdamW, WarmupLinearSchedule, WarmupConstantSchedule\n",
        "\n",
        "\n",
        "class CaptionTSVDataset(Dataset):\n",
        "    def __init__(self, yaml_file, tokenizer=None, add_od_labels=True,\n",
        "            max_img_seq_length=50, max_seq_length=70, max_seq_a_length=40, \n",
        "            is_train=True, mask_prob=0.15, max_masked_tokens=3, overfit=False, **kwargs):\n",
        "        \"\"\"Constructor.\n",
        "        Args:\n",
        "            yaml file with all required data (image feature, caption, labels, etc)\n",
        "            tokenizer: tokenizer for text processing.\n",
        "            add_od_labels: whether to add labels from yaml file to BERT. \n",
        "            max_img_seq_length: max image sequence length.\n",
        "            max_seq_length: max text sequence length.\n",
        "            max_seq_a_length: max caption sequence length.\n",
        "            is_train: train or test mode.\n",
        "            mask_prob: probability to mask a input token.\n",
        "            max_masked_tokens: maximum number of tokens to be masked in one sentence.\n",
        "            overfit: provide a small dataset to test pipeline\n",
        "            kwargs: other arguments.\n",
        "        \"\"\"\n",
        "        self.yaml_file = yaml_file\n",
        "        self.cfg = load_from_yaml_file(yaml_file)\n",
        "        self.root = op.dirname(yaml_file)\n",
        "        self.label_file = find_file_path_in_yaml(self.cfg['label'], self.root)\n",
        "        self.feat_file = find_file_path_in_yaml(self.cfg['feature'], self.root)\n",
        "        self.caption_file = find_file_path_in_yaml(self.cfg.get('caption'), self.root)\n",
        "\n",
        "        self.overfit = overfit\n",
        "\n",
        "        assert op.isfile(self.feat_file)\n",
        "        if add_od_labels: assert op.isfile(self.label_file)\n",
        "        if is_train: assert op.isfile(self.caption_file) and tokenizer is not None\n",
        "\n",
        "        self.label_tsv = None if not self.label_file else TSVFile(self.label_file)\n",
        "        self.feat_tsv = TSVFile(self.feat_file)\n",
        "        \n",
        "        self.captions = []\n",
        "        if self.caption_file and op.isfile(self.caption_file):\n",
        "            if self.overfit == False:\n",
        "                with open(self.caption_file, 'r') as f:\n",
        "                    self.captions = json.load(f)\n",
        "            else:\n",
        "                with open(\"/content/drive/MyDrive/OscarPlus/datasets/coco_caption/test_caption_overfit.json\", 'r') as f:\n",
        "                    self.captions = json.load(f)                \n",
        "        # if self.overfit == True:\n",
        "        #     self.captions = self.captions[0:100]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tensorizer = CaptionTensorizer(self.tokenizer, max_img_seq_length,\n",
        "                max_seq_length, max_seq_a_length, mask_prob, max_masked_tokens,\n",
        "                is_train=is_train)\n",
        "        self.add_od_labels = add_od_labels\n",
        "        self.is_train = is_train\n",
        "        self.kwargs = kwargs\n",
        "        self.image_keys = self.prepare_image_keys()\n",
        "        self.key2index = self.prepare_image_key_to_index()\n",
        "        self.key2captions = self.prepare_image_key_to_captions()\n",
        "\n",
        "    def get_valid_tsv(self):\n",
        "        # based on the order of file size\n",
        "        if self.label_tsv:\n",
        "            return self.label_tsv\n",
        "        if self.feat_tsv:\n",
        "            return self.feat_tsv\n",
        "\n",
        "    def prepare_image_keys(self):\n",
        "        tsv = self.get_valid_tsv()\n",
        "        if self.overfit == True:\n",
        "            try:\n",
        "                return [tsv.seek(i)[0] for i in range(100)]\n",
        "            except Exception as e:\n",
        "                print(\"Original dataset smaller than 100!\")\n",
        "                raise\n",
        "        else:\n",
        "            return [tsv.seek(i)[0] for i in range(tsv.num_rows())]\n",
        "\n",
        "    def prepare_image_key_to_index(self):\n",
        "        tsv = self.get_valid_tsv()\n",
        "        if self.overfit == True:\n",
        "            try:\n",
        "                return {tsv.seek(i)[0] : i for i in range(100)}\n",
        "            except Exception as e:\n",
        "                print(\"Original dataset smaller than 100!\")\n",
        "                raise\n",
        "        else:\n",
        "            return {tsv.seek(i)[0] : i for i in range(tsv.num_rows())}\n",
        "\n",
        "    def prepare_image_key_to_captions(self):\n",
        "        if self.captions:\n",
        "            key2captions = {key: [] for key in self.image_keys}\n",
        "            for cap in self.captions:\n",
        "                key2captions[cap['image_id']].append(cap['caption'])\n",
        "            # for key in self.image_keys:\n",
        "            #     key2captions[key].append(cap['caption'])\n",
        "            return key2captions\n",
        "\n",
        "    def get_image_index(self, idx):\n",
        "        if self.is_train:\n",
        "            img_cap_pair = self.captions[idx]\n",
        "            img_key = img_cap_pair['image_id']\n",
        "            return self.key2index[img_key]\n",
        "        return idx\n",
        "\n",
        "    def get_image_key(self, idx):\n",
        "        img_idx = self.get_image_index(idx)\n",
        "        return self.image_keys[img_idx]\n",
        "\n",
        "    def get_image_features(self, img_idx):\n",
        "        feat_info = json.loads(self.feat_tsv.seek(img_idx)[1])\n",
        "        num_boxes = feat_info['num_boxes']\n",
        "        features = np.frombuffer(base64.b64decode(feat_info['features']), np.float32\n",
        "                ).reshape((num_boxes, -1))\n",
        "        return torch.Tensor(features)\n",
        "\n",
        "    def get_caption(self, idx):\n",
        "        if self.is_train:\n",
        "            img_cap_pair = self.captions[idx]\n",
        "            return img_cap_pair['caption']\n",
        "        return \"\"\n",
        "\n",
        "    def get_od_labels(self, img_idx):\n",
        "        od_labels = None\n",
        "        if self.add_od_labels:\n",
        "            label_info = json.loads(self.label_tsv.seek(img_idx)[1])\n",
        "            od_labels = \" \".join([l['class'] for l in label_info])\n",
        "        return od_labels\n",
        "\n",
        "    def get_caption_file_in_coco_format(self):\n",
        "        cap_file = op.splitext(self.caption_file)[0] + '_coco_format.json'\n",
        "        return cap_file\n",
        "\n",
        "    def get_captions_by_key(self, key):\n",
        "        return self.key2captions[key]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_idx = self.get_image_index(idx)\n",
        "        img_key = self.image_keys[img_idx]\n",
        "        features = self.get_image_features(img_idx)\n",
        "        caption = self.get_caption(idx)\n",
        "        od_labels = self.get_od_labels(img_idx)\n",
        "        example = self.tensorizer.tensorize_example(caption, features, text_b=od_labels)\n",
        "        return img_key, example\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.is_train or self.overfit:\n",
        "            return len(self.captions)\n",
        "        return self.get_valid_tsv().num_rows()\n",
        "\n",
        "\n",
        "class CaptionTSVDatasetWithConstraints(CaptionTSVDataset):\n",
        "    r\"\"\"\n",
        "    Providing inputs for inference with Constraint Beam Search\n",
        "\n",
        "    nms_threshold: float, optional (default = 0.85)\n",
        "        NMS threshold for suppressing generic object class names during constraint filtering,\n",
        "        for two boxes with IoU higher than this threshold, \"dog\" suppresses \"animal\".\n",
        "    max_given_constraints: int, optional (default = 3)\n",
        "        Maximum number of constraints which can be specified for CBS decoding. Constraints are\n",
        "        selected based on the prediction confidence score of their corresponding bounding boxes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, yaml_file,\n",
        "        nms_threshold=0.85,\n",
        "        max_given_constraints=3, **kwargs\n",
        "    ):\n",
        "        super().__init__(yaml_file, **kwargs)\n",
        "        boxes_tsvpath = find_file_path_in_yaml(self.cfg['cbs_box'], self.root)\n",
        "        constraint2tokens_tsvpath = find_file_path_in_yaml(self.cfg['cbs_constraint'], self.root)\n",
        "        tokenforms_tsvpath = find_file_path_in_yaml(self.cfg['cbs_tokenforms'], self.root)\n",
        "        hierarchy_jsonpath = find_file_path_in_yaml(self.cfg['cbs_hierarchy'], self.root)\n",
        "\n",
        "        self._boxes_reader = ConstraintBoxesReader(boxes_tsvpath)\n",
        "        self._constraint_filter = ConstraintFilter(\n",
        "            hierarchy_jsonpath, nms_threshold, max_given_constraints\n",
        "        )\n",
        "        self._fsm_builder = FiniteStateMachineBuilder(self.tokenizer,\n",
        "                constraint2tokens_tsvpath, tokenforms_tsvpath,\n",
        "                max_given_constraints)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_key, example = super().__getitem__(index)\n",
        "\n",
        "        # Apply constraint filtering to object class names.\n",
        "        constraint_boxes = self._boxes_reader[img_key]\n",
        "\n",
        "        candidates = self._constraint_filter(\n",
        "            constraint_boxes[\"boxes\"], constraint_boxes[\"class_names\"], constraint_boxes[\"scores\"]\n",
        "        )\n",
        "        num_constraints = len(candidates)\n",
        "        fsm, nstates = self._fsm_builder.build(candidates)\n",
        "\n",
        "        return img_key, example + (fsm, num_constraints, )\n",
        "\n",
        "\n",
        "class CaptionTensorizer(object):\n",
        "    def __init__(self, tokenizer, max_img_seq_length=50, max_seq_length=70, \n",
        "            max_seq_a_length=40, mask_prob=0.15, max_masked_tokens=3,\n",
        "            is_train=True):\n",
        "        \"\"\"Constructor.\n",
        "        Args:\n",
        "            tokenizer: tokenizer for text processing.\n",
        "            max_img_seq_length: max image sequence length.\n",
        "            max_seq_length: max text sequence length.\n",
        "            max_seq_a_length: max caption sequence length.\n",
        "            is_train: train or test mode.\n",
        "            mask_prob: probability to mask a input token.\n",
        "            max_masked_tokens: maximum number of tokens to be masked in one sentence.\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.is_train = is_train\n",
        "        self.max_img_seq_len = max_img_seq_length\n",
        "        self.max_seq_len = max_seq_length\n",
        "        self.max_seq_a_len = max_seq_a_length\n",
        "        self.mask_prob = mask_prob\n",
        "        self.max_masked_tokens = max_masked_tokens\n",
        "        self._triangle_mask = torch.tril(torch.ones((self.max_seq_len, \n",
        "            self.max_seq_len), dtype=torch.long))\n",
        "\n",
        "    def tensorize_example(self, text_a, img_feat, text_b=None,\n",
        "            cls_token_segment_id=0, pad_token_segment_id=0,\n",
        "            sequence_a_segment_id=0, sequence_b_segment_id=1):\n",
        "        if self.is_train:\n",
        "            tokens_a = self.tokenizer.tokenize(text_a)\n",
        "        else:\n",
        "            # fake tokens to generate masks\n",
        "            tokens_a = [self.tokenizer.mask_token] * (self.max_seq_a_len - 2)\n",
        "        if len(tokens_a) > self.max_seq_a_len - 2:\n",
        "            tokens_a = tokens_a[:(self.max_seq_a_len - 2)]\n",
        "\n",
        "        tokens = [self.tokenizer.cls_token] + tokens_a + [self.tokenizer.sep_token]\n",
        "        segment_ids = [cls_token_segment_id] + [sequence_a_segment_id] * (len(tokens) - 1)\n",
        "        seq_a_len = len(tokens)\n",
        "        if text_b:\n",
        "            # pad text_a to keep it in fixed length for better inference.\n",
        "            padding_a_len = self.max_seq_a_len - seq_a_len\n",
        "            tokens += [self.tokenizer.pad_token] * padding_a_len\n",
        "            segment_ids += ([pad_token_segment_id] * padding_a_len)\n",
        "\n",
        "            tokens_b = self.tokenizer.tokenize(text_b)\n",
        "            if len(tokens_b) > self.max_seq_len - len(tokens) - 1:\n",
        "                tokens_b = tokens_b[: (self.max_seq_len - len(tokens) - 1)]\n",
        "            tokens += tokens_b + [self.tokenizer.sep_token]\n",
        "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "        seq_len = len(tokens)\n",
        "        if self.is_train:\n",
        "            masked_pos = torch.zeros(self.max_seq_len, dtype=torch.int)\n",
        "            # randomly mask words for prediction, ignore [CLS]\n",
        "            candidate_masked_idx = list(range(1, seq_a_len)) # only mask text_a\n",
        "            random.shuffle(candidate_masked_idx)\n",
        "            num_masked = min(max(round(self.mask_prob * seq_a_len), 1), self.max_masked_tokens)\n",
        "            num_masked = int(num_masked)\n",
        "            masked_idx = candidate_masked_idx[:num_masked]\n",
        "            masked_idx = sorted(masked_idx)\n",
        "            masked_token = [tokens[i] for i in masked_idx]\n",
        "            for pos in masked_idx:\n",
        "                if random.random() <= 0.8:\n",
        "                    # 80% chance to be a ['MASK'] token\n",
        "                    tokens[pos] = self.tokenizer.mask_token\n",
        "                elif random.random() <= 0.5:\n",
        "                    # 10% chance to be a random word ((1-0.8)*0.5)\n",
        "                    from random import randint\n",
        "                    i = randint(0, len(self.tokenizer.vocab))\n",
        "                    self.tokenizer._convert_id_to_token(i)\n",
        "                    tokens[pos] = self.tokenizer._convert_id_to_token(i)\n",
        "                else:\n",
        "                    # 10% chance to remain the same (1-0.8-0.1)\n",
        "                    pass\n",
        "\n",
        "            masked_pos[masked_idx] = 1 \n",
        "            # pad masked tokens to the same length\n",
        "            if num_masked < self.max_masked_tokens:\n",
        "                masked_token = masked_token + ([self.tokenizer.pad_token] *\n",
        "                        (self.max_masked_tokens - num_masked))\n",
        "            masked_ids = self.tokenizer.convert_tokens_to_ids(masked_token)\n",
        "        else:\n",
        "            masked_pos = torch.ones(self.max_seq_len, dtype=torch.int)\n",
        "\n",
        "        # pad on the right for image captioning\n",
        "        padding_len = self.max_seq_len - seq_len\n",
        "        tokens = tokens + ([self.tokenizer.pad_token] * padding_len)\n",
        "        segment_ids += ([pad_token_segment_id] * padding_len)\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # image features\n",
        "        img_len = img_feat.shape[0]\n",
        "        if img_len > self.max_img_seq_len:\n",
        "            img_feat = img_feat[0 : self.max_img_seq_len, ]\n",
        "            img_len = img_feat.shape[0]\n",
        "        else:\n",
        "            padding_matrix = torch.zeros((self.max_img_seq_len - img_len,\n",
        "                                          img_feat.shape[1]))\n",
        "            img_feat = torch.cat((img_feat, padding_matrix), 0)\n",
        "\n",
        "        # prepare attention mask:\n",
        "        # note that there is no attention from caption to image\n",
        "        # because otherwise it will violate the triangle attention \n",
        "        # for caption as caption will have full attention on image. \n",
        "        max_len = self.max_seq_len + self.max_img_seq_len\n",
        "        attention_mask = torch.zeros((max_len, max_len), dtype=torch.long)\n",
        "        # C: caption, L: label, R: image region\n",
        "        c_start, c_end = 0, seq_a_len\n",
        "        l_start, l_end = self.max_seq_a_len, seq_len\n",
        "        r_start, r_end = self.max_seq_len, self.max_seq_len + img_len\n",
        "        # triangle mask for caption to caption\n",
        "        attention_mask[c_start : c_end, c_start : c_end].copy_(self._triangle_mask[0 : seq_a_len, 0 : seq_a_len])\n",
        "        # full attention for L-L, R-R\n",
        "        attention_mask[l_start : l_end, l_start : l_end] = 1\n",
        "        attention_mask[r_start : r_end, r_start : r_end] = 1\n",
        "        # full attention for C-L, C-R\n",
        "        attention_mask[c_start : c_end, l_start : l_end] = 1\n",
        "        attention_mask[c_start : c_end, r_start : r_end] = 1\n",
        "        # full attention for L-R:\n",
        "        attention_mask[l_start : l_end, r_start : r_end] = 1\n",
        "        attention_mask[r_start : r_end, l_start : l_end] = 1\n",
        "\n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "        segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
        "\n",
        "        if self.is_train:\n",
        "            masked_ids = torch.tensor(masked_ids, dtype=torch.long)\n",
        "            return (input_ids, attention_mask, segment_ids, img_feat, masked_pos, masked_ids)\n",
        "        return (input_ids, attention_mask, segment_ids, img_feat, masked_pos)\n",
        "\n",
        "\n",
        "def build_dataset(yaml_file, tokenizer, args, is_train=True):\n",
        "    if not op.isfile(yaml_file):\n",
        "        yaml_file = op.join(args.data_dir, yaml_file)\n",
        "        assert op.isfile(yaml_file)\n",
        "\n",
        "    if is_train:\n",
        "        return CaptionTSVDataset(yaml_file, tokenizer=tokenizer,\n",
        "            add_od_labels=args.add_od_labels, max_img_seq_length=args.max_img_seq_length,\n",
        "            max_seq_length=args.max_seq_length, max_seq_a_length=args.max_seq_a_length,\n",
        "            is_train=True, mask_prob=args.mask_prob, max_masked_tokens=args.max_masked_tokens)\n",
        "    if args.use_cbs:\n",
        "        dataset_class = CaptionTSVDatasetWithConstraints\n",
        "    else:\n",
        "        dataset_class = CaptionTSVDataset\n",
        "    return dataset_class(yaml_file, tokenizer=tokenizer,\n",
        "            add_od_labels=args.add_od_labels, max_img_seq_length=args.max_img_seq_length,\n",
        "            max_seq_length=args.max_seq_length, max_seq_a_length=args.max_gen_length,\n",
        "            is_train=False, overfit=args.overfit)\n",
        "\n",
        "\n",
        "def make_data_sampler(dataset, shuffle, distributed):\n",
        "    if distributed:\n",
        "        return torch.utils.data.distributed.DistributedSampler(dataset, shuffle=shuffle)\n",
        "    if shuffle:\n",
        "        sampler = torch.utils.data.sampler.RandomSampler(dataset)\n",
        "    else:\n",
        "        sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n",
        "    return sampler\n",
        "\n",
        "\n",
        "def make_data_loader(args, yaml_file, tokenizer, is_distributed=True, \n",
        "        is_train=True):\n",
        "    dataset = build_dataset(yaml_file, tokenizer, args, \n",
        "        is_train=(is_train and not args.scst))\n",
        "    if is_train:\n",
        "        shuffle = True\n",
        "        images_per_gpu = args.per_gpu_train_batch_size\n",
        "        images_per_batch = images_per_gpu * get_world_size()\n",
        "        iters_per_batch = len(dataset) // images_per_batch\n",
        "        num_iters = iters_per_batch * args.num_train_epochs\n",
        "        logger.info(\"Train with {} images per GPU.\".format(images_per_gpu))\n",
        "        logger.info(\"Total batch size {}\".format(images_per_batch))\n",
        "        logger.info(\"Total training steps {}\".format(num_iters))\n",
        "    else:\n",
        "        shuffle = False\n",
        "        images_per_gpu = args.per_gpu_eval_batch_size\n",
        "\n",
        "    sampler = make_data_sampler(dataset, shuffle, is_distributed)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, num_workers=args.num_workers, sampler=sampler,\n",
        "        batch_size=images_per_gpu,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "def save_checkpoint(model, tokenizer, args, epoch, iteration, num_trial=10):\n",
        "    checkpoint_dir = op.join(args.output_dir, 'checkpoint-{}-{}'.format(\n",
        "        epoch, iteration))\n",
        "    if not is_main_process():\n",
        "        return checkpoint_dir\n",
        "    mkdir(checkpoint_dir)\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    for i in range(num_trial):\n",
        "        try:\n",
        "            model_to_save.save_pretrained(checkpoint_dir)\n",
        "            torch.save(args, op.join(checkpoint_dir, 'training_args.bin'))\n",
        "            tokenizer.save_pretrained(checkpoint_dir)\n",
        "            logger.info(\"Save checkpoint to {}\".format(checkpoint_dir))\n",
        "            break\n",
        "        except:\n",
        "            pass\n",
        "    else:\n",
        "        logger.info(\"Failed to save checkpoint after {} trails.\".format(num_trial))\n",
        "    return checkpoint_dir\n",
        "\n",
        "\n",
        "def compute_score_with_logits(logits, labels):\n",
        "    logits = torch.max(logits, -1)[1].data # argmax\n",
        "    scores = logits == labels \n",
        "    return scores\n",
        "\n",
        "\n",
        "def train(args, train_dataloader, val_dataset, model, tokenizer):\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], \n",
        "            output_device=args.local_rank,\n",
        "            find_unused_parameters=True,\n",
        "        )\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // \\\n",
        "                args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps \\\n",
        "                * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and scheduler\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not \\\n",
        "            any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if \\\n",
        "            any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    if args.scheduler == \"constant\":\n",
        "        scheduler = WarmupConstantSchedule(\n",
        "                optimizer, warmup_steps=args.warmup_steps)\n",
        "    elif args.scheduler == \"linear\":\n",
        "        scheduler = WarmupLinearSchedule(\n",
        "                optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown scheduler type: {}\".format(args.scheduler))\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\"  Total train batch size (w. parallel, & accumulation) = %d\",\n",
        "                   args.per_gpu_train_batch_size * get_world_size() * args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    if args.scst:\n",
        "        scst_criterion = ScstRewardCriterion(\n",
        "            cider_cached_tokens=op.join(args.data_dir, args.cider_cached_tokens),\n",
        "            baseline_type=args.sc_baseline_type,\n",
        "        )\n",
        "        logger.info(\"  SCST training...\")\n",
        "\n",
        "\n",
        "    global_step, global_loss, global_acc =0,  0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    eval_log = []\n",
        "    best_score = 0\n",
        "    for epoch in range(int(args.num_train_epochs)):\n",
        "        for step, (img_keys, batch) in enumerate(train_dataloader):\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            if not args.scst:\n",
        "                model.train()\n",
        "                inputs = {'input_ids': batch[0], 'attention_mask': batch[1],\n",
        "                    'token_type_ids': batch[2], 'img_feats': batch[3], \n",
        "                    'masked_pos': batch[4], 'masked_ids': batch[5]\n",
        "                }\n",
        "                outputs = model(**inputs)\n",
        "                loss, logits = outputs[:2]\n",
        "                masked_ids = inputs['masked_ids']\n",
        "                masked_ids = masked_ids[masked_ids != 0]\n",
        "                batch_score = compute_score_with_logits(logits, masked_ids)\n",
        "                batch_acc = torch.sum(batch_score.float()) / torch.sum(inputs['masked_pos'])\n",
        "            else:\n",
        "                loss = scst_train_iter(args, train_dataloader, model, scst_criterion, img_keys, batch, tokenizer)\n",
        "                batch_acc = scst_criterion.get_score()\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "            global_loss += loss.item()\n",
        "            global_acc += batch_acc\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                global_step += 1\n",
        "                scheduler.step()\n",
        "                optimizer.step()\n",
        "                model.zero_grad()\n",
        "                if global_step % args.logging_steps == 0:\n",
        "                    logger.info(\"Epoch: {}, global_step: {}, lr: {:.6f}, loss: {:.4f} ({:.4f}), \" \\\n",
        "                        \"score: {:.4f} ({:.4f})\".format(epoch, global_step, \n",
        "                        optimizer.param_groups[0][\"lr\"], loss, global_loss / global_step, \n",
        "                        batch_acc, global_acc / global_step)\n",
        "                    )\n",
        "\n",
        "                if (args.save_steps > 0 and global_step % args.save_steps == 0) or \\\n",
        "                        global_step == t_total:\n",
        "                    checkpoint_dir = save_checkpoint(model, tokenizer, args, epoch, global_step) \n",
        "                    # evaluation\n",
        "                    if args.evaluate_during_training: \n",
        "                        logger.info(\"Perform evaluation at step: %d\" % (global_step))\n",
        "                        evaluate_file = evaluate(args, val_dataset, model, tokenizer,\n",
        "                                checkpoint_dir)\n",
        "                        with open(evaluate_file, 'r') as f:\n",
        "                            res = json.load(f)\n",
        "                        best_score = max(best_score, res['CIDEr'])\n",
        "                        res['epoch'] = epoch\n",
        "                        res['global_step'] = step\n",
        "                        res['best_CIDEr'] = best_score\n",
        "                        eval_log.append(res)\n",
        "                        with open(args.output_dir + '/eval_logs.json', 'w') as f:\n",
        "                            json.dump(eval_log, f)\n",
        "    return checkpoint_dir\n",
        "\n",
        "\n",
        "def scst_train_iter(args, train_dataloader, model, scst_criterion, \n",
        "        img_keys, batch, tokenizer):\n",
        "    cls_token_id, sep_token_id, pad_token_id, mask_token_id = \\\n",
        "        tokenizer.convert_tokens_to_ids([tokenizer.cls_token, \n",
        "        tokenizer.sep_token, tokenizer.pad_token, tokenizer.mask_token]\n",
        "    )\n",
        "    inputs = {'is_decode': True,\n",
        "        'input_ids': batch[0], 'attention_mask': batch[1],\n",
        "        'token_type_ids': batch[2], 'img_feats': batch[3],\n",
        "        'masked_pos': batch[4],\n",
        "        'do_sample': False,\n",
        "        'bos_token_id': cls_token_id,\n",
        "        'pad_token_id': pad_token_id,\n",
        "        'eos_token_ids': [sep_token_id],\n",
        "        'mask_token_id': mask_token_id,\n",
        "        # for adding od labels\n",
        "        'add_od_labels': args.add_od_labels, 'od_labels_start_posid': args.max_seq_a_length,\n",
        "        # hyperparameters of beam search\n",
        "        'max_length': args.max_gen_length,\n",
        "        'num_beams': args.sc_beam_size,\n",
        "        \"temperature\": args.temperature,\n",
        "        \"top_k\": args.top_k,\n",
        "        \"top_p\": args.top_p,\n",
        "        \"repetition_penalty\": args.repetition_penalty,\n",
        "        \"length_penalty\": args.length_penalty,\n",
        "        \"num_return_sequences\": 1,\n",
        "        \"num_keep_best\": 1,\n",
        "    }\n",
        "\n",
        "    def _ids_to_captions(all_ids):\n",
        "        captions = []\n",
        "        for ids in all_ids:\n",
        "            c = tokenizer.decode(ids.tolist(), skip_special_tokens=True)\n",
        "            captions.append(c)\n",
        "        return captions\n",
        "\n",
        "    if args.sc_baseline_type == 'greedy':\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            greedy_res_raw, _ = model(**inputs)\n",
        "            greedy_res_raw.squeeze_(1)  # batch_size * max_len\n",
        "        greedy_res = _ids_to_captions(greedy_res_raw)\n",
        "    else:\n",
        "        greedy_res = None\n",
        "\n",
        "    model.train()\n",
        "    inputs['do_sample'] = True\n",
        "    inputs['num_return_sequences'] = args.sc_train_sample_n\n",
        "    sample_res_raw, sample_logprobs = model(**inputs)\n",
        "    sample_res_raw.squeeze_(1)\n",
        "    sample_logprobs.squeeze_(1)\n",
        "    assert sample_logprobs.requires_grad == True\n",
        "    assert sample_res_raw.requires_grad == False\n",
        "    sample_res = _ids_to_captions(sample_res_raw)\n",
        "\n",
        "    gt_res = [train_dataloader.dataset.get_captions_by_key(k) for k in img_keys]\n",
        "    loss = scst_criterion(gt_res, greedy_res, sample_res, sample_logprobs)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def  get_predict_file(output_dir, yaml_file, args):\n",
        "    cc = ['pred']\n",
        "    # make sure it works with/without / in end of the path.\n",
        "    data = op.basename(op.join(args.data_dir, '')[:-1])\n",
        "    split = op.basename(yaml_file)\n",
        "    assert split.endswith('.yaml')\n",
        "    split = split[:-5]\n",
        "    cc.append(data)\n",
        "    cc.append(split)\n",
        "    cc.append('beam{}'.format(args.num_beams))\n",
        "    cc.append('max{}'.format(args.max_gen_length))\n",
        "    if args.add_od_labels:\n",
        "        cc.append('odlabels')\n",
        "    if args.num_keep_best != 1:\n",
        "        cc.append('best{}'.format(args.num_keep_best))\n",
        "    if args.use_cbs:\n",
        "        cc.append('cbs{}'.format(args.min_constraints_to_satisfy))\n",
        "    if args.output_hidden_states:\n",
        "        cc.append('hidden')\n",
        "    return op.join(output_dir, '{}.tsv'.format('.'.join(cc)))\n",
        "\n",
        "\n",
        "def get_evaluate_file(predict_file):\n",
        "    assert predict_file.endswith('.tsv')\n",
        "    fpath = op.splitext(predict_file)[0]\n",
        "    return fpath + '.eval.json'\n",
        "\n",
        "\n",
        "def get_evaluate_method(predict_file):\n",
        "    if 'nocaps' in op.basename(predict_file):\n",
        "        return 'nocaps'\n",
        "    else:\n",
        "        return 'coco'\n",
        "\n",
        "\n",
        "def evaluate(args, val_dataloader, model, tokenizer, output_dir):\n",
        "    predict_file = get_predict_file(output_dir,\n",
        "            val_dataloader.dataset.yaml_file, args)\n",
        "    test(args, val_dataloader, model, tokenizer, predict_file)\n",
        "\n",
        "    if get_world_size() > 1:\n",
        "        torch.distributed.barrier()\n",
        "    evaluate_file = get_evaluate_file(predict_file)\n",
        "    if is_main_process():\n",
        "        caption_file = val_dataloader.dataset.get_caption_file_in_coco_format()\n",
        "        data = val_dataloader.dataset.yaml_file.split('/')[-2]\n",
        "        if 'nocaps' not in data:\n",
        "            result = evaluate_on_coco_caption(predict_file, caption_file, outfile=evaluate_file)\n",
        "            logger.info('evaluation result: {}'.format(str(result)))\n",
        "            logger.info('evaluation result saved to {}'.format(evaluate_file))\n",
        "    if get_world_size() > 1:\n",
        "        torch.distributed.barrier()\n",
        "    return evaluate_file\n",
        "\n",
        "\n",
        "def test(args, test_dataloader, model, tokenizer, predict_file):\n",
        "    cls_token_id, sep_token_id, pad_token_id, mask_token_id, period_token_id = \\\n",
        "        tokenizer.convert_tokens_to_ids([tokenizer.cls_token, tokenizer.sep_token, \n",
        "        tokenizer.pad_token, tokenizer.mask_token, '.'])\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        cache_file = predict_file\n",
        "    else:\n",
        "        cache_file = op.splitext(predict_file)[0] + '_{}_{}'.format(get_rank(), \n",
        "                world_size) + op.splitext(predict_file)[1]\n",
        "\n",
        "    model.eval()\n",
        "    inputs_param = {'is_decode': True,\n",
        "        'do_sample': False,\n",
        "        'bos_token_id': cls_token_id,\n",
        "        'pad_token_id': pad_token_id,\n",
        "        'eos_token_ids': [sep_token_id],\n",
        "        'mask_token_id': mask_token_id,\n",
        "        # for adding od labels\n",
        "        'add_od_labels': args.add_od_labels, 'od_labels_start_posid': args.max_seq_a_length,\n",
        "\n",
        "        # hyperparameters of beam search\n",
        "        'max_length': args.max_gen_length,\n",
        "        'num_beams': args.num_beams,\n",
        "        \"temperature\": args.temperature,\n",
        "        \"top_k\": args.top_k,\n",
        "        \"top_p\": args.top_p,\n",
        "        \"repetition_penalty\": args.repetition_penalty,\n",
        "        \"length_penalty\": args.length_penalty,\n",
        "        \"num_return_sequences\": args.num_return_sequences,\n",
        "        \"num_keep_best\": args.num_keep_best,\n",
        "    }\n",
        "    if args.use_cbs:\n",
        "        inputs_param.update({'use_cbs': True,\n",
        "            'min_constraints_to_satisfy': args.min_constraints_to_satisfy,\n",
        "        })\n",
        "    def gen_rows():\n",
        "        time_meter = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step, (img_keys, batch) in tqdm(enumerate(test_dataloader)):\n",
        "                batch = tuple(t.to(args.device) for t in batch)\n",
        "                inputs = {\n",
        "                    'input_ids': batch[0], 'attention_mask': batch[1],\n",
        "                    'token_type_ids': batch[2], 'img_feats': batch[3],\n",
        "                    'masked_pos': batch[4],\n",
        "                }\n",
        "                if args.use_cbs:\n",
        "                    inputs.update({\n",
        "                        'fsm': batch[5],\n",
        "                        'num_constraints': batch[6],\n",
        "                    })\n",
        "                inputs.update(inputs_param)\n",
        "                tic = time.time()\n",
        "                # captions, logprobs\n",
        "                outputs = model(**inputs)\n",
        "                time_meter += time.time() - tic\n",
        "                all_caps = outputs[0]  # batch_size * num_keep_best * max_len\n",
        "                all_confs = torch.exp(outputs[1])\n",
        "\n",
        "                for img_key, caps, confs in zip(img_keys, all_caps, all_confs):\n",
        "                    res = []\n",
        "                    for cap, conf in zip(caps, confs):\n",
        "                        cap = tokenizer.decode(cap.tolist(), skip_special_tokens=True)\n",
        "                        res.append({'caption': cap, 'conf': conf.item()})\n",
        "                    if isinstance(img_key, torch.Tensor):\n",
        "                        img_key = img_key.item()\n",
        "                    yield img_key, json.dumps(res)\n",
        "\n",
        "        logger.info(\"Inference model computing time: {} seconds per batch\".format(time_meter / (step+1)))\n",
        "\n",
        "    tsv_writer(gen_rows(), cache_file)\n",
        "    if world_size > 1:\n",
        "        torch.distributed.barrier()\n",
        "    if world_size > 1 and is_main_process():\n",
        "        cache_files = [op.splitext(predict_file)[0] + '_{}_{}'.format(i, world_size) + \\\n",
        "            op.splitext(predict_file)[1] for i in range(world_size)]\n",
        "        concat_tsv_files(cache_files, predict_file)\n",
        "        delete_tsv_files(cache_files)\n",
        "        reorder_tsv_keys(predict_file, test_dataloader.dataset.image_keys, predict_file)\n",
        "    if world_size > 1:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "\n",
        "def restore_training_settings(args):\n",
        "    if args.do_train:\n",
        "        if not args.scst:\n",
        "            return args\n",
        "        checkpoint = args.model_name_or_path\n",
        "    else:\n",
        "        assert args.do_test or args.do_eval\n",
        "        checkpoint = args.eval_model_dir\n",
        "    # restore training settings, check hasattr for backward compatibility\n",
        "    print(op.join(checkpoint, 'training_args.bin'))\n",
        "    train_args = torch.load(op.join(checkpoint, 'training_args.bin'))\n",
        "\n",
        "    \n",
        "\n",
        "    if hasattr(train_args, 'max_seq_a_length'):\n",
        "        if hasattr(train_args, 'scst') and train_args.scst:\n",
        "            max_od_labels_len = train_args.max_seq_length - train_args.max_gen_length\n",
        "        else:\n",
        "            max_od_labels_len = train_args.max_seq_length - train_args.max_seq_a_length\n",
        "        max_seq_length = args.max_gen_length + max_od_labels_len\n",
        "        args.max_seq_length = max_seq_length\n",
        "        logger.warning('Override max_seq_length to {} = max_gen_length:{} + od_labels_len:{}'.format(\n",
        "                max_seq_length, args.max_gen_length, max_od_labels_len))\n",
        "\n",
        "\n",
        "    override_params = ['max_seq_a_length', 'do_lower_case', 'add_od_labels',\n",
        "            'max_img_seq_length']\n",
        "    for param in override_params:\n",
        "        if hasattr(train_args, param):\n",
        "            train_v = getattr(train_args, param)\n",
        "            test_v = getattr(args, param)\n",
        "            if train_v != test_v:\n",
        "                logger.warning('Override {} with train args: {} -> {}'.format(param,\n",
        "                    test_v, train_v))\n",
        "                setattr(args, param, train_v)\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not dist.is_available():\n",
        "        return 1\n",
        "    if not dist.is_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not dist.is_available():\n",
        "        return 0\n",
        "    if not dist.is_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def synchronize():\n",
        "    \"\"\"\n",
        "    Helper function to synchronize (barrier) among all processes when\n",
        "    using distributed training\n",
        "    \"\"\"\n",
        "    if not dist.is_available():\n",
        "        return\n",
        "    if not dist.is_initialized():\n",
        "        return\n",
        "    world_size = dist.get_world_size()\n",
        "    if world_size == 1:\n",
        "        return\n",
        "    dist.barrier()\n",
        "\n",
        "\n",
        "def ensure_init_process_group(local_rank=None, port=12345):\n",
        "    # init with env\n",
        "    world_size = int(os.environ['WORLD_SIZE']) if 'WORLD_SIZE' in os.environ else 1\n",
        "    if world_size > 1 and not dist.is_initialized():\n",
        "        assert local_rank is not None\n",
        "        print(\"Init distributed training on local rank {}\".format(local_rank))\n",
        "        torch.cuda.set_device(local_rank)\n",
        "        dist.init_process_group(\n",
        "            backend='nccl', init_method='env://'\n",
        "        )\n",
        "    return local_rank\n",
        "\n",
        "class Arguments(object):\n",
        "  pass\n",
        "\n",
        "def main():\n",
        "    args = Arguments()\n",
        "    args.data_dir = 'datasets/coco_caption' # The input data dir with all required files.\n",
        "    args.train_yaml = 'train.yaml' # yaml file for training.\n",
        "    args.test_yaml = 'test.yaml' # yaml file for testing.\n",
        "    args.val_yaml = 'val.yaml' # yaml file used for validation during training.\n",
        "    args.model_name_or_path = None # Path to pre-trained model or model type.\n",
        "    args.output_dir = 'output/'# The output directory to save checkpoint and test results.\n",
        "    args.loss_type = 'sfmx'# Loss function types: support kl, x2, sfmx\n",
        "    args.config_name = \"\", # Pretrained config name or path if not the same as model_name.\n",
        "    args.tokenizer_name = \"\" # Pretrained tokenizer name or path if not the same as model_name.\n",
        "    args.max_seq_length = 70 # The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
        "    args.max_seq_a_length = 40 #The maximum sequence length for caption.\n",
        "    args.do_train = False # Whether to run training.\n",
        "    args.do_test = False # Whether to run inference.\n",
        "    args.do_eval = False # Whether to run evaluation\n",
        "    args.do_lower_case = False # Set this flag if you are using an uncased model.\n",
        "    args.mask_prob = 0.15 # Probability to mask input sentence during training\n",
        "    args.max_masked_tokens = 3 # The max number of masked tokens per sentence.\n",
        "    args.add_od_labels = False # Whether to add object detection labels or not\n",
        "    args.drop_out = 0.1 # Drop out in BERT\n",
        "\n",
        "    args.max_img_seq_length = 50 # The maximum total input image sequence length.\")\n",
        "    args.img_feature_dim = 2054 # The Image Feature Dimension.\")\n",
        "    args.img_feature_type= 'frcnn' #Image feature type.\")\n",
        "    args.tie_weights = False # Whether to tie decoding weights to that of encoding\")\n",
        "    args.freeze_embedding = False # Whether to freeze word embeddings in Bert\")\n",
        "    args.label_smoothing = 0\n",
        "    args.drop_worst_ratio = 0\n",
        "    args.drop_worst_after = 0\n",
        "    args.per_gpu_train_batch_size = 64 # Batch size per GPU/CPU for training.\")\n",
        "    args.per_gpu_eval_batch_size = 64 # Batch size per GPU/CPU for evaluation.\")\n",
        "    args.output_mode = 'classification' # output mode, support classification or regression.\")\n",
        "    args.num_labels = 2 # num_labels is 2 for classification and 1 for regression.\")\n",
        "    args.gradient_accumulation_steps = 1 # Number of updates steps to accumulate before backward.\")\n",
        "    args.learning_rate = 3e-5 # The initial lr.\")\n",
        "    args.weight_decay = 0.05 # Weight deay.\")\n",
        "    args.adam_epsilon = 1e-8 # Epsilon for Adam.\")\n",
        "    args.max_grad_norm = 1.0 # Max gradient norm.\")\n",
        "    args.warmup_steps = 0 # Linear warmup.\")\n",
        "    args.scheduler ='linear' # constant or linear or\")\n",
        "    args.num_workers = 4 # Workers in dataloader.\")\n",
        "    args.num_train_epochs = 40 # Total number of training epochs to perform.\")\n",
        "\n",
        "    args.max_steps = -1 # \"Total number of training steps. Override num_train_epochs.\")\n",
        "    args.logging_steps = 20 # \"Log every X steps.\")\n",
        "\n",
        "    args.save_steps = -1 # \"Save checkpoint every X steps. Will also perform evaluatin.\")\n",
        "    args.evaluate_during_training = False # Run evaluation during training at each save_steps.\")\n",
        "    args.no_cuda = False # Avoid using CUDA.\")\n",
        "    args.local_rank = 0 # For distributed training.\")\n",
        "    args.seed = 88 # random seed for initialization.\")\n",
        "    # for self-critical sequence training\n",
        "    args.scst = False # Self-critical sequence training')\n",
        "    args.sc_train_sample_n = 5 # \"number of sampled captions for sc training\")\n",
        "    args.sc_baseline_type = 'greedy' # \"baseline tyep of REINFORCE algorithm\")\n",
        "    args.sc_beam_size = 1 # beam size for scst training\")\n",
        "    args.cider_cached_tokens = 'coco-train-words.p' #path to cached cPickle file used to calculate CIDEr scores\")\n",
        "    # for generation\n",
        "    args.eval_model_dir = '' # \"Model directory for evaluation.\")\n",
        "    args.max_gen_length = 20 # \"max length of generated sentences\")\n",
        "    args.output_hidden_states = False # \"Turn on for fast decoding\")\n",
        "    args.num_return_sequences = 1 # repeating times per image\")\n",
        "    args.num_beams = 1 # beam search width\")\n",
        "    args.num_keep_best = 1 # number of hypotheses to keep in beam search\")\n",
        "    args.temperature = 1 # \"temperature in softmax for sampling\")\n",
        "    args.top_k = 0 # \"filter distribution for sampling\")\n",
        "    args.top_p = 1 # filter distribution for sampling\")\n",
        "    args.repetition_penalty = 1 # \"repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)\")\n",
        "    args.length_penalty = 1 # beam search length penalty\")\n",
        "    # for Constrained Beam Search\n",
        "    args.use_cbs = False # Use constrained beam search for decoding')\n",
        "    args.min_constraints_to_satisfy = 2 # minimum number of constraints to satisfy\")\n",
        "\n",
        "    args.overfit = False\n",
        "\n",
        "\n",
        "    global logger\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    local_rank = ensure_init_process_group(local_rank=args.local_rank)\n",
        "    args.local_rank = local_rank\n",
        "    args.num_gpus = get_world_size()\n",
        "    args.distributed = args.num_gpus > 1\n",
        "    args.device = torch.device('cuda')\n",
        "    args.data_dir = \"/content/drive/MyDrive/OscarPlus/datasets/coco_caption\"\n",
        "\n",
        "    # Setup custom arguments at here, this is for python notebook compatability\n",
        "    args.do_test = True\n",
        "    args.do_eval = True\n",
        "    args.test_yaml = \"test.yaml\"\n",
        "    args.num_beams = 5\n",
        "    args.max_gen_length = 20\n",
        "    args.eval_model_dir = \"/content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000\"\n",
        "    args.add_od_labels = True\n",
        "\n",
        "    # use overfit dataset\n",
        "    # args.overfit = True\n",
        "\n",
        "    # fine-tune setup arguments\n",
        "\n",
        "    # args.model_name_or_path = \"/content/drive/MyDrive/OscarPlus/weights/checkpoint0\"\n",
        "    # args.do_train = True\n",
        "    # args.do_lower_case = True\n",
        "    # # bug\n",
        "    # args.evaluate_during_training = False\n",
        "    # args.num_train_epochs = 30\n",
        "    # args.save_steps = 2000\n",
        "    # args.output_dir = \"/content/drive/MyDrive/OscarPlus/output/checkpoint3\"\n",
        "    # args.data_dir = \"/content/drive/MyDrive/OscarPlus/datasets/coco_caption\"\n",
        "    # args.train_yaml = 'train.yaml' \n",
        "    # args.test_yaml = 'test.yaml' \n",
        "    # args.val_yaml = 'val.yaml'\n",
        "\n",
        "    print(type(args.model_name_or_path))\n",
        "\n",
        "    synchronize()\n",
        "\n",
        "    output_dir = args.output_dir\n",
        "    mkdir(output_dir)\n",
        "\n",
        "    logger = setup_logger(\"vlpretrain\", output_dir, args.local_rank)\n",
        "    logger.warning(\"Device: %s, n_gpu: %s\", args.device, args.num_gpus)\n",
        "    set_seed(args.seed, args.num_gpus)\n",
        "    args = restore_training_settings(args)\n",
        "\n",
        "    # ying!\n",
        "    args.add_od_labels = True\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config_class, model_class, tokenizer_class = BertConfig, BertForImageCaptioning, BertTokenizer\n",
        "    if args.do_train:\n",
        "        assert args.model_name_or_path is not None\n",
        "        config = config_class.from_pretrained(args.model_name_or_path, num_labels=args.num_labels, finetuning_task='image_captioning')\n",
        "        if args.scst:\n",
        "            # avoid using too much memory\n",
        "            config.output_hidden_states = True\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name \\\n",
        "                else args.model_name_or_path, do_lower_case=args.do_lower_case)\n",
        "        config.img_feature_dim = args.img_feature_dim\n",
        "        config.img_feature_type = args.img_feature_type\n",
        "        config.hidden_dropout_prob = args.drop_out\n",
        "        config.loss_type = args.loss_type\n",
        "        config.tie_weights = args.tie_weights\n",
        "        config.freeze_embedding = args.freeze_embedding\n",
        "        config.label_smoothing = args.label_smoothing\n",
        "        config.drop_worst_ratio = args.drop_worst_ratio\n",
        "        config.drop_worst_after = args.drop_worst_after\n",
        "        model = model_class.from_pretrained(args.model_name_or_path,\n",
        "                from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n",
        "    else:\n",
        "        checkpoint = args.eval_model_dir\n",
        "        assert op.isdir(checkpoint)\n",
        "        config = config_class.from_pretrained(checkpoint)\n",
        "        config.output_hidden_states = args.output_hidden_states\n",
        "        tokenizer = tokenizer_class.from_pretrained(checkpoint)\n",
        "        logger.info(\"Evaluate the following checkpoint: %s\", checkpoint)\n",
        "        model = model_class.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "    if args.no_cuda:\n",
        "        pass\n",
        "    else:\n",
        "        model.to(args.device)\n",
        "        \n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "    if args.do_train:\n",
        "        train_dataloader = make_data_loader(args, args.train_yaml, tokenizer,\n",
        "            args.distributed, is_train=True)\n",
        "        val_dataloader = None\n",
        "        if args.evaluate_during_training:\n",
        "            val_dataloader = make_data_loader(args, args.val_yaml, tokenizer,\n",
        "                args.distributed, is_train=False)\n",
        "        last_checkpoint = train(args, train_dataloader, val_dataloader, model, tokenizer)\n",
        "\n",
        "        # test the last checkpoint after training\n",
        "        if args.do_test:\n",
        "            logger.info(\"Evaluate on dataset: \" + args.test_yaml)\n",
        "            test_dataloader = make_data_loader(args, args.test_yaml, \n",
        "                tokenizer, args.distributed, is_train=False)\n",
        "            evaluate(args, test_dataloader, model, tokenizer, last_checkpoint)\n",
        "\n",
        "    # inference and evaluation\n",
        "    elif args.do_test or args.do_eval:\n",
        "        logger.info(\"Evaluate on dataset: \" + args.test_yaml)\n",
        "        test_dataloader = make_data_loader(args, args.test_yaml,\n",
        "            tokenizer, args.distributed, is_train=False)\n",
        "\n",
        "        if not args.do_eval:\n",
        "            predict_file = get_predict_file(checkpoint, test_dataloader.dataset.yaml_file, args)\n",
        "            test(args, test_dataloader, model, tokenizer, predict_file)\n",
        "            logger.info(\"Prediction results saved to: {}\".format(predict_file))\n",
        "        else:\n",
        "            try:\n",
        "              evaluate_file = evaluate(args, test_dataloader, model, tokenizer,\n",
        "                    checkpoint)\n",
        "            except BrokenPipeError:\n",
        "              pass\n",
        "            logger.info(\"Evaluation results saved to: {}\".format(evaluate_file))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'NoneType'>\n",
            "2021-12-09 03:09:56,917 vlpretrain WARNING: Device: cuda, n_gpu: 1\n",
            "/content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000/training_args.bin\n",
            "2021-12-09 03:09:58,646 vlpretrain WARNING: Override max_seq_length to 50 = max_gen_length:20 + od_labels_len:30\n",
            "2021-12-09 03:09:58,647 vlpretrain WARNING: Override do_lower_case with train args: False -> True\n",
            "2021-12-09 03:09:58,649 vlpretrain WARNING: Override add_od_labels with train args: True -> False\n",
            "2021-12-09 03:10:00,592 vlpretrain INFO: Evaluate the following checkpoint: /content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000\n",
            "2021-12-09 03:10:21,208 vlpretrain INFO: Training/evaluation parameters <__main__.Arguments object at 0x7f6342175050>\n",
            "2021-12-09 03:10:21,210 vlpretrain INFO: Evaluate on dataset: test.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/content/drive/MyDrive/OscarPlus/oscar/modeling/modeling_utils.py:506: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beam_id = idx // vocab_size\n",
            "79it [25:27, 19.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-09 03:35:52,604 vlpretrain INFO: Inference model computing time: 19.28908288931545 seconds per batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "INFO:vlpretrain:Inference model computing time: 19.28908288931545 seconds per batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "0:00:01.070275\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...     \n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "tokenization...\n",
            "setting up scorers...\n",
            "computing Bleu score...\n",
            "{'testlen': 52669, 'reflen': 51042, 'guess': [52669, 47669, 42669, 37669], 'correct': [38961, 21626, 11177, 5793]}\n",
            "ratio: 1.0318757101994234\n",
            "Bleu_1: 0.740\n",
            "Bleu_2: 0.579\n",
            "Bleu_3: 0.445\n",
            "Bleu_4: 0.341\n",
            "computing METEOR score...\n",
            "METEOR: 0.290\n",
            "computing Rouge score...\n",
            "ROUGE_L: 0.568\n",
            "computing CIDEr score...\n",
            "CIDEr: 1.157\n",
            "computing SPICE score...\n",
            "SPICE: 0.221\n",
            "2021-12-09 03:37:45,843 vlpretrain INFO: evaluation result: {'Bleu_1': 0.7397330498015771, 'Bleu_2': 0.5793054174827693, 'Bleu_3': 0.4446408114409983, 'Bleu_4': 0.34098623749468476, 'METEOR': 0.29049820238722385, 'ROUGE_L': 0.5684687596555889, 'CIDEr': 1.1574415975737964, 'SPICE': 0.22115992464983875}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vlpretrain:evaluation result: {'Bleu_1': 0.7397330498015771, 'Bleu_2': 0.5793054174827693, 'Bleu_3': 0.4446408114409983, 'Bleu_4': 0.34098623749468476, 'METEOR': 0.29049820238722385, 'ROUGE_L': 0.5684687596555889, 'CIDEr': 1.1574415975737964, 'SPICE': 0.22115992464983875}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-09 03:37:45,846 vlpretrain INFO: evaluation result saved to /content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000/pred.coco_caption.test.beam5.max20.odlabels.eval.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vlpretrain:evaluation result saved to /content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000/pred.coco_caption.test.beam5.max20.odlabels.eval.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-09 03:37:45,849 vlpretrain INFO: Evaluation results saved to: /content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000/pred.coco_caption.test.beam5.max20.odlabels.eval.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vlpretrain:Evaluation results saved to: /content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000/pred.coco_caption.test.beam5.max20.odlabels.eval.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOYff8a1JcKR",
        "outputId": "0b6392ac-85eb-449a-d3fa-c7194f77ade1"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 10 07:46:23 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkT3U3wxSUHl",
        "outputId": "994297d4-065a-45a1-a55f-24fb2d97a828"
      },
      "source": [
        "!ls /content/drive/MyDrive/OscarPlus/weights/checkpoint0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added_tokens.json\n",
            "config.json\n",
            "pred.coco_caption.test.beam5.max20.odlabels_coco_format.json\n",
            "pred.coco_caption.test.beam5.max20.odlabels.eval.json\n",
            "pred.coco_caption.test.beam5.max20.odlabels.tsv\n",
            "pytorch_model.bin\n",
            "special_tokens_map.json\n",
            "training_args.bin\n",
            "vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ank1s00lkjd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a495869a-81ae-4ab9-9944-621b68a6a8e4"
      },
      "source": [
        "!pip list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                       Version\n",
            "----------------------------- --------------\n",
            "absl-py                       0.12.0\n",
            "alabaster                     0.7.12\n",
            "albumentations                0.1.12\n",
            "altair                        4.1.0\n",
            "anytree                       2.8.0\n",
            "appdirs                       1.4.4\n",
            "argcomplete                   1.12.3\n",
            "argon2-cffi                   21.1.0\n",
            "arviz                         0.11.4\n",
            "astor                         0.8.1\n",
            "astropy                       4.3.1\n",
            "astunparse                    1.6.3\n",
            "atari-py                      0.2.9\n",
            "atomicwrites                  1.4.0\n",
            "attrs                         21.2.0\n",
            "audioread                     2.1.9\n",
            "autograd                      1.3\n",
            "Babel                         2.9.1\n",
            "backcall                      0.2.0\n",
            "beautifulsoup4                4.6.3\n",
            "bleach                        4.1.0\n",
            "blis                          0.4.1\n",
            "bokeh                         2.3.3\n",
            "boto3                         1.20.21\n",
            "botocore                      1.23.21\n",
            "Bottleneck                    1.3.2\n",
            "branca                        0.4.2\n",
            "bs4                           0.0.1\n",
            "CacheControl                  0.12.10\n",
            "cached-property               1.5.2\n",
            "cachetools                    4.2.4\n",
            "catalogue                     1.0.0\n",
            "certifi                       2021.10.8\n",
            "cffi                          1.15.0\n",
            "cftime                        1.5.1.1\n",
            "chardet                       3.0.4\n",
            "charset-normalizer            2.0.8\n",
            "click                         7.1.2\n",
            "cloudpickle                   1.3.0\n",
            "cmake                         3.12.0\n",
            "cmdstanpy                     0.9.5\n",
            "colorcet                      2.0.6\n",
            "colorlover                    0.3.0\n",
            "community                     1.0.0b1\n",
            "contextlib2                   0.5.5\n",
            "convertdate                   2.3.2\n",
            "coverage                      3.7.1\n",
            "coveralls                     0.5\n",
            "crcmod                        1.7\n",
            "cufflinks                     0.17.3\n",
            "cupy-cuda111                  9.4.0\n",
            "cvxopt                        1.2.7\n",
            "cvxpy                         1.0.31\n",
            "cycler                        0.11.0\n",
            "cymem                         2.0.6\n",
            "Cython                        0.29.24\n",
            "daft                          0.0.4\n",
            "dask                          2.12.0\n",
            "datascience                   0.10.6\n",
            "debugpy                       1.0.0\n",
            "decorator                     4.4.2\n",
            "defusedxml                    0.7.1\n",
            "descartes                     1.1.0\n",
            "dill                          0.3.4\n",
            "distributed                   1.25.3\n",
            "dlib                          19.18.0\n",
            "dm-tree                       0.1.6\n",
            "docopt                        0.6.2\n",
            "docutils                      0.17.1\n",
            "dopamine-rl                   1.0.5\n",
            "earthengine-api               0.1.290\n",
            "easydict                      1.9\n",
            "ecos                          2.0.7.post1\n",
            "editdistance                  0.5.3\n",
            "en-core-web-sm                2.2.5\n",
            "entrypoints                   0.3\n",
            "ephem                         4.1\n",
            "et-xmlfile                    1.1.0\n",
            "fa2                           0.3.5\n",
            "fastai                        1.0.61\n",
            "fastdtw                       0.3.4\n",
            "fastprogress                  1.0.0\n",
            "fastrlock                     0.8\n",
            "fbprophet                     0.7.1\n",
            "feather-format                0.4.1\n",
            "filelock                      3.4.0\n",
            "firebase-admin                4.4.0\n",
            "fix-yahoo-finance             0.0.22\n",
            "Flask                         1.1.4\n",
            "flatbuffers                   2.0\n",
            "folium                        0.8.3\n",
            "future                        0.16.0\n",
            "gast                          0.4.0\n",
            "GDAL                          2.2.2\n",
            "gdown                         3.6.4\n",
            "gensim                        3.6.0\n",
            "geographiclib                 1.52\n",
            "geopy                         1.17.0\n",
            "gin-config                    0.5.0\n",
            "glob2                         0.7\n",
            "google                        2.0.3\n",
            "google-api-core               1.26.3\n",
            "google-api-python-client      1.12.8\n",
            "google-auth                   1.35.0\n",
            "google-auth-httplib2          0.0.4\n",
            "google-auth-oauthlib          0.4.6\n",
            "google-cloud-bigquery         1.21.0\n",
            "google-cloud-bigquery-storage 1.1.0\n",
            "google-cloud-core             1.0.3\n",
            "google-cloud-datastore        1.8.0\n",
            "google-cloud-firestore        1.7.0\n",
            "google-cloud-language         1.2.0\n",
            "google-cloud-storage          1.18.1\n",
            "google-cloud-translate        1.5.0\n",
            "google-colab                  1.0.0\n",
            "google-pasta                  0.2.0\n",
            "google-resumable-media        0.4.1\n",
            "googleapis-common-protos      1.53.0\n",
            "googledrivedownloader         0.4\n",
            "graphviz                      0.10.1\n",
            "greenlet                      1.1.2\n",
            "grpcio                        1.42.0\n",
            "gspread                       3.0.1\n",
            "gspread-dataframe             3.0.8\n",
            "gym                           0.17.3\n",
            "h5py                          3.1.0\n",
            "HeapDict                      1.0.1\n",
            "hijri-converter               2.2.2\n",
            "holidays                      0.10.5.2\n",
            "holoviews                     1.14.6\n",
            "html5lib                      1.0.1\n",
            "httpimport                    0.5.18\n",
            "httplib2                      0.17.4\n",
            "httplib2shim                  0.0.3\n",
            "humanize                      0.5.1\n",
            "hyperopt                      0.1.2\n",
            "ideep4py                      2.0.0.post3\n",
            "idna                          2.10\n",
            "imageio                       2.4.1\n",
            "imagesize                     1.3.0\n",
            "imbalanced-learn              0.8.1\n",
            "imblearn                      0.0\n",
            "imgaug                        0.2.9\n",
            "importlib-metadata            4.8.2\n",
            "importlib-resources           5.4.0\n",
            "imutils                       0.5.4\n",
            "inflect                       2.1.0\n",
            "iniconfig                     1.1.1\n",
            "intel-openmp                  2021.4.0\n",
            "intervaltree                  2.1.0\n",
            "ipykernel                     4.10.1\n",
            "ipython                       5.5.0\n",
            "ipython-genutils              0.2.0\n",
            "ipython-sql                   0.3.9\n",
            "ipywidgets                    7.6.5\n",
            "itsdangerous                  1.1.0\n",
            "jax                           0.2.25\n",
            "jaxlib                        0.1.71+cuda111\n",
            "jdcal                         1.4.1\n",
            "jedi                          0.18.1\n",
            "jieba                         0.42.1\n",
            "Jinja2                        2.11.3\n",
            "jmespath                      0.10.0\n",
            "joblib                        1.1.0\n",
            "jpeg4py                       0.1.4\n",
            "jsonschema                    2.6.0\n",
            "jupyter                       1.0.0\n",
            "jupyter-client                5.3.5\n",
            "jupyter-console               5.2.0\n",
            "jupyter-core                  4.9.1\n",
            "jupyterlab-pygments           0.1.2\n",
            "jupyterlab-widgets            1.0.2\n",
            "kaggle                        1.5.12\n",
            "kapre                         0.3.6\n",
            "keras                         2.7.0\n",
            "Keras-Preprocessing           1.1.2\n",
            "keras-vis                     0.4.1\n",
            "kiwisolver                    1.3.2\n",
            "korean-lunar-calendar         0.2.1\n",
            "libclang                      12.0.0\n",
            "librosa                       0.8.1\n",
            "lightgbm                      2.2.3\n",
            "llvmlite                      0.34.0\n",
            "lmdb                          0.99\n",
            "LunarCalendar                 0.0.9\n",
            "lxml                          4.2.6\n",
            "Markdown                      3.3.6\n",
            "MarkupSafe                    2.0.1\n",
            "matplotlib                    3.2.2\n",
            "matplotlib-inline             0.1.3\n",
            "matplotlib-venn               0.11.6\n",
            "missingno                     0.5.0\n",
            "mistune                       0.8.4\n",
            "mizani                        0.6.0\n",
            "mkl                           2019.0\n",
            "mlxtend                       0.14.0\n",
            "more-itertools                8.12.0\n",
            "moviepy                       0.2.3.5\n",
            "mpmath                        1.2.1\n",
            "msgpack                       1.0.3\n",
            "multiprocess                  0.70.12.2\n",
            "multitasking                  0.0.10\n",
            "murmurhash                    1.0.6\n",
            "music21                       5.5.0\n",
            "natsort                       5.5.0\n",
            "nbclient                      0.5.9\n",
            "nbconvert                     5.6.1\n",
            "nbformat                      5.1.3\n",
            "nest-asyncio                  1.5.4\n",
            "netCDF4                       1.5.8\n",
            "networkx                      2.6.3\n",
            "nibabel                       3.0.2\n",
            "nltk                          3.2.5\n",
            "notebook                      5.3.1\n",
            "numba                         0.51.2\n",
            "numexpr                       2.7.3\n",
            "numpy                         1.19.5\n",
            "nvidia-ml-py3                 7.352.0\n",
            "oauth2client                  4.1.3\n",
            "oauthlib                      3.1.1\n",
            "okgrade                       0.4.3\n",
            "opencv-contrib-python         4.1.2.30\n",
            "opencv-python                 4.1.2.30\n",
            "openpyxl                      2.5.9\n",
            "opt-einsum                    3.3.0\n",
            "osqp                          0.6.2.post0\n",
            "packaging                     21.3\n",
            "palettable                    3.3.0\n",
            "pandas                        1.1.5\n",
            "pandas-datareader             0.9.0\n",
            "pandas-gbq                    0.13.3\n",
            "pandas-profiling              1.4.1\n",
            "pandocfilters                 1.5.0\n",
            "panel                         0.12.1\n",
            "param                         1.12.0\n",
            "parso                         0.8.3\n",
            "pathlib                       1.0.1\n",
            "patsy                         0.5.2\n",
            "pep517                        0.12.0\n",
            "pexpect                       4.8.0\n",
            "pickleshare                   0.7.5\n",
            "Pillow                        7.1.2\n",
            "pip                           21.1.3\n",
            "pip-tools                     6.2.0\n",
            "plac                          1.1.3\n",
            "plotly                        4.4.1\n",
            "plotnine                      0.6.0\n",
            "pluggy                        0.7.1\n",
            "pooch                         1.5.2\n",
            "portpicker                    1.3.9\n",
            "prefetch-generator            1.0.1\n",
            "preshed                       3.0.6\n",
            "prettytable                   2.4.0\n",
            "progressbar2                  3.38.0\n",
            "prometheus-client             0.12.0\n",
            "promise                       2.3\n",
            "prompt-toolkit                1.0.18\n",
            "protobuf                      3.17.3\n",
            "psutil                        5.4.8\n",
            "psycopg2                      2.7.6.1\n",
            "ptyprocess                    0.7.0\n",
            "py                            1.11.0\n",
            "pyarrow                       3.0.0\n",
            "pyasn1                        0.4.8\n",
            "pyasn1-modules                0.2.8\n",
            "pycocotools                   2.0.3\n",
            "pycparser                     2.21\n",
            "pyct                          0.4.8\n",
            "pydata-google-auth            1.2.0\n",
            "pydot                         1.3.0\n",
            "pydot-ng                      2.0.0\n",
            "pydotplus                     2.0.2\n",
            "PyDrive                       1.3.1\n",
            "pyemd                         0.5.1\n",
            "pyerfa                        2.0.0.1\n",
            "pyglet                        1.5.0\n",
            "Pygments                      2.6.1\n",
            "pygobject                     3.26.1\n",
            "pymc3                         3.11.4\n",
            "PyMeeus                       0.5.11\n",
            "pymongo                       3.12.1\n",
            "pymystem3                     0.2.0\n",
            "PyOpenGL                      3.1.5\n",
            "pyparsing                     3.0.6\n",
            "pyrsistent                    0.18.0\n",
            "pysndfile                     1.3.8\n",
            "PySocks                       1.7.1\n",
            "pystan                        2.19.1.1\n",
            "pytest                        3.6.4\n",
            "python-apt                    0.0.0\n",
            "python-chess                  0.23.11\n",
            "python-dateutil               2.8.2\n",
            "python-louvain                0.15\n",
            "python-slugify                5.0.2\n",
            "python-utils                  2.5.6\n",
            "pytz                          2018.9\n",
            "pyviz-comms                   2.1.0\n",
            "PyWavelets                    1.2.0\n",
            "PyYAML                        3.13\n",
            "pyzmq                         22.3.0\n",
            "qdldl                         0.1.5.post0\n",
            "qtconsole                     5.2.1\n",
            "QtPy                          1.11.2\n",
            "regex                         2019.12.20\n",
            "requests                      2.23.0\n",
            "requests-oauthlib             1.3.0\n",
            "resampy                       0.2.2\n",
            "retrying                      1.3.3\n",
            "rpy2                          3.4.5\n",
            "rsa                           4.8\n",
            "s3transfer                    0.5.0\n",
            "scikit-image                  0.18.3\n",
            "scikit-learn                  1.0.1\n",
            "scipy                         1.4.1\n",
            "screen-resolution-extra       0.0.0\n",
            "scs                           2.1.4\n",
            "seaborn                       0.11.2\n",
            "semver                        2.13.0\n",
            "Send2Trash                    1.8.0\n",
            "setuptools                    57.4.0\n",
            "setuptools-git                1.2\n",
            "Shapely                       1.8.0\n",
            "simplegeneric                 0.8.1\n",
            "six                           1.15.0\n",
            "sklearn                       0.0\n",
            "sklearn-pandas                1.8.0\n",
            "smart-open                    5.2.1\n",
            "snowballstemmer               2.2.0\n",
            "sortedcontainers              2.4.0\n",
            "SoundFile                     0.10.3.post1\n",
            "spacy                         2.2.4\n",
            "Sphinx                        1.8.6\n",
            "sphinxcontrib-serializinghtml 1.1.5\n",
            "sphinxcontrib-websupport      1.2.4\n",
            "SQLAlchemy                    1.4.27\n",
            "sqlparse                      0.4.2\n",
            "srsly                         1.0.5\n",
            "statsmodels                   0.10.2\n",
            "sympy                         1.7.1\n",
            "tables                        3.4.4\n",
            "tabulate                      0.8.9\n",
            "tblib                         1.7.0\n",
            "tensorboard                   2.7.0\n",
            "tensorboard-data-server       0.6.1\n",
            "tensorboard-plugin-wit        1.8.0\n",
            "tensorflow                    2.7.0\n",
            "tensorflow-datasets           4.0.1\n",
            "tensorflow-estimator          2.7.0\n",
            "tensorflow-gcs-config         2.7.0\n",
            "tensorflow-hub                0.12.0\n",
            "tensorflow-io-gcs-filesystem  0.22.0\n",
            "tensorflow-metadata           1.4.0\n",
            "tensorflow-probability        0.15.0\n",
            "termcolor                     1.1.0\n",
            "terminado                     0.12.1\n",
            "testpath                      0.5.0\n",
            "text-unidecode                1.3\n",
            "textblob                      0.15.3\n",
            "Theano-PyMC                   1.1.2\n",
            "thinc                         7.4.0\n",
            "threadpoolctl                 3.0.0\n",
            "tifffile                      2021.11.2\n",
            "toml                          0.10.2\n",
            "tomli                         1.2.2\n",
            "toolz                         0.11.2\n",
            "torch                         1.10.0+cu111\n",
            "torchaudio                    0.10.0+cu111\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.11.0\n",
            "torchvision                   0.11.1+cu111\n",
            "tornado                       5.1.1\n",
            "tqdm                          4.62.3\n",
            "traitlets                     5.1.1\n",
            "tweepy                        3.10.0\n",
            "typeguard                     2.7.1\n",
            "typing-extensions             3.10.0.2\n",
            "tzlocal                       1.5.1\n",
            "uritemplate                   3.0.1\n",
            "urllib3                       1.25.11\n",
            "vega-datasets                 0.9.0\n",
            "wasabi                        0.8.2\n",
            "wcwidth                       0.2.5\n",
            "webencodings                  0.5.1\n",
            "Werkzeug                      1.0.1\n",
            "wheel                         0.37.0\n",
            "widgetsnbextension            3.5.2\n",
            "wordcloud                     1.5.0\n",
            "wrapt                         1.13.3\n",
            "xarray                        0.18.2\n",
            "xgboost                       0.90\n",
            "xkit                          0.0.0\n",
            "xlrd                          1.1.0\n",
            "xlwt                          1.3.0\n",
            "yellowbrick                   1.3.post1\n",
            "zict                          2.0.0\n",
            "zipp                          3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrain"
      ],
      "metadata": {
        "id": "-PnL0sYeOMZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from oscar.modeling.modeling_bert import BertImgForPreTraining\n",
        "from transformers.pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n",
        "                                  BertTokenizer)\n",
        "\n",
        "from oscar.datasets.build import make_data_loader\n",
        "\n",
        "from transformers.pytorch_transformers import AdamW, WarmupLinearSchedule\n",
        "from oscar.utils.misc import mkdir, get_rank\n",
        "from oscar.utils.metric_logger import TensorboardLogger\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (BertConfig,)), ())\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertImgForPreTraining, BertTokenizer),\n",
        "}\n",
        "\n",
        "\n",
        "\"\"\" ****** Pretraining ****** \"\"\"\n",
        "\n",
        "class Arguments(object):\n",
        "  pass\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = Arguments()\n",
        "\n",
        "\n",
        "    ## Required parameters\n",
        "    args.data_dir = None # The input data dir. Should contain the .yaml files for the task.\n",
        "    args.dataset_file = None # The training dataset yaml file.\n",
        "    args.extra_dataset_file = None # The extra training dataset yaml file.\n",
        "    args.bert_model = None # Bert pre-trained model selected in the list: bert-base-uncased, \n",
        "                          #   \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"\n",
        "    args.output_dir = None # REQUIRED! The output directory where the model checkpoints will be written.\n",
        "\n",
        "    # image chunks\n",
        "    args.chunk_start_id = -1 # Image Chunk Start ID \n",
        "    args.chunk_end_id = -1 # Image Chunk End ID\n",
        "\n",
        "    ## Image parameters\n",
        "    args.max_img_seq_length = 50 # The maximum total input image sequence length.\n",
        "    args.img_feature_dim = 2054 # The Image Feature Dimension.\n",
        "    args.img_feature_type = 'faster_r-cnn' # faster_r-cnn or mask_r-cnn\n",
        "    args.use_layernorm = False # use_layernorm\n",
        "\n",
        "    args.drop_out = 0.1 # Drop out for BERT.\n",
        "\n",
        "    args.use_b = 1 # \"use_b\"\n",
        "    args.textb_sample_mode = 0  #\"0: sample from both texta&textb, \"\n",
        "                           #  \"1: sample from textb, \"\n",
        "                           #  \"2: sample from QA answers\"\n",
        "    args.extra_textb_sample_mode =1 \n",
        "    args.texta_false_prob = 0.0 # the probality that we sample wrong texta, should in [0.0, 0.5]\n",
        "\n",
        "    args.model_name_or_path = None # REQUIRED! Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(\n",
        "                            # ALL_MODELS))\n",
        "    args.config_name = \"\"  # Pretrained config name or path if not the same as model_name\n",
        "    args.tokenizer_name = \"\" # Pretrained tokenizer name or path if not the same as model_name\n",
        "    args.cache_dir = \"\" # Where do you want to store the pre-trained models downloaded from s3\n",
        "\n",
        "    args.max_seq_length = 35 # The maximum total input sequence length after WordPiece tokenization.\n",
        "                             #\"Sequences longer than this will be truncated, and sequences shorter than this will be padded.\")\n",
        "    args.do_train = True # Whether to run training.\n",
        "    args.learning_rate = 5e-5 # The initial learning rate for Adam.\n",
        "    args.max_iters = 2000000 # Maximal number of training iterations.\n",
        "    args.train_batch_size = 1024 # Batch size for training.\n",
        "    args.num_workers = 6  # Number of workers for dataset.\n",
        "    args.adam_epsilon = 1e-8 # Epsilon for Adam optimizer.\n",
        "    args.optim = 'adamw' # The optimizer used for Bert, [adamw, lamb], default: adamw\"\n",
        "    args.max_grad_norm = -1.0 # Max gradient norm.\n",
        "    args.warmup_steps = 0 # Linear warmup over warmup_steps.\n",
        "    args.no_cuda = False # Whether not to use CUDA when available\n",
        "    args.on_memory = True # Whether to load train samples into memory or use disk\n",
        "    args.do_lower_case = True # Whether to lower case the input text. True for uncased models, False for cased models.\n",
        "    args.local_rank = -1 # local_rank for distributed training on gpus\n",
        "    args.seed = 42 # random seed for initialization\n",
        "    args.gradient_accumulation_steps = 1 # Number of updates steps to accumualte before performing a backward/update pass.\n",
        "\n",
        "    args.from_scratch = True # train from scratch\n",
        "    args.use_img_layernorm = 0 # Normalize image features with bertlayernorm\n",
        "    args.img_layer_norm_eps = 1e-12 # The eps in image feature laynorm layer\n",
        "    # distributed\n",
        "    args.gpu_ids = '-1'\n",
        "    args.mask_loss_for_unmatched = 1 # masked language model loss for unmatched triplets\n",
        "    args.extra_loss_weight = 0.0 # the loss weight for the extra train data batch (should be in [0,1])\n",
        "    args.use_gtlabels = 1 # use groundtruth labels for text b or not\n",
        "\n",
        "    # logging\n",
        "    args.ckpt_period = 10000 # Period for saving checkpoint\n",
        "    args.log_period = 100 # Period for saving logging info\n",
        "\n",
        "    # arguments for pretrain\n",
        "\n",
        "    args.max_grad_norm = 10\n",
        "    args.gradient_accumulation_steps = 1\n",
        "    args.use_img_layernorm = True\n",
        "    args.output_dir = '/content/drive/MyDrive/OscarPlus/output/pretrain/test' \n",
        "    args.bert_model = 'bert'\n",
        "    args.model_name_or_path = 'bert-base-uncased'\n",
        "    # args.model_name_or_path = '/content/drive/MyDrive/OscarPlus/weights/pretrain/pretrained_base/checkpoint-2000000'\n",
        "    args.do_lower_case = True\n",
        "    args.learning_rate = 5e-05\n",
        "    args.warmup_steps = 0\n",
        "    args.do_train = True\n",
        "    args.max_seq_length = 35\n",
        "    args.on_memory = True\n",
        "    args.max_img_seq_length = 50\n",
        "    args.img_feature_dim = 2054\n",
        "    args.drop_out = 0.1\n",
        "    args.train_batch_size = 64\n",
        "    args.ckpt_period = 50\n",
        "    args.max_iters = 2000300\n",
        "    args.log_period = 50\n",
        "    args.data_dir = '/content/drive/MyDrive/OscarPlus/datasets/pretrain'\n",
        "    args.dataset_file = '/content/drive/MyDrive/OscarPlus/datasets/pretrain/coco.yaml'\n",
        "    args.from_scratch = False\n",
        "    args.textb_sample_mode = 1\n",
        "    args.texta_false_prob = 0.25\n",
        "\n",
        "\n",
        "    if args.gpu_ids != '-1':\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_ids\n",
        "\n",
        "    args.num_gpus = int(\n",
        "        os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1\n",
        "    args.distributed = args.num_gpus > 1\n",
        "    print(args.distributed)\n",
        "\n",
        "    if args.gpu_ids != '-1':\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_ids\n",
        "\n",
        "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n",
        "        logger.info(\"Output Directory Exists.\")\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(\n",
        "            backend='nccl', init_method=\"env://\"\n",
        "        )\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
        "        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s\",\n",
        "        args.local_rank, device, args.n_gpu, bool(args.local_rank != -1)\n",
        "    )\n",
        "\n",
        "    if args.gradient_accumulation_steps < 1:\n",
        "        raise ValueError(\n",
        "            \"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
        "                args.gradient_accumulation_steps))\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    if not args.do_train:\n",
        "        raise ValueError(\n",
        "            \"Training is currently the only implemented execution option. Please set `do_train`.\")\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        mkdir(args.output_dir)\n",
        "\n",
        "    last_checkpoint_dir = None\n",
        "    arguments = {\"iteration\": 0}\n",
        "    if os.path.exists(args.output_dir):\n",
        "        save_file = os.path.join(args.output_dir, \"last_checkpoint\")\n",
        "        try:\n",
        "            with open(save_file, \"r\") as f:\n",
        "                last_saved = f.read()\n",
        "                last_saved = last_saved.strip()\n",
        "        except IOError:\n",
        "            # if file doesn't exist, maybe because it has just been\n",
        "            # deleted by a separate process\n",
        "            last_saved = \"\"\n",
        "        if last_saved:\n",
        "            folder_name = os.path.splitext(last_saved.split('/')[0])[0] # in the form of checkpoint-00001 or checkpoint-00001/pytorch_model.bin\n",
        "            last_checkpoint_dir = os.path.join(args.output_dir, folder_name)\n",
        "            arguments[\"iteration\"] = int(folder_name.split('-')[-1])\n",
        "            assert os.path.isfile(os.path.join(last_checkpoint_dir, WEIGHTS_NAME)), \"Last_checkpoint detected, but file not found!\"\n",
        "\n",
        "    # # define ckpt path\n",
        "    # last_checkpoint_dir = '/content/drive/MyDrive/OscarPlus/weights/pretrain/pretrained_base/checkpoint-2000000'\n",
        "    print(\"Last checkpoint dir: \", last_checkpoint_dir)\n",
        "\n",
        "    # model first\n",
        "    if get_rank() != 0:\n",
        "        torch.distributed.barrier()\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.bert_model]\n",
        "    if last_checkpoint_dir is not None:  # recovery\n",
        "        args.model_name_or_path = last_checkpoint_dir\n",
        "        logger.info(\" -> Recovering model from {}\".format(last_checkpoint_dir))\n",
        "\n",
        "    config = config_class.from_pretrained(\n",
        "        args.config_name if args.config_name else args.model_name_or_path,\n",
        "    )\n",
        "    config.img_layer_norm_eps = args.img_layer_norm_eps\n",
        "    config.use_img_layernorm = args.use_img_layernorm\n",
        "\n",
        "    # discrete code\n",
        "    config.img_feature_dim = args.img_feature_dim\n",
        "    config.img_feature_type = args.img_feature_type\n",
        "    config.hidden_dropout_prob = args.drop_out\n",
        "    if args.texta_false_prob < 0.5 and (args.texta_false_prob > 0 or not args.use_b):\n",
        "        args.num_contrast_classes = 3\n",
        "    else:\n",
        "        args.num_contrast_classes = 2\n",
        "    config.num_contrast_classes = args.num_contrast_classes\n",
        "\n",
        "    # Prepare model\n",
        "    # model = BertForPreTraining.from_pretrained(args.bert_model)\n",
        "    load_num = 0\n",
        "    while load_num < 10:\n",
        "        try:\n",
        "            model = BertImgForPreTraining.from_pretrained(\n",
        "                args.model_name_or_path,\n",
        "                from_tf=bool('.ckpt' in args.model_name_or_path),\n",
        "                config=config)\n",
        "            break\n",
        "        except:\n",
        "            load_num += 1\n",
        "\n",
        "    # train from scratch\n",
        "    if args.from_scratch:\n",
        "        if last_checkpoint_dir is None:\n",
        "            logger.info(\"Training from scratch ... \")\n",
        "            model.apply(model.init_weights)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    logger.info(\n",
        "        'Total Parameters: {}'.format(total_params))\n",
        "\n",
        "    for key, val in vars(config).items():\n",
        "        setattr(args, key, val)\n",
        "\n",
        "    if get_rank() == 0 and args.local_rank != -1:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    tb_log_dir = os.path.join(args.output_dir, 'train_logs')\n",
        "    meters = TensorboardLogger(\n",
        "        log_dir=tb_log_dir,\n",
        "        delimiter=\"  \",\n",
        "    )\n",
        "\n",
        "    # Prepare optimizer\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if\n",
        "                    not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if\n",
        "                    any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                              lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = WarmupLinearSchedule(optimizer,\n",
        "                                     warmup_steps=args.warmup_steps,\n",
        "                                     t_total=args.max_iters)\n",
        "\n",
        "    if arguments['iteration'] > 0 and os.path.isfile(os.path.join(last_checkpoint_dir, 'optimizer.pth')):  # recovery\n",
        "        logger.info(\n",
        "            \"Load BERT optimizer from {}\".format(last_checkpoint_dir))\n",
        "        optimizer_to_load = torch.load(\n",
        "            os.path.join(last_checkpoint_dir, 'optimizer.pth'),\n",
        "            map_location=torch.device(\"cpu\"))\n",
        "        optimizer.load_state_dict(optimizer_to_load.pop(\"optimizer\"))\n",
        "        scheduler.load_state_dict(optimizer_to_load.pop(\"scheduler\"))\n",
        "\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank,\n",
        "            find_unused_parameters=True)\n",
        "    elif args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # train_examples = None\n",
        "    train_dataloaders = make_data_loader(\n",
        "        args, is_distributed=args.distributed, arguments=arguments\n",
        "    )\n",
        "\n",
        "    if isinstance(train_dataloaders, list):\n",
        "        train_dataloader = train_dataloaders[0]\n",
        "    else:\n",
        "        train_dataloader = train_dataloaders\n",
        "    train_dataloader_extra = [None] * len(train_dataloader)\n",
        "    if isinstance(train_dataloaders, list) and len(train_dataloaders) > 1:\n",
        "        logger.info(\"Having two train dataloaders!\")\n",
        "        train_dataloader_extra = train_dataloaders[1]\n",
        "    tokenizer = train_dataloader.dataset.tokenizer\n",
        "\n",
        "    # torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    max_iter = len(train_dataloader)\n",
        "    start_iter = arguments[\"iteration\"]\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\" Num examples = {}\".format(len(train_dataloader.dataset)))\n",
        "    logger.info(\"  Instantaneous batch size = %d\",\n",
        "                args.train_batch_size // args.gradient_accumulation_steps)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\",\n",
        "                args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\",\n",
        "                max_iter // args.gradient_accumulation_steps)\n",
        "\n",
        "    log_json = {}\n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    clock_started = False\n",
        "    # Every args.ckpt_period, report train_score and save model\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, (batch, batch_extra) in enumerate(zip(train_dataloader, train_dataloader_extra), start_iter):\n",
        "        if not clock_started:\n",
        "            start_training_time = time.time()\n",
        "            end = time.time()\n",
        "            clock_started = True\n",
        "\n",
        "        def data_process(mini_batch):\n",
        "            images, targets, qa_inds = \\\n",
        "                mini_batch[0], mini_batch[1], mini_batch[2]\n",
        "            targets_transposed = list(zip(*targets))\n",
        "            input_ids = torch.stack(targets_transposed[0]).to(args.device, non_blocking=True)\n",
        "            input_mask = torch.stack(targets_transposed[1]).to(args.device, non_blocking=True)\n",
        "            segment_ids = torch.stack(targets_transposed[2]).to(args.device, non_blocking=True)\n",
        "            lm_label_ids = torch.stack(targets_transposed[3]).to(args.device, non_blocking=True)\n",
        "            is_next = torch.stack(targets_transposed[4]).to(args.device, non_blocking=True)\n",
        "            is_img_match = torch.stack(targets_transposed[5]).to(args.device, non_blocking=True)\n",
        "\n",
        "            return images, input_ids, input_mask, segment_ids, lm_label_ids, is_next\n",
        "\n",
        "        images1, input_ids1, input_mask1, segment_ids1, lm_label_ids1, is_next1 \\\n",
        "            = data_process(batch)\n",
        "        if batch_extra is not None:\n",
        "            images2, input_ids2, input_mask2, segment_ids2, lm_label_ids2, is_next2 \\\n",
        "                = data_process(batch_extra)\n",
        "\n",
        "        data_time = time.time() - end\n",
        "\n",
        "        def forward_backward(images, input_ids, input_mask, segment_ids,\n",
        "                             lm_label_ids, is_next, loss_weight=1.0):\n",
        "            # feature as input\n",
        "            image_features = torch.stack(images).to(args.device, non_blocking=True)\n",
        "\n",
        "            outputs = model(input_ids, segment_ids, input_mask,\n",
        "                            lm_label_ids, is_next, img_feats=image_features)\n",
        "\n",
        "            loss = loss_weight * outputs[0]\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu.\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            return loss.item(), input_ids.size(0)\n",
        "\n",
        "        start1 = time.time()\n",
        "        loss1, nb_tr_example1 = forward_backward(\n",
        "            images1, input_ids1, input_mask1,\n",
        "            segment_ids1, lm_label_ids1, is_next1,\n",
        "            loss_weight=1.0-args.extra_loss_weight\n",
        "        )\n",
        "        tr_loss += loss1\n",
        "        nb_tr_examples += nb_tr_example1\n",
        "        compute_time1 = time.time() - start1\n",
        "\n",
        "        loss2, nb_tr_example2 = 0.0, 0\n",
        "        compute_time2 = 0.0\n",
        "        if batch_extra is not None:\n",
        "            start2 = time.time()\n",
        "            loss2, nb_tr_example2 = forward_backward(\n",
        "                images2, input_ids2, input_mask2,\n",
        "                segment_ids2, lm_label_ids2, is_next2,\n",
        "                loss_weight=args.extra_loss_weight\n",
        "            )\n",
        "            tr_loss += loss2\n",
        "            nb_tr_examples += nb_tr_example2\n",
        "            compute_time2 = time.time() - start2\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        arguments[\"iteration\"] = step + 1\n",
        "\n",
        "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "            # do gradient clipping\n",
        "            if args.max_grad_norm > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "            # do the optimization steps\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Update learning rate schedule\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time = time.time() - end\n",
        "            end = time.time()\n",
        "            metrics_to_log = {\n",
        "                'time_info': {'compute': batch_time, 'data': data_time,\n",
        "                              'compute1': compute_time1,\n",
        "                              'compute2': compute_time2},\n",
        "                'batch_metrics': {'loss': loss1+loss2}\n",
        "            }\n",
        "            params_to_log = {'params': {'bert_lr': optimizer.param_groups[0][\"lr\"]}}\n",
        "            meters.update_metrics(metrics_to_log)\n",
        "            meters.update_params(params_to_log)\n",
        "\n",
        "            if args.log_period > 0 and (step + 1) % args.log_period == 0:\n",
        "                avg_time = meters.meters['time_info']['compute'].global_avg\n",
        "                eta_seconds = avg_time * (max_iter - step - 1)\n",
        "                eta_string = str(\n",
        "                    datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                logger.info(\n",
        "                    meters.delimiter.join(\n",
        "                        [\n",
        "                            \"eta: {eta}\",\n",
        "                            \"iter: {iter}\",\n",
        "                            \"max mem: {memory:.0f}\",\n",
        "                        ]\n",
        "                    ).format(\n",
        "                        eta=eta_string,\n",
        "                        iter=step + 1,\n",
        "                        memory=torch.cuda.max_memory_allocated() / 1024.0 / 1024.0,\n",
        "                    ) + \"\\n    \" + meters.get_logs(step + 1)\n",
        "                )\n",
        "\n",
        "        if (step + 1) == max_iter or (step + 1) % args.ckpt_period == 0:  # Save a trained model\n",
        "            log_json[step+1] = tr_loss\n",
        "            train_metrics_total = torch.Tensor([tr_loss, nb_tr_examples, nb_tr_steps]).to(args.device)\n",
        "            if args.distributed:\n",
        "              torch.distributed.all_reduce(train_metrics_total)\n",
        "            # reset metrics\n",
        "            tr_loss = 0\n",
        "            nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "            if get_rank() == 0:\n",
        "                # report metrics\n",
        "                train_score_gathered = train_metrics_total[0] / \\\n",
        "                                       train_metrics_total[2]\n",
        "                logger.info(\"PROGRESS: {}%\".format(\n",
        "                    round(100 * (step + 1) / max_iter, 4)))\n",
        "                logger.info(\n",
        "                    \"EVALERR: {}%\".format(train_score_gathered))\n",
        "                meters.update_metrics(\n",
        "                    {\n",
        "                        'epoch_metrics': {'ex_cnt': train_metrics_total[1],\n",
        "                                          'loss': train_score_gathered}\n",
        "                    }\n",
        "                )\n",
        "                with open(os.path.join(args.output_dir, 'loss_logs.json'),\n",
        "                          'w') as fp:\n",
        "                    json.dump(log_json, fp)\n",
        "\n",
        "                # save checkpoint\n",
        "                output_dir = os.path.join(args.output_dir,\n",
        "                                          'checkpoint-{:07d}'.format(\n",
        "                                              step + 1))\n",
        "                if not os.path.exists(output_dir):\n",
        "                    os.makedirs(output_dir)\n",
        "                model_to_save = model.module if hasattr(\n",
        "                    model,\n",
        "                    'module') else model  # Take care of distributed/parallel training\n",
        "                optimizer_to_save = {\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"scheduler\": scheduler.state_dict()}\n",
        "\n",
        "                save_num = 0\n",
        "                while save_num < 10:\n",
        "                    try:\n",
        "                        model_to_save.save_pretrained(output_dir)\n",
        "                        torch.save(args, os.path.join(output_dir,\n",
        "                                                      'training_args.bin'))\n",
        "                        tokenizer.save_pretrained(output_dir)\n",
        "                        torch.save(optimizer_to_save,\n",
        "                                   os.path.join(output_dir,\n",
        "                                                'optimizer.pth'))\n",
        "                        save_file = os.path.join(args.output_dir, \"last_checkpoint\")\n",
        "                        with open(save_file, \"w\") as f:\n",
        "                            f.write('checkpoint-{:07d}/pytorch_model.bin'.format(step + 1))\n",
        "                        break\n",
        "                    except:\n",
        "                        save_num += 1\n",
        "                logger.info(\n",
        "                    \"Saving model checkpoint {0} to {1}\".format(\n",
        "                        step + 1, output_dir))\n",
        "\n",
        "    if clock_started:\n",
        "        total_training_time = time.time() - start_training_time\n",
        "    else:\n",
        "        total_training_time = 0.0\n",
        "    total_time_str = str(datetime.timedelta(seconds=total_training_time))\n",
        "    logger.info(\n",
        "        \"Total training time: {} ({:.4f} s / it)\".format(\n",
        "            total_time_str, total_training_time / max_iter\n",
        "        )\n",
        "    )\n",
        "    # close the tb logger\n",
        "    meters.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrGCLt7wOP32",
        "outputId": "0662d7de-fe48-4c26-dc14-f1fac20e0b2a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/10/2021 07:46:54 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n",
            "12/10/2021 07:46:55 - INFO - __main__ -  -> Recovering model from /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000200\n",
            "12/10/2021 07:46:55 - INFO - transformers.pytorch_transformers.modeling_utils - loading configuration file /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000200/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last checkpoint dir:  /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/10/2021 07:46:55 - INFO - transformers.pytorch_transformers.modeling_utils - Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"img_feature_dim\": 2054,\n",
            "  \"img_feature_type\": \"faster_r-cnn\",\n",
            "  \"img_layer_norm_eps\": 1e-12,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_bert\": true,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_contrast_classes\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_img_layernorm\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/10/2021 07:46:55 - INFO - root - loading weights file /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000200/pytorch_model.bin\n",
            "12/10/2021 07:46:58 - INFO - oscar.modeling.modeling_bert - BertImgModel Image Dimension: 2054\n",
            "12/10/2021 07:47:10 - INFO - __main__ - Total Parameters: 111686973\n",
            "12/10/2021 07:47:20 - INFO - __main__ - Training/evaluation parameters <__main__.Arguments object at 0x7f24bfe4c090>\n",
            "12/10/2021 07:47:21 - INFO - __main__ - Load BERT optimizer from /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000200\n",
            "12/10/2021 07:47:34 - INFO - oscar.datasets.build - Train with 64 images per GPU\n",
            "12/10/2021 07:47:35 - INFO - transformers.pytorch_transformers.tokenization_utils - Model name '/content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000200' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000200' is a path or url to a directory containing tokenizer files.\n",
            "12/10/2021 07:47:35 - INFO - transformers.pytorch_transformers.tokenization_utils - loading file /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000200/added_tokens.json\n",
            "12/10/2021 07:47:35 - INFO - transformers.pytorch_transformers.tokenization_utils - loading file /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000200/special_tokens_map.json\n",
            "12/10/2021 07:47:35 - INFO - transformers.pytorch_transformers.tokenization_utils - loading file /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000200/vocab.txt\n",
            "12/10/2021 07:47:37 - INFO - root - Datasets: coco\n",
            "12/10/2021 07:47:37 - INFO - root - Open image label file /content/drive/MyDrive/OscarPlus/datasets/pretrain/labels/coco/predictions_gt.tsv, time: 0.2660808563232422\n",
            "12/10/2021 07:47:40 - INFO - root - Load img label offset map: /content/drive/MyDrive/OscarPlus/datasets/pretrain/labels/coco/imageid2idx.json, time: 2.6069998741149902\n",
            "12/10/2021 07:47:40 - INFO - root - * Loading dataset coco\n",
            "12/10/2021 07:47:41 - INFO - root - Open dataset /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.tsv, time: 0.7018575668334961\n",
            "12/10/2021 07:47:41 - INFO - root - Info: loading img features using 3.592477321624756 secs\n",
            "12/10/2021 07:47:41 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/coco.lineidx\n",
            "  0%|          | 0/3691045 [00:00<?, ?it/s]12/10/2021 07:47:44 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/labels/coco/predictions_gt.lineidx\n",
            " 15%|█▌        | 558818/3691045 [02:51<10:49, 4823.28it/s]12/10/2021 07:50:36 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/labels/coco/QA_fileB.lineidx\n",
            "100%|██████████| 3691045/3691045 [07:33<00:00, 8130.30it/s] \n",
            "12/10/2021 07:55:18 - INFO - root - Max_tokens: 184\n",
            "12/10/2021 07:55:18 - INFO - root - Total docs - Corpus_lines: 1588501-3177002\n",
            "12/10/2021 07:55:18 - INFO - root - Total QA docs - Corpus_lines: 1029500\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "12/10/2021 07:55:18 - INFO - __main__ - ***** Running training *****\n",
            "12/10/2021 07:55:18 - INFO - __main__ -  Num examples = 1588501\n",
            "12/10/2021 07:55:18 - INFO - __main__ -   Instantaneous batch size = 64\n",
            "12/10/2021 07:55:18 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "12/10/2021 07:55:18 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "12/10/2021 07:55:18 - INFO - __main__ -   Total optimization steps = 2000300\n",
            "12/10/2021 07:55:18 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "12/10/2021 07:55:18 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "12/10/2021 07:55:18 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "12/10/2021 07:55:18 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "12/10/2021 07:55:18 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "12/10/2021 07:55:19 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "12/10/2021 07:55:22 - INFO - root - *** Example ***\n",
            "12/10/2021 07:55:22 - INFO - root - *** Example ***\n",
            "12/10/2021 07:55:22 - INFO - root - *** Example ***\n",
            "12/10/2021 07:55:22 - INFO - root - guid: 0\n",
            "12/10/2021 07:55:22 - INFO - root - *** Example ***\n",
            "12/10/2021 07:55:22 - INFO - root - guid: 0\n",
            "12/10/2021 07:55:22 - INFO - root - tokens: [CLS] two small bears stand next to each other . [SEP] [MASK] bear floor [SEP]\n",
            "12/10/2021 07:55:22 - INFO - root - guid: 0\n",
            "12/10/2021 07:55:22 - INFO - root - guid: 0\n",
            "12/10/2021 07:55:22 - INFO - root - tokens: [CLS] what type of vehicle is [MASK] ? [SEP] [MASK] cart [SEP]\n",
            "12/10/2021 07:55:22 - INFO - root - input_ids: 101 2048 2235 6468 3233 2279 2000 2169 2060 1012 102 103 4562 2723 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - tokens: [CLS] [MASK] man [MASK] a paddle board on top of [MASK] wave . [SEP] brown [SEP]\n",
            "12/10/2021 07:55:22 - INFO - root - input_ids: 101 2054 2828 1997 4316 2003 103 1029 102 103 11122 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - input_ids: 101 103 2158 103 1037 20890 2604 2006 2327 1997 103 4400 1012 102 2829 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - tokens: [CLS] some yellow plants in a [MASK] green bucket [SEP] pot ##ted plant vase [MASK] furniture [MASK] table concrete [MASK] [SEP]\n",
            "12/10/2021 07:55:22 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - input_ids: 101 2070 3756 4264 1999 1037 103 2665 13610 102 8962 3064 3269 18781 103 7390 103 2795 5509 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - LM label: [-1, -1, -1, -1, -1, -1, 2023, -1, -1, 4744, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "12/10/2021 07:55:22 - INFO - root - LM label: [-1, 1037, -1, 5559, -1, -1, -1, -1, -1, -1, 1037, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "12/10/2021 07:55:22 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - Is next sentence label: 0 \n",
            "12/10/2021 07:55:22 - INFO - root - Is next sentence label: 1 \n",
            "12/10/2021 07:55:22 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - LM label: [-1, -1, -1, -1, -1, -1, 2235, -1, -1, -1, -1, -1, -1, -1, 6847, -1, 2422, -1, -1, 2813, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "12/10/2021 07:55:22 - INFO - root - Is next sentence label: 0 \n",
            "12/10/2021 07:55:22 - INFO - root - LM label: [-1, -1, -1, -1, 3233, -1, -1, -1, -1, -1, -1, 4562, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "12/10/2021 07:55:22 - INFO - root - Is next sentence label: 0 \n",
            "12/10/2021 07:55:22 - INFO - root - *** Example ***\n",
            "12/10/2021 07:55:22 - INFO - root - guid: 0\n",
            "12/10/2021 07:55:22 - INFO - root - tokens: [CLS] what is the enables holding ? [SEP] person surf ##board clothes sea [SEP]\n",
            "12/10/2021 07:55:22 - INFO - root - input_ids: 101 2054 2003 1996 12939 3173 1029 102 2711 14175 6277 4253 2712 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:22 - INFO - root - LM label: [-1, -1, -1, -1, 2158, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "12/10/2021 07:55:22 - INFO - root - Is next sentence label: 2 \n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "12/10/2021 07:55:24 - INFO - root - *** Example ***\n",
            "12/10/2021 07:55:24 - INFO - root - guid: 0\n",
            "12/10/2021 07:55:24 - INFO - root - tokens: [CLS] what is the zebra sniffing ? [SEP] ground [SEP]\n",
            "12/10/2021 07:55:24 - INFO - root - input_ids: 101 2054 2003 1996 29145 27646 1029 102 2598 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:24 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:24 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 07:55:24 - INFO - root - LM label: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "12/10/2021 07:55:24 - INFO - root - Is next sentence label: 0 \n",
            "/content/drive/MyDrive/OscarPlus/transformers/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "12/10/2021 08:07:34 - INFO - __main__ - eta: 0:07:21  iter: 2000250  max mem: 10787\n",
            "    batch_metrics - loss: 1.4485 (1.6567)\n",
            "    time_info     - compute: 0.8277 (8.8255)  data: 0.0031 (7.9375)  compute1: 0.7102 (0.7364)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 08:07:34 - INFO - __main__ - PROGRESS: 99.9975%\n",
            "12/10/2021 08:07:34 - INFO - __main__ - EVALERR: 1.6567271947860718%\n",
            "12/10/2021 08:07:45 - INFO - __main__ - Saving model checkpoint 2000250 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000250\n",
            "12/10/2021 08:10:42 - INFO - __main__ - eta: 0:00:00  iter: 2000300  max mem: 10787\n",
            "    batch_metrics - loss: 1.3313 (1.5760)\n",
            "    epoch_metrics - ex_cnt: 3200.0000 (3200.0000)  loss: 1.6567 (1.6567)\n",
            "    time_info     - compute: 0.7728 (6.2965)  data: 0.0017 (5.4479)  compute1: 0.7045 (0.7255)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 08:10:42 - INFO - __main__ - PROGRESS: 100.0%\n",
            "12/10/2021 08:10:42 - INFO - __main__ - EVALERR: 1.4953161478042603%\n",
            "12/10/2021 08:10:49 - INFO - __main__ - Saving model checkpoint 2000300 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000300\n",
            "12/10/2021 08:10:50 - INFO - __main__ - Total training time: 0:10:37.260918 (0.0003 s / it)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=1 oscar/run_oscarplus_pretrain.py"
      ],
      "metadata": {
        "id": "BFb_jurgRLU-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}