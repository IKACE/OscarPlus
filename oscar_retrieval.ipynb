{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IKACE/OscarPlus/blob/main/oscar_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEDqnYIMuAQp"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsewaoP1pLkG"
      },
      "source": [
        "Make sure that modules and datasets are properly mounted from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4_8Y6CLSaFq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8936e0-7952-4458-a185-79685f790e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85AegIjlg5tH"
      },
      "source": [
        "Download Corpus lineidx file to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B9n4u6Mg-h1",
        "outputId": "1a41ff47-8e27-4b65-8b97-0d6a1fcbd084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: ./azcopy_linux_amd64_10.13.0/azcopy: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!./azcopy_linux_amd64_10.13.0/azcopy copy https://biglmdiag.blob.core.windows.net/vinvl/pretrain_corpus/coco_flickr30k_gqa.lineidx /content/drive/MyDrive/OscarPlus/datasets/pretrain "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ivvYXNiYVm4"
      },
      "source": [
        "Download COCO image labels to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SgsX08gYUwc",
        "outputId": "8c39f2f3-65ed-404e-bf12-5f984a02ff0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: ./azcopy_linux_amd64_10.13.0/azcopy: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!./azcopy_linux_amd64_10.13.0/azcopy copy https://biglmdiag.blcore.windows.net/vinvl/pretrain_corpus/X152C4_frcnnbig2_exp168model_0060000model.roi_heads.nm_filter_2_model.roi_heads.score_thresh_0.2 /content/drive/MyDrive/OscarPlus/datasets/pretrain --recursive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utb2nvtygBfC"
      },
      "source": [
        "Download COCO image features to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_HDOmHWgAhT",
        "outputId": "88a4eb20-ba17-4054-c4c4-081fba3fc923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-09 23:33:13--  https://biglmdiag.blob.core.windows.net/vinvl/image_features/coco_X152C4_frcnnbig2_exp168model_0060000model.roi_heads.nm_filter_2_model.roi_heads.score_thresh_0.2/model_0060000/\n",
            "Resolving biglmdiag.blob.core.windows.net (biglmdiag.blob.core.windows.net)... 52.240.48.36\n",
            "Connecting to biglmdiag.blob.core.windows.net (biglmdiag.blob.core.windows.net)|52.240.48.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 The specified blob does not exist.\n",
            "2021-12-09 23:33:13 ERROR 404: The specified blob does not exist..\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --recursive  \"https://biglmdiag.blob.core.windows.net/vinvl/image_features/coco_X152C4_frcnnbig2_exp168model_0060000model.roi_heads.nm_filter_2_model.roi_heads.score_thresh_0.2/model_0060000/\" -P \"/content/drive/MyDrive/OscarPlus/datasets/pretrain\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf azcopy_linux_amd64_10.13.0.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY5KIQhSHfy2",
        "outputId": "f2688b25-c06f-4a83-c6a3-5f635f5761c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "azcopy_linux_amd64_10.13.0/\n",
            "azcopy_linux_amd64_10.13.0/NOTICE.txt\n",
            "azcopy_linux_amd64_10.13.0/azcopy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm_MQb9ejatx",
        "outputId": "679ccd2d-4a80-4b3b-8088-af4618fa301f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Scanning...\n",
            "INFO: Any empty folders will not be processed, because source and/or destination doesn't have full folder support\n",
            "\n",
            "Job 62b8a218-7665-9e41-51ba-787517bf2b9a has started\n",
            "Log file is located at: /root/.azcopy/62b8a218-7665-9e41-51ba-787517bf2b9a.log\n",
            "\n",
            "100.0 %, 30 Done, 0 Failed, 1 Pending, 0 Skipped, 31 Total, 2-sec Throughput (Mb/s): 167.7438\n",
            "\n",
            "\n",
            "Job 62b8a218-7665-9e41-51ba-787517bf2b9a summary\n",
            "Elapsed Time (Minutes): 0.1\n",
            "Number of File Transfers: 31\n",
            "Number of Folder Property Transfers: 0\n",
            "Total Number of Transfers: 31\n",
            "Number of Transfers Completed: 31\n",
            "Number of Transfers Failed: 0\n",
            "Number of Transfers Skipped: 0\n",
            "TotalBytesTransferred: 819240583\n",
            "Final Job Status: Completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!./azcopy_linux_amd64_10.13.0/azcopy copy https://biglmdiag.blob.core.windows.net/vinvl/datasets/coco_ir /content/drive/MyDrive/OscarPlus/datasets/retrieval --recursive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMxRZuDWh_wp",
        "outputId": "8e55e69a-9d98-4e5a-edb4-bb719b62aae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "coco.lineidx  coco.tsv\tcoco.yaml  labels  model_0060000\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/OscarPlus/datasets/pretrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMiNxOBauO6G",
        "outputId": "64601885-35a7-4f5c-d6fa-2cfeb5422007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "annotations\t\t__init__.py  pycocoevalcap  results\n",
            "cocoEvalCapDemo.ipynb\tlicense.txt  pycocotools\n",
            "get_stanford_models.sh\t__pycache__  README.md\n"
          ]
        }
      ],
      "source": [
        "!ls drive/MyDrive/OscarPlus/coco_caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-9VGiTmPyfP",
        "outputId": "4511f42b-694b-4979-8c1d-da2801f57abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 281project4\n",
            " 450DR2.drawio\n",
            " 450DR2multimodal.drawio\n",
            "'Colab Notebooks'\n",
            " Notability\n",
            " OscarPlus\n",
            " pred.coco_caption.test.beam5.max20.odlabels.tsv.tmp\n",
            " ps_9_sshoouri.ipynb\n",
            " sshoouri.ipynb\n",
            " VE450\n",
            "'VE450 Design Review #1 - 条形图 1.gsheet'\n",
            "'VE450 Design Review #1 - 柱形图 1.gsheet'\n",
            "'VE450 Design Review #1 - 柱形图 2.gsheet'\n",
            " Yile_Gu_Final_Essay_517370910109.docx\n",
            "'大三DD学生转学分P F或letter grade意向调查表.gform'\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSi-WnNx5nfR"
      },
      "source": [
        "Install python requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVf0dNpPbuS5",
        "outputId": "f0f3004c-cbb3-4e00-dda5-ef733c1f5c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardx\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 41.5 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 46.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx) (1.15.0)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_od9neO5p-4",
        "outputId": "0dd71b03-31d1-4f11-a352-39848cb35e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 1)) (4.62.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 2)) (3.13)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (0.18.3)\n",
            "Collecting anytree\n",
            "  Downloading anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 519 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/OscarPlus/requirements.txt (line 7)) (2019.12.20)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.23-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 22.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (3.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (2.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (2.6.3)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r /content/drive/MyDrive/OscarPlus/requirements.txt (line 5)) (1.2.0)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.24.0,>=1.23.23\n",
            "  Downloading botocore-1.23.23-py3-none-any.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 73.0 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.9 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 76.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, jmespath, botocore, s3transfer, boto3, anytree\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed anytree-2.8.0 boto3-1.20.23 botocore-1.23.23 jmespath-0.10.0 s3transfer-0.5.0 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/drive/MyDrive/OscarPlus/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-UehtJCpXVw"
      },
      "source": [
        "Add drive directory to system path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Ryure3unWA"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/OscarPlus\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miKiUiLn1DrN"
      },
      "source": [
        "Make sure submodules can be imported"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKF7ggoyu1aX"
      },
      "outputs": [],
      "source": [
        "from coco_caption.pycocotools.coco import COCO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EitKYqG4yHih"
      },
      "source": [
        "# Image Caption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxKxhP8l4U2f"
      },
      "source": [
        "## Define Bert class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0pCRvzy4Q9R"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from transformers.pytorch_transformers.modeling_bert import (BertEmbeddings, \n",
        "        BertSelfAttention, BertAttention, BertEncoder, BertLayer, \n",
        "        BertSelfOutput, BertIntermediate, BertOutput,\n",
        "        BertPooler, BertLayerNorm, BertPreTrainedModel,\n",
        "\t\tBertPredictionHeadTransform, BertOnlyMLMHead, BertLMPredictionHead,\n",
        "        BertConfig, BERT_PRETRAINED_MODEL_ARCHIVE_MAP,\n",
        "        load_tf_weights_in_bert)\n",
        "from oscar.modeling.modeling_utils import CaptionPreTrainedModel, ImgPreTrainedModel\n",
        "from oscar.modeling.modeling_bert import BertImgModel\n",
        "from oscar.utils.cbs import ConstrainedBeamSearch, select_best_beam_with_constraints\n",
        "\n",
        "class BertCaptioningLoss(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.label_smoothing = getattr(config, 'label_smoothing', 0)\n",
        "        self.drop_worst_ratio = getattr(config, 'drop_worst_ratio', 0)\n",
        "        self.drop_worst_after = getattr(config, 'drop_worst_after', 0)\n",
        "        self.log_soft = nn.LogSoftmax(dim=1)\n",
        "        self.kl = nn.KLDivLoss(reduction='none')\n",
        "        self.iter = 0\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        self.iter += 1\n",
        "        eps = self.label_smoothing\n",
        "        n_class = logits.size(1)\n",
        "        one_hot = torch.zeros_like(logits).scatter(1, target.view(-1, 1), 1)\n",
        "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
        "        log_prb = self.log_soft(logits)\n",
        "        loss = self.kl(log_prb, one_hot).sum(1)\n",
        "\n",
        "        if self.drop_worst_ratio > 0 and self.iter > self.drop_worst_after:\n",
        "            loss, _ = torch.topk(loss,\n",
        "                    k=int(loss.shape[0] * (1-self.drop_worst_ratio)),\n",
        "                    largest=False)\n",
        "\n",
        "        loss = loss.mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "class BertForImageCaptioning(CaptionPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Bert for Image Captioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForImageCaptioning, self).__init__(config)\n",
        "        self.config = config\n",
        "        self.bert = BertImgModel(config)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "        self.loss = BertCaptioningLoss(config)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        if hasattr(self.config, 'tie_weights') and self.config.tie_weights:\n",
        "            self._tie_or_clone_weights(self.cls.predictions.decoder,\n",
        "                                       self.bert.embeddings.word_embeddings)\n",
        "        freeze = False\n",
        "        if hasattr(self.config, 'freeze_embedding'):\n",
        "            freeze = self.config.freeze_embedding\n",
        "        self.bert.embeddings.word_embeddings.weight.requires_grad = not freeze\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        is_decode = kwargs.get('is_decode', False)\n",
        "        if is_decode:\n",
        "            return self.generate(*args, **kwargs)\n",
        "        else:\n",
        "            return self.encode_forward(*args, **kwargs)\n",
        "\n",
        "    def encode_forward(self, input_ids, img_feats, attention_mask, masked_pos, masked_ids=None, \n",
        "            token_type_ids=None, position_ids=None, head_mask=None,\n",
        "            is_training=True, encoder_history_states=None):\n",
        "        outputs = self.bert(input_ids, img_feats=img_feats, attention_mask=attention_mask, \n",
        "                position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                head_mask=head_mask,\n",
        "                encoder_history_states=encoder_history_states)\n",
        "        sequence_output = outputs[0][:, :masked_pos.shape[-1], :]\n",
        "\n",
        "        if is_training:\n",
        "            sequence_output = outputs[0][:, :masked_pos.shape[-1], :]\n",
        "            # num_masks_in_batch * hidden_size\n",
        "            sequence_output_masked = sequence_output[masked_pos==1, :]\n",
        "            class_logits = self.cls(sequence_output_masked)\n",
        "            masked_ids = masked_ids[masked_ids != 0]   # remove padding masks\n",
        "            masked_loss = self.loss(class_logits.float(), masked_ids)\n",
        "            outputs = (masked_loss, class_logits,) + outputs[2:]\n",
        "        else:\n",
        "            sequence_output = outputs[0][:, :input_ids.shape[-1], :]\n",
        "            class_logits = self.cls(sequence_output)\n",
        "            outputs = (class_logits,) + outputs[2:]\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def prepare_inputs_for_generation(self, curr_ids, past=None):\n",
        "        # NOTE: if attention is on, it should be the token used to mask words in training\n",
        "        mask_token_id = self.mask_token_id\n",
        "        batch_size = curr_ids.shape[0]\n",
        "        mask_ids = torch.full(\n",
        "            (batch_size, 1), mask_token_id, dtype=torch.long, device=curr_ids.device\n",
        "        )\n",
        "\n",
        "        def _slice(t, start, end):\n",
        "            if t is None:\n",
        "                return t\n",
        "            assert t.shape == (batch_size, self.max_seq_len + self.od_labels_len)\n",
        "            return t[:, start: end]\n",
        "\n",
        "        def _remove_elements(t, start, end):\n",
        "            if t is None:\n",
        "                return t\n",
        "            assert t.shape == (batch_size, self.max_seq_len + self.od_labels_len)\n",
        "            return torch.cat([t[:, :start], t[:, end:]], dim=1)\n",
        "\n",
        "        if past is None:\n",
        "            input_ids = torch.cat([curr_ids, mask_ids], dim=1)\n",
        "\n",
        "            curr_len = input_ids.shape[1]\n",
        "            full_len = self.max_seq_len + self.od_labels_len + self.img_seq_len\n",
        "            assert self.full_attention_mask.shape == (batch_size,\n",
        "                    full_len, full_len)\n",
        "\n",
        "            def _remove_rows_cols(t, row_start, row_end, col_start, col_end):\n",
        "                t00 = t[:, :row_start, :col_start]\n",
        "                t01 = t[:, :row_start, col_end:]\n",
        "                t10 = t[:, row_end:, :col_start]\n",
        "                t11 = t[:, row_end:, col_end:]\n",
        "                res = torch.cat([torch.cat([t00, t01], dim=2), torch.cat([t10, t11],\n",
        "                            dim=2)], dim=1)\n",
        "                assert res.shape == (t.shape[0], t.shape[1]-row_end+row_start,\n",
        "                        t.shape[2]-col_end+col_start)\n",
        "                return res\n",
        "\n",
        "            seq_start = curr_len\n",
        "            seq_end = self.max_seq_len\n",
        "            attention_mask = _remove_rows_cols(self.full_attention_mask, seq_start,\n",
        "                    seq_end, seq_start, seq_end)\n",
        "\n",
        "            masked_pos = _remove_elements(self.full_masked_pos, seq_start, seq_end)\n",
        "            token_type_ids = _remove_elements(self.full_token_type_ids, seq_start, seq_end)\n",
        "            position_ids = _remove_elements(self.full_position_ids, seq_start, seq_end)\n",
        "            img_feats = self.img_feats\n",
        "\n",
        "            if self.add_od_labels:\n",
        "                assert self.od_label_ids.shape[1] == self.od_labels_len\n",
        "                input_ids = torch.cat([input_ids, self.od_label_ids], dim=1)\n",
        "        else:\n",
        "            last_token = curr_ids[:, -1:]\n",
        "            # The representation of last token should be re-computed, because\n",
        "            # it depends on both self-attention context and input tensor\n",
        "            input_ids = torch.cat([last_token, mask_ids], dim=1)\n",
        "            start_pos = curr_ids.shape[1] - 1\n",
        "            end_pos = start_pos + input_ids.shape[1]\n",
        "            masked_pos = _slice(self.full_masked_pos, start_pos, end_pos)\n",
        "            token_type_ids = _slice(self.full_token_type_ids, start_pos, end_pos)\n",
        "            position_ids = _slice(self.full_position_ids, start_pos, end_pos)\n",
        "\n",
        "            img_feats = None\n",
        "            assert past[0].shape[0] == batch_size\n",
        "            if self.prev_encoded_layers is None:\n",
        "                assert start_pos == 1  # the first token after BOS\n",
        "                assert past[0].shape[1] == 2 + self.od_labels_len + self.img_seq_len\n",
        "                # reorder to [od_labels, img_feats, sentence]\n",
        "                self.prev_encoded_layers = [\n",
        "                        torch.cat([x[:, 2:, :], x[:, :start_pos,:]], dim=1)\n",
        "                        for x in past]\n",
        "                s2s = self.full_attention_mask[:, :self.max_seq_len,\n",
        "                        :self.max_seq_len]\n",
        "                s2i = self.full_attention_mask[:, :self.max_seq_len,\n",
        "                        self.max_seq_len:]\n",
        "                i2s = self.full_attention_mask[:, self.max_seq_len:,\n",
        "                        :self.max_seq_len]\n",
        "                i2i = self.full_attention_mask[:, self.max_seq_len:,\n",
        "                        self.max_seq_len:]\n",
        "                self.full_attention_mask = torch.cat(\n",
        "                        [torch.cat([i2i, i2s], dim=2),\n",
        "                        torch.cat([s2i, s2s], dim=2)],\n",
        "                        dim=1)\n",
        "            else:\n",
        "                assert start_pos > 1\n",
        "                assert past[0].shape[1] == 2\n",
        "                self.prev_encoded_layers = [torch.cat([x, p[:, :-1, :]], dim=1)\n",
        "                        for x, p in zip(self.prev_encoded_layers, past)]\n",
        "\n",
        "            attention_mask = self.full_attention_mask[:,\n",
        "                self.od_labels_len+self.img_seq_len+start_pos: self.od_labels_len+self.img_seq_len+end_pos,\n",
        "                :self.od_labels_len+self.img_seq_len+end_pos]\n",
        "\n",
        "        return {'input_ids': input_ids, 'img_feats': img_feats,\n",
        "            'masked_pos': masked_pos, 'attention_mask': attention_mask,\n",
        "            'token_type_ids': token_type_ids, 'position_ids': position_ids,\n",
        "            'is_training': False,\n",
        "            'encoder_history_states': self.prev_encoded_layers}\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.decoder\n",
        "\n",
        "    def generate(self, img_feats, attention_mask, masked_pos, token_type_ids=None,\n",
        "            position_ids=None, head_mask=None, input_ids=None, max_length=None,\n",
        "            do_sample=None, num_beams=None, temperature=None, top_k=None, top_p=None,\n",
        "            repetition_penalty=None, bos_token_id=None, pad_token_id=None,\n",
        "            eos_token_ids=None, mask_token_id=None, length_penalty=None,\n",
        "            num_return_sequences=None,\n",
        "            num_keep_best=1, is_decode=None,\n",
        "            add_od_labels=False, od_labels_start_posid=None,\n",
        "            use_cbs=False, fsm=None, num_constraints=None,\n",
        "            min_constraints_to_satisfy=None, use_hypo=False,\n",
        "            decoding_constraint_flag=None, bad_ending_ids=None,\n",
        "            ):\n",
        "        \"\"\" Generates captions given image features\n",
        "        \"\"\"\n",
        "        assert is_decode\n",
        "        batch_size = img_feats.shape[0]\n",
        "        self.img_seq_len = img_feats.shape[1]\n",
        "        self.max_seq_len = max_length\n",
        "        self.mask_token_id = mask_token_id\n",
        "        self.prev_encoded_layers = None\n",
        "        # NOTE: num_keep_best is not equavilant to num_return_sequences\n",
        "        # num_keep_best is the number of hypotheses to keep in beam search\n",
        "        # num_return_sequences is the repeating times of input, coupled with\n",
        "        # do_sample=True can generate more than one samples per image\n",
        "        self.num_keep_best = num_keep_best\n",
        "\n",
        "        vocab_size = self.config.vocab_size\n",
        "        if not use_cbs:\n",
        "            num_fsm_states = 1\n",
        "        else:\n",
        "            b, num_fsm_states, f1, v = fsm.shape\n",
        "            assert b==batch_size and v==vocab_size and f1==num_fsm_states\n",
        "\n",
        "        self.add_od_labels = add_od_labels\n",
        "        # avoid position_ids collision of caption and od labels\n",
        "        self.od_labels_start_posid = max(od_labels_start_posid, self.max_seq_len)\n",
        "        if self.add_od_labels:\n",
        "            # get od labels part from input_ids\n",
        "            assert input_ids.shape[0] == batch_size\n",
        "            od_label_ids = input_ids[:, self.max_seq_len:]\n",
        "            self.od_labels_len = input_ids.shape[1] - self.max_seq_len\n",
        "            input_ids = None\n",
        "        else:\n",
        "            self.od_labels_len = 0\n",
        "            od_label_ids = None\n",
        "            print(input_ids.shape)\n",
        "            print((batch_size, self.max_seq_len))\n",
        "            # assert input_ids.shape == (batch_size, self.max_seq_len)\n",
        "\n",
        "            input_ids = None\n",
        "\n",
        "        if input_ids is None:\n",
        "            input_ids = torch.full(\n",
        "                (batch_size, 1), bos_token_id, dtype=torch.long, device=img_feats.device\n",
        "            )\n",
        "        else:\n",
        "            assert input_ids.dim() == 2, \"Input prompt should be of shape (batch_size, sequence length).\"\n",
        "            assert input_ids.shape[0] == batch_size, \"Input batch size must match image features\"\n",
        "\n",
        "        cur_len = input_ids.shape[1]\n",
        "        if  num_return_sequences != 1:\n",
        "            # Expand input to num return sequences\n",
        "            input_ids = self._expand_for_beams(input_ids, num_return_sequences)\n",
        "            effective_batch_size = batch_size * num_return_sequences\n",
        "        else:\n",
        "            effective_batch_size = batch_size\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = torch.arange(self.max_seq_len, dtype=torch.long, device=input_ids.device)\n",
        "            posids_len = self.max_seq_len\n",
        "            if self.add_od_labels:\n",
        "                od_labels_posids = torch.arange(\n",
        "                        self.od_labels_start_posid,\n",
        "                        self.od_labels_start_posid + self.od_labels_len, dtype=torch.long, device=input_ids.device)\n",
        "                position_ids = torch.cat([position_ids, od_labels_posids])\n",
        "                posids_len += self.od_labels_len\n",
        "            position_ids = position_ids.unsqueeze(0).expand([batch_size, posids_len])\n",
        "\n",
        "        num_expand = num_beams * num_fsm_states * num_return_sequences\n",
        "        self.od_label_ids = self._expand_for_beams(od_label_ids, num_expand)\n",
        "        self.img_feats = self._expand_for_beams(img_feats, num_expand)\n",
        "        self.full_attention_mask = self._expand_for_beams(attention_mask, num_expand)\n",
        "        self.full_masked_pos = self._expand_for_beams(masked_pos, num_expand)\n",
        "        self.full_token_type_ids = self._expand_for_beams(token_type_ids, num_expand)\n",
        "        self.full_position_ids = self._expand_for_beams(position_ids, num_expand)\n",
        "        self.full_head_mask = self._expand_for_beams(head_mask, num_expand)\n",
        "\n",
        "        if not use_cbs:\n",
        "            if num_beams > 1:\n",
        "                output = self._generate_beam_search(\n",
        "                    input_ids,\n",
        "                    cur_len,\n",
        "                    max_length,\n",
        "                    do_sample,\n",
        "                    temperature,\n",
        "                    top_k,\n",
        "                    top_p,\n",
        "                    repetition_penalty,\n",
        "                    pad_token_id,\n",
        "                    eos_token_ids,\n",
        "                    effective_batch_size,\n",
        "                    length_penalty,\n",
        "                    num_beams,\n",
        "                    vocab_size,\n",
        "                )\n",
        "            else:\n",
        "                output = self._generate_no_beam_search(\n",
        "                    input_ids,\n",
        "                    cur_len,\n",
        "                    max_length,\n",
        "                    do_sample,\n",
        "                    temperature,\n",
        "                    top_k,\n",
        "                    top_p,\n",
        "                    repetition_penalty,\n",
        "                    pad_token_id,\n",
        "                    eos_token_ids,\n",
        "                    effective_batch_size,\n",
        "                )\n",
        "        else:\n",
        "            assert self.num_keep_best == 1, 'not supported n_best > 1 for CBS'\n",
        "            searcher = ConstrainedBeamSearch(eos_token_ids, max_length,\n",
        "                    num_beams)\n",
        "            curr_ids, sum_logprobs = searcher.search(\n",
        "                    input_ids,\n",
        "                    None,\n",
        "                    self._decode_step,\n",
        "                    fsm,\n",
        "            )\n",
        "            curr_ids, logprobs = select_best_beam_with_constraints(\n",
        "                curr_ids,\n",
        "                sum_logprobs,\n",
        "                num_constraints,\n",
        "                min_constraints_to_satisfy,\n",
        "                eos_token_ids,\n",
        "            )\n",
        "            # (batch_size, n_best, max_len), (batch_size, n_best)\n",
        "            output = (curr_ids.unsqueeze(1), logprobs.unsqueeze(1))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _expand_for_beams(self, x, num_expand):\n",
        "        if x is None or num_expand == 1:\n",
        "            return x\n",
        "\n",
        "        input_shape = list(x.shape)\n",
        "        expanded_shape = input_shape[:1] + [num_expand] + input_shape[1:]\n",
        "        x = x.unsqueeze(1).expand(expanded_shape)\n",
        "        # (batch_size * num_expand, ...)\n",
        "        x = x.contiguous().view([input_shape[0] * num_expand] + input_shape[1:])\n",
        "        return x\n",
        "\n",
        "    def _do_output_past(self, outputs):\n",
        "        return len(outputs) > 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLaa4Izd4o0i"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFvdqUMW4pP5"
      },
      "source": [
        "## Driver for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uONXz2DAyNNa",
        "outputId": "fcf15029-839b-4cb5-e841-6d946f0ef3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'NoneType'>\n",
            "2021-12-09 03:09:56,917 vlpretrain WARNING: Device: cuda, n_gpu: 1\n",
            "/content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000/training_args.bin\n",
            "2021-12-09 03:09:58,646 vlpretrain WARNING: Override max_seq_length to 50 = max_gen_length:20 + od_labels_len:30\n",
            "2021-12-09 03:09:58,647 vlpretrain WARNING: Override do_lower_case with train args: False -> True\n",
            "2021-12-09 03:09:58,649 vlpretrain WARNING: Override add_od_labels with train args: True -> False\n",
            "2021-12-09 03:10:00,592 vlpretrain INFO: Evaluate the following checkpoint: /content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000\n",
            "2021-12-09 03:10:21,208 vlpretrain INFO: Training/evaluation parameters <__main__.Arguments object at 0x7f6342175050>\n",
            "2021-12-09 03:10:21,210 vlpretrain INFO: Evaluate on dataset: test.yaml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/content/drive/MyDrive/OscarPlus/oscar/modeling/modeling_utils.py:506: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beam_id = idx // vocab_size\n",
            "79it [25:27, 19.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-12-09 03:35:52,604 vlpretrain INFO: Inference model computing time: 19.28908288931545 seconds per batch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:vlpretrain:Inference model computing time: 19.28908288931545 seconds per batch\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "0:00:01.070275\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...     \n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "tokenization...\n",
            "setting up scorers...\n",
            "computing Bleu score...\n",
            "{'testlen': 52669, 'reflen': 51042, 'guess': [52669, 47669, 42669, 37669], 'correct': [38961, 21626, 11177, 5793]}\n",
            "ratio: 1.0318757101994234\n",
            "Bleu_1: 0.740\n",
            "Bleu_2: 0.579\n",
            "Bleu_3: 0.445\n",
            "Bleu_4: 0.341\n",
            "computing METEOR score...\n",
            "METEOR: 0.290\n",
            "computing Rouge score...\n",
            "ROUGE_L: 0.568\n",
            "computing CIDEr score...\n",
            "CIDEr: 1.157\n",
            "computing SPICE score...\n",
            "SPICE: 0.221\n",
            "2021-12-09 03:37:45,843 vlpretrain INFO: evaluation result: {'Bleu_1': 0.7397330498015771, 'Bleu_2': 0.5793054174827693, 'Bleu_3': 0.4446408114409983, 'Bleu_4': 0.34098623749468476, 'METEOR': 0.29049820238722385, 'ROUGE_L': 0.5684687596555889, 'CIDEr': 1.1574415975737964, 'SPICE': 0.22115992464983875}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:vlpretrain:evaluation result: {'Bleu_1': 0.7397330498015771, 'Bleu_2': 0.5793054174827693, 'Bleu_3': 0.4446408114409983, 'Bleu_4': 0.34098623749468476, 'METEOR': 0.29049820238722385, 'ROUGE_L': 0.5684687596555889, 'CIDEr': 1.1574415975737964, 'SPICE': 0.22115992464983875}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-12-09 03:37:45,846 vlpretrain INFO: evaluation result saved to /content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000/pred.coco_caption.test.beam5.max20.odlabels.eval.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:vlpretrain:evaluation result saved to /content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000/pred.coco_caption.test.beam5.max20.odlabels.eval.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-12-09 03:37:45,849 vlpretrain INFO: Evaluation results saved to: /content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000/pred.coco_caption.test.beam5.max20.odlabels.eval.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:vlpretrain:Evaluation results saved to: /content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000/pred.coco_caption.test.beam5.max20.odlabels.eval.json\n"
          ]
        }
      ],
      "source": [
        "# Copyright (c) 2021 Microsoft Corporation. Licensed under the MIT license.\n",
        "\n",
        "import argparse\n",
        "import base64\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path as op\n",
        "import random, time, json\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "from oscar.utils.logger import setup_logger\n",
        "from oscar.utils.tsv_file import TSVFile\n",
        "from oscar.utils.tsv_file_ops import (tsv_writer, concat_tsv_files,\n",
        "        delete_tsv_files, reorder_tsv_keys)\n",
        "from oscar.utils.misc import (mkdir, set_seed, \n",
        "        load_from_yaml_file, find_file_path_in_yaml)\n",
        "from oscar.utils.caption_evaluate import (evaluate_on_coco_caption,\n",
        "        ScstRewardCriterion)\n",
        "from oscar.utils.cbs import ConstraintFilter, ConstraintBoxesReader\n",
        "from oscar.utils.cbs import FiniteStateMachineBuilder\n",
        "# from oscar.modeling.modeling_bert import BertForImageCaptioning\n",
        "from transformers.pytorch_transformers import BertTokenizer, BertConfig\n",
        "from transformers.pytorch_transformers import AdamW, WarmupLinearSchedule, WarmupConstantSchedule\n",
        "\n",
        "\n",
        "class CaptionTSVDataset(Dataset):\n",
        "    def __init__(self, yaml_file, tokenizer=None, add_od_labels=True,\n",
        "            max_img_seq_length=50, max_seq_length=70, max_seq_a_length=40, \n",
        "            is_train=True, mask_prob=0.15, max_masked_tokens=3, overfit=False, **kwargs):\n",
        "        \"\"\"Constructor.\n",
        "        Args:\n",
        "            yaml file with all required data (image feature, caption, labels, etc)\n",
        "            tokenizer: tokenizer for text processing.\n",
        "            add_od_labels: whether to add labels from yaml file to BERT. \n",
        "            max_img_seq_length: max image sequence length.\n",
        "            max_seq_length: max text sequence length.\n",
        "            max_seq_a_length: max caption sequence length.\n",
        "            is_train: train or test mode.\n",
        "            mask_prob: probability to mask a input token.\n",
        "            max_masked_tokens: maximum number of tokens to be masked in one sentence.\n",
        "            overfit: provide a small dataset to test pipeline\n",
        "            kwargs: other arguments.\n",
        "        \"\"\"\n",
        "        self.yaml_file = yaml_file\n",
        "        self.cfg = load_from_yaml_file(yaml_file)\n",
        "        self.root = op.dirname(yaml_file)\n",
        "        self.label_file = find_file_path_in_yaml(self.cfg['label'], self.root)\n",
        "        self.feat_file = find_file_path_in_yaml(self.cfg['feature'], self.root)\n",
        "        self.caption_file = find_file_path_in_yaml(self.cfg.get('caption'), self.root)\n",
        "\n",
        "        self.overfit = overfit\n",
        "\n",
        "        assert op.isfile(self.feat_file)\n",
        "        if add_od_labels: assert op.isfile(self.label_file)\n",
        "        if is_train: assert op.isfile(self.caption_file) and tokenizer is not None\n",
        "\n",
        "        self.label_tsv = None if not self.label_file else TSVFile(self.label_file)\n",
        "        self.feat_tsv = TSVFile(self.feat_file)\n",
        "        \n",
        "        self.captions = []\n",
        "        if self.caption_file and op.isfile(self.caption_file):\n",
        "            if self.overfit == False:\n",
        "                with open(self.caption_file, 'r') as f:\n",
        "                    self.captions = json.load(f)\n",
        "            else:\n",
        "                with open(\"/content/drive/MyDrive/OscarPlus/datasets/coco_caption/test_caption_overfit.json\", 'r') as f:\n",
        "                    self.captions = json.load(f)                \n",
        "        # if self.overfit == True:\n",
        "        #     self.captions = self.captions[0:100]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tensorizer = CaptionTensorizer(self.tokenizer, max_img_seq_length,\n",
        "                max_seq_length, max_seq_a_length, mask_prob, max_masked_tokens,\n",
        "                is_train=is_train)\n",
        "        self.add_od_labels = add_od_labels\n",
        "        self.is_train = is_train\n",
        "        self.kwargs = kwargs\n",
        "        self.image_keys = self.prepare_image_keys()\n",
        "        self.key2index = self.prepare_image_key_to_index()\n",
        "        self.key2captions = self.prepare_image_key_to_captions()\n",
        "\n",
        "    def get_valid_tsv(self):\n",
        "        # based on the order of file size\n",
        "        if self.label_tsv:\n",
        "            return self.label_tsv\n",
        "        if self.feat_tsv:\n",
        "            return self.feat_tsv\n",
        "\n",
        "    def prepare_image_keys(self):\n",
        "        tsv = self.get_valid_tsv()\n",
        "        if self.overfit == True:\n",
        "            try:\n",
        "                return [tsv.seek(i)[0] for i in range(100)]\n",
        "            except Exception as e:\n",
        "                print(\"Original dataset smaller than 100!\")\n",
        "                raise\n",
        "        else:\n",
        "            return [tsv.seek(i)[0] for i in range(tsv.num_rows())]\n",
        "\n",
        "    def prepare_image_key_to_index(self):\n",
        "        tsv = self.get_valid_tsv()\n",
        "        if self.overfit == True:\n",
        "            try:\n",
        "                return {tsv.seek(i)[0] : i for i in range(100)}\n",
        "            except Exception as e:\n",
        "                print(\"Original dataset smaller than 100!\")\n",
        "                raise\n",
        "        else:\n",
        "            return {tsv.seek(i)[0] : i for i in range(tsv.num_rows())}\n",
        "\n",
        "    def prepare_image_key_to_captions(self):\n",
        "        if self.captions:\n",
        "            key2captions = {key: [] for key in self.image_keys}\n",
        "            for cap in self.captions:\n",
        "                key2captions[cap['image_id']].append(cap['caption'])\n",
        "            # for key in self.image_keys:\n",
        "            #     key2captions[key].append(cap['caption'])\n",
        "            return key2captions\n",
        "\n",
        "    def get_image_index(self, idx):\n",
        "        if self.is_train:\n",
        "            img_cap_pair = self.captions[idx]\n",
        "            img_key = img_cap_pair['image_id']\n",
        "            return self.key2index[img_key]\n",
        "        return idx\n",
        "\n",
        "    def get_image_key(self, idx):\n",
        "        img_idx = self.get_image_index(idx)\n",
        "        return self.image_keys[img_idx]\n",
        "\n",
        "    def get_image_features(self, img_idx):\n",
        "        feat_info = json.loads(self.feat_tsv.seek(img_idx)[1])\n",
        "        num_boxes = feat_info['num_boxes']\n",
        "        features = np.frombuffer(base64.b64decode(feat_info['features']), np.float32\n",
        "                ).reshape((num_boxes, -1))\n",
        "        return torch.Tensor(features)\n",
        "\n",
        "    def get_caption(self, idx):\n",
        "        if self.is_train:\n",
        "            img_cap_pair = self.captions[idx]\n",
        "            return img_cap_pair['caption']\n",
        "        return \"\"\n",
        "\n",
        "    def get_od_labels(self, img_idx):\n",
        "        od_labels = None\n",
        "        if self.add_od_labels:\n",
        "            label_info = json.loads(self.label_tsv.seek(img_idx)[1])\n",
        "            od_labels = \" \".join([l['class'] for l in label_info])\n",
        "        return od_labels\n",
        "\n",
        "    def get_caption_file_in_coco_format(self):\n",
        "        cap_file = op.splitext(self.caption_file)[0] + '_coco_format.json'\n",
        "        return cap_file\n",
        "\n",
        "    def get_captions_by_key(self, key):\n",
        "        return self.key2captions[key]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_idx = self.get_image_index(idx)\n",
        "        img_key = self.image_keys[img_idx]\n",
        "        features = self.get_image_features(img_idx)\n",
        "        caption = self.get_caption(idx)\n",
        "        od_labels = self.get_od_labels(img_idx)\n",
        "        example = self.tensorizer.tensorize_example(caption, features, text_b=od_labels)\n",
        "        return img_key, example\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.is_train or self.overfit:\n",
        "            return len(self.captions)\n",
        "        return self.get_valid_tsv().num_rows()\n",
        "\n",
        "\n",
        "class CaptionTSVDatasetWithConstraints(CaptionTSVDataset):\n",
        "    r\"\"\"\n",
        "    Providing inputs for inference with Constraint Beam Search\n",
        "\n",
        "    nms_threshold: float, optional (default = 0.85)\n",
        "        NMS threshold for suppressing generic object class names during constraint filtering,\n",
        "        for two boxes with IoU higher than this threshold, \"dog\" suppresses \"animal\".\n",
        "    max_given_constraints: int, optional (default = 3)\n",
        "        Maximum number of constraints which can be specified for CBS decoding. Constraints are\n",
        "        selected based on the prediction confidence score of their corresponding bounding boxes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, yaml_file,\n",
        "        nms_threshold=0.85,\n",
        "        max_given_constraints=3, **kwargs\n",
        "    ):\n",
        "        super().__init__(yaml_file, **kwargs)\n",
        "        boxes_tsvpath = find_file_path_in_yaml(self.cfg['cbs_box'], self.root)\n",
        "        constraint2tokens_tsvpath = find_file_path_in_yaml(self.cfg['cbs_constraint'], self.root)\n",
        "        tokenforms_tsvpath = find_file_path_in_yaml(self.cfg['cbs_tokenforms'], self.root)\n",
        "        hierarchy_jsonpath = find_file_path_in_yaml(self.cfg['cbs_hierarchy'], self.root)\n",
        "\n",
        "        self._boxes_reader = ConstraintBoxesReader(boxes_tsvpath)\n",
        "        self._constraint_filter = ConstraintFilter(\n",
        "            hierarchy_jsonpath, nms_threshold, max_given_constraints\n",
        "        )\n",
        "        self._fsm_builder = FiniteStateMachineBuilder(self.tokenizer,\n",
        "                constraint2tokens_tsvpath, tokenforms_tsvpath,\n",
        "                max_given_constraints)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_key, example = super().__getitem__(index)\n",
        "\n",
        "        # Apply constraint filtering to object class names.\n",
        "        constraint_boxes = self._boxes_reader[img_key]\n",
        "\n",
        "        candidates = self._constraint_filter(\n",
        "            constraint_boxes[\"boxes\"], constraint_boxes[\"class_names\"], constraint_boxes[\"scores\"]\n",
        "        )\n",
        "        num_constraints = len(candidates)\n",
        "        fsm, nstates = self._fsm_builder.build(candidates)\n",
        "\n",
        "        return img_key, example + (fsm, num_constraints, )\n",
        "\n",
        "\n",
        "class CaptionTensorizer(object):\n",
        "    def __init__(self, tokenizer, max_img_seq_length=50, max_seq_length=70, \n",
        "            max_seq_a_length=40, mask_prob=0.15, max_masked_tokens=3,\n",
        "            is_train=True):\n",
        "        \"\"\"Constructor.\n",
        "        Args:\n",
        "            tokenizer: tokenizer for text processing.\n",
        "            max_img_seq_length: max image sequence length.\n",
        "            max_seq_length: max text sequence length.\n",
        "            max_seq_a_length: max caption sequence length.\n",
        "            is_train: train or test mode.\n",
        "            mask_prob: probability to mask a input token.\n",
        "            max_masked_tokens: maximum number of tokens to be masked in one sentence.\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.is_train = is_train\n",
        "        self.max_img_seq_len = max_img_seq_length\n",
        "        self.max_seq_len = max_seq_length\n",
        "        self.max_seq_a_len = max_seq_a_length\n",
        "        self.mask_prob = mask_prob\n",
        "        self.max_masked_tokens = max_masked_tokens\n",
        "        self._triangle_mask = torch.tril(torch.ones((self.max_seq_len, \n",
        "            self.max_seq_len), dtype=torch.long))\n",
        "\n",
        "    def tensorize_example(self, text_a, img_feat, text_b=None,\n",
        "            cls_token_segment_id=0, pad_token_segment_id=0,\n",
        "            sequence_a_segment_id=0, sequence_b_segment_id=1):\n",
        "        if self.is_train:\n",
        "            tokens_a = self.tokenizer.tokenize(text_a)\n",
        "        else:\n",
        "            # fake tokens to generate masks\n",
        "            tokens_a = [self.tokenizer.mask_token] * (self.max_seq_a_len - 2)\n",
        "        if len(tokens_a) > self.max_seq_a_len - 2:\n",
        "            tokens_a = tokens_a[:(self.max_seq_a_len - 2)]\n",
        "\n",
        "        tokens = [self.tokenizer.cls_token] + tokens_a + [self.tokenizer.sep_token]\n",
        "        segment_ids = [cls_token_segment_id] + [sequence_a_segment_id] * (len(tokens) - 1)\n",
        "        seq_a_len = len(tokens)\n",
        "        if text_b:\n",
        "            # pad text_a to keep it in fixed length for better inference.\n",
        "            padding_a_len = self.max_seq_a_len - seq_a_len\n",
        "            tokens += [self.tokenizer.pad_token] * padding_a_len\n",
        "            segment_ids += ([pad_token_segment_id] * padding_a_len)\n",
        "\n",
        "            tokens_b = self.tokenizer.tokenize(text_b)\n",
        "            if len(tokens_b) > self.max_seq_len - len(tokens) - 1:\n",
        "                tokens_b = tokens_b[: (self.max_seq_len - len(tokens) - 1)]\n",
        "            tokens += tokens_b + [self.tokenizer.sep_token]\n",
        "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "        seq_len = len(tokens)\n",
        "        if self.is_train:\n",
        "            masked_pos = torch.zeros(self.max_seq_len, dtype=torch.int)\n",
        "            # randomly mask words for prediction, ignore [CLS]\n",
        "            candidate_masked_idx = list(range(1, seq_a_len)) # only mask text_a\n",
        "            random.shuffle(candidate_masked_idx)\n",
        "            num_masked = min(max(round(self.mask_prob * seq_a_len), 1), self.max_masked_tokens)\n",
        "            num_masked = int(num_masked)\n",
        "            masked_idx = candidate_masked_idx[:num_masked]\n",
        "            masked_idx = sorted(masked_idx)\n",
        "            masked_token = [tokens[i] for i in masked_idx]\n",
        "            for pos in masked_idx:\n",
        "                if random.random() <= 0.8:\n",
        "                    # 80% chance to be a ['MASK'] token\n",
        "                    tokens[pos] = self.tokenizer.mask_token\n",
        "                elif random.random() <= 0.5:\n",
        "                    # 10% chance to be a random word ((1-0.8)*0.5)\n",
        "                    from random import randint\n",
        "                    i = randint(0, len(self.tokenizer.vocab))\n",
        "                    self.tokenizer._convert_id_to_token(i)\n",
        "                    tokens[pos] = self.tokenizer._convert_id_to_token(i)\n",
        "                else:\n",
        "                    # 10% chance to remain the same (1-0.8-0.1)\n",
        "                    pass\n",
        "\n",
        "            masked_pos[masked_idx] = 1 \n",
        "            # pad masked tokens to the same length\n",
        "            if num_masked < self.max_masked_tokens:\n",
        "                masked_token = masked_token + ([self.tokenizer.pad_token] *\n",
        "                        (self.max_masked_tokens - num_masked))\n",
        "            masked_ids = self.tokenizer.convert_tokens_to_ids(masked_token)\n",
        "        else:\n",
        "            masked_pos = torch.ones(self.max_seq_len, dtype=torch.int)\n",
        "\n",
        "        # pad on the right for image captioning\n",
        "        padding_len = self.max_seq_len - seq_len\n",
        "        tokens = tokens + ([self.tokenizer.pad_token] * padding_len)\n",
        "        segment_ids += ([pad_token_segment_id] * padding_len)\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # image features\n",
        "        img_len = img_feat.shape[0]\n",
        "        if img_len > self.max_img_seq_len:\n",
        "            img_feat = img_feat[0 : self.max_img_seq_len, ]\n",
        "            img_len = img_feat.shape[0]\n",
        "        else:\n",
        "            padding_matrix = torch.zeros((self.max_img_seq_len - img_len,\n",
        "                                          img_feat.shape[1]))\n",
        "            img_feat = torch.cat((img_feat, padding_matrix), 0)\n",
        "\n",
        "        # prepare attention mask:\n",
        "        # note that there is no attention from caption to image\n",
        "        # because otherwise it will violate the triangle attention \n",
        "        # for caption as caption will have full attention on image. \n",
        "        max_len = self.max_seq_len + self.max_img_seq_len\n",
        "        attention_mask = torch.zeros((max_len, max_len), dtype=torch.long)\n",
        "        # C: caption, L: label, R: image region\n",
        "        c_start, c_end = 0, seq_a_len\n",
        "        l_start, l_end = self.max_seq_a_len, seq_len\n",
        "        r_start, r_end = self.max_seq_len, self.max_seq_len + img_len\n",
        "        # triangle mask for caption to caption\n",
        "        attention_mask[c_start : c_end, c_start : c_end].copy_(self._triangle_mask[0 : seq_a_len, 0 : seq_a_len])\n",
        "        # full attention for L-L, R-R\n",
        "        attention_mask[l_start : l_end, l_start : l_end] = 1\n",
        "        attention_mask[r_start : r_end, r_start : r_end] = 1\n",
        "        # full attention for C-L, C-R\n",
        "        attention_mask[c_start : c_end, l_start : l_end] = 1\n",
        "        attention_mask[c_start : c_end, r_start : r_end] = 1\n",
        "        # full attention for L-R:\n",
        "        attention_mask[l_start : l_end, r_start : r_end] = 1\n",
        "        attention_mask[r_start : r_end, l_start : l_end] = 1\n",
        "\n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "        segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
        "\n",
        "        if self.is_train:\n",
        "            masked_ids = torch.tensor(masked_ids, dtype=torch.long)\n",
        "            return (input_ids, attention_mask, segment_ids, img_feat, masked_pos, masked_ids)\n",
        "        return (input_ids, attention_mask, segment_ids, img_feat, masked_pos)\n",
        "\n",
        "\n",
        "def build_dataset(yaml_file, tokenizer, args, is_train=True):\n",
        "    if not op.isfile(yaml_file):\n",
        "        yaml_file = op.join(args.data_dir, yaml_file)\n",
        "        assert op.isfile(yaml_file)\n",
        "\n",
        "    if is_train:\n",
        "        return CaptionTSVDataset(yaml_file, tokenizer=tokenizer,\n",
        "            add_od_labels=args.add_od_labels, max_img_seq_length=args.max_img_seq_length,\n",
        "            max_seq_length=args.max_seq_length, max_seq_a_length=args.max_seq_a_length,\n",
        "            is_train=True, mask_prob=args.mask_prob, max_masked_tokens=args.max_masked_tokens)\n",
        "    if args.use_cbs:\n",
        "        dataset_class = CaptionTSVDatasetWithConstraints\n",
        "    else:\n",
        "        dataset_class = CaptionTSVDataset\n",
        "    return dataset_class(yaml_file, tokenizer=tokenizer,\n",
        "            add_od_labels=args.add_od_labels, max_img_seq_length=args.max_img_seq_length,\n",
        "            max_seq_length=args.max_seq_length, max_seq_a_length=args.max_gen_length,\n",
        "            is_train=False, overfit=args.overfit)\n",
        "\n",
        "\n",
        "def make_data_sampler(dataset, shuffle, distributed):\n",
        "    if distributed:\n",
        "        return torch.utils.data.distributed.DistributedSampler(dataset, shuffle=shuffle)\n",
        "    if shuffle:\n",
        "        sampler = torch.utils.data.sampler.RandomSampler(dataset)\n",
        "    else:\n",
        "        sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n",
        "    return sampler\n",
        "\n",
        "\n",
        "def make_data_loader(args, yaml_file, tokenizer, is_distributed=True, \n",
        "        is_train=True):\n",
        "    dataset = build_dataset(yaml_file, tokenizer, args, \n",
        "        is_train=(is_train and not args.scst))\n",
        "    if is_train:\n",
        "        shuffle = True\n",
        "        images_per_gpu = args.per_gpu_train_batch_size\n",
        "        images_per_batch = images_per_gpu * get_world_size()\n",
        "        iters_per_batch = len(dataset) // images_per_batch\n",
        "        num_iters = iters_per_batch * args.num_train_epochs\n",
        "        logger.info(\"Train with {} images per GPU.\".format(images_per_gpu))\n",
        "        logger.info(\"Total batch size {}\".format(images_per_batch))\n",
        "        logger.info(\"Total training steps {}\".format(num_iters))\n",
        "    else:\n",
        "        shuffle = False\n",
        "        images_per_gpu = args.per_gpu_eval_batch_size\n",
        "\n",
        "    sampler = make_data_sampler(dataset, shuffle, is_distributed)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, num_workers=args.num_workers, sampler=sampler,\n",
        "        batch_size=images_per_gpu,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "def save_checkpoint(model, tokenizer, args, epoch, iteration, num_trial=10):\n",
        "    checkpoint_dir = op.join(args.output_dir, 'checkpoint-{}-{}'.format(\n",
        "        epoch, iteration))\n",
        "    if not is_main_process():\n",
        "        return checkpoint_dir\n",
        "    mkdir(checkpoint_dir)\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    for i in range(num_trial):\n",
        "        try:\n",
        "            model_to_save.save_pretrained(checkpoint_dir)\n",
        "            torch.save(args, op.join(checkpoint_dir, 'training_args.bin'))\n",
        "            tokenizer.save_pretrained(checkpoint_dir)\n",
        "            logger.info(\"Save checkpoint to {}\".format(checkpoint_dir))\n",
        "            break\n",
        "        except:\n",
        "            pass\n",
        "    else:\n",
        "        logger.info(\"Failed to save checkpoint after {} trails.\".format(num_trial))\n",
        "    return checkpoint_dir\n",
        "\n",
        "\n",
        "def compute_score_with_logits(logits, labels):\n",
        "    logits = torch.max(logits, -1)[1].data # argmax\n",
        "    scores = logits == labels \n",
        "    return scores\n",
        "\n",
        "\n",
        "def train(args, train_dataloader, val_dataset, model, tokenizer):\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], \n",
        "            output_device=args.local_rank,\n",
        "            find_unused_parameters=True,\n",
        "        )\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // \\\n",
        "                args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps \\\n",
        "                * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and scheduler\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not \\\n",
        "            any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if \\\n",
        "            any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    if args.scheduler == \"constant\":\n",
        "        scheduler = WarmupConstantSchedule(\n",
        "                optimizer, warmup_steps=args.warmup_steps)\n",
        "    elif args.scheduler == \"linear\":\n",
        "        scheduler = WarmupLinearSchedule(\n",
        "                optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown scheduler type: {}\".format(args.scheduler))\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\"  Total train batch size (w. parallel, & accumulation) = %d\",\n",
        "                   args.per_gpu_train_batch_size * get_world_size() * args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    if args.scst:\n",
        "        scst_criterion = ScstRewardCriterion(\n",
        "            cider_cached_tokens=op.join(args.data_dir, args.cider_cached_tokens),\n",
        "            baseline_type=args.sc_baseline_type,\n",
        "        )\n",
        "        logger.info(\"  SCST training...\")\n",
        "\n",
        "\n",
        "    global_step, global_loss, global_acc =0,  0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    eval_log = []\n",
        "    best_score = 0\n",
        "    for epoch in range(int(args.num_train_epochs)):\n",
        "        for step, (img_keys, batch) in enumerate(train_dataloader):\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            if not args.scst:\n",
        "                model.train()\n",
        "                inputs = {'input_ids': batch[0], 'attention_mask': batch[1],\n",
        "                    'token_type_ids': batch[2], 'img_feats': batch[3], \n",
        "                    'masked_pos': batch[4], 'masked_ids': batch[5]\n",
        "                }\n",
        "                outputs = model(**inputs)\n",
        "                loss, logits = outputs[:2]\n",
        "                masked_ids = inputs['masked_ids']\n",
        "                masked_ids = masked_ids[masked_ids != 0]\n",
        "                batch_score = compute_score_with_logits(logits, masked_ids)\n",
        "                batch_acc = torch.sum(batch_score.float()) / torch.sum(inputs['masked_pos'])\n",
        "            else:\n",
        "                loss = scst_train_iter(args, train_dataloader, model, scst_criterion, img_keys, batch, tokenizer)\n",
        "                batch_acc = scst_criterion.get_score()\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "            global_loss += loss.item()\n",
        "            global_acc += batch_acc\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                global_step += 1\n",
        "                scheduler.step()\n",
        "                optimizer.step()\n",
        "                model.zero_grad()\n",
        "                if global_step % args.logging_steps == 0:\n",
        "                    logger.info(\"Epoch: {}, global_step: {}, lr: {:.6f}, loss: {:.4f} ({:.4f}), \" \\\n",
        "                        \"score: {:.4f} ({:.4f})\".format(epoch, global_step, \n",
        "                        optimizer.param_groups[0][\"lr\"], loss, global_loss / global_step, \n",
        "                        batch_acc, global_acc / global_step)\n",
        "                    )\n",
        "\n",
        "                if (args.save_steps > 0 and global_step % args.save_steps == 0) or \\\n",
        "                        global_step == t_total:\n",
        "                    checkpoint_dir = save_checkpoint(model, tokenizer, args, epoch, global_step) \n",
        "                    # evaluation\n",
        "                    if args.evaluate_during_training: \n",
        "                        logger.info(\"Perform evaluation at step: %d\" % (global_step))\n",
        "                        evaluate_file = evaluate(args, val_dataset, model, tokenizer,\n",
        "                                checkpoint_dir)\n",
        "                        with open(evaluate_file, 'r') as f:\n",
        "                            res = json.load(f)\n",
        "                        best_score = max(best_score, res['CIDEr'])\n",
        "                        res['epoch'] = epoch\n",
        "                        res['global_step'] = step\n",
        "                        res['best_CIDEr'] = best_score\n",
        "                        eval_log.append(res)\n",
        "                        with open(args.output_dir + '/eval_logs.json', 'w') as f:\n",
        "                            json.dump(eval_log, f)\n",
        "    return checkpoint_dir\n",
        "\n",
        "\n",
        "def scst_train_iter(args, train_dataloader, model, scst_criterion, \n",
        "        img_keys, batch, tokenizer):\n",
        "    cls_token_id, sep_token_id, pad_token_id, mask_token_id = \\\n",
        "        tokenizer.convert_tokens_to_ids([tokenizer.cls_token, \n",
        "        tokenizer.sep_token, tokenizer.pad_token, tokenizer.mask_token]\n",
        "    )\n",
        "    inputs = {'is_decode': True,\n",
        "        'input_ids': batch[0], 'attention_mask': batch[1],\n",
        "        'token_type_ids': batch[2], 'img_feats': batch[3],\n",
        "        'masked_pos': batch[4],\n",
        "        'do_sample': False,\n",
        "        'bos_token_id': cls_token_id,\n",
        "        'pad_token_id': pad_token_id,\n",
        "        'eos_token_ids': [sep_token_id],\n",
        "        'mask_token_id': mask_token_id,\n",
        "        # for adding od labels\n",
        "        'add_od_labels': args.add_od_labels, 'od_labels_start_posid': args.max_seq_a_length,\n",
        "        # hyperparameters of beam search\n",
        "        'max_length': args.max_gen_length,\n",
        "        'num_beams': args.sc_beam_size,\n",
        "        \"temperature\": args.temperature,\n",
        "        \"top_k\": args.top_k,\n",
        "        \"top_p\": args.top_p,\n",
        "        \"repetition_penalty\": args.repetition_penalty,\n",
        "        \"length_penalty\": args.length_penalty,\n",
        "        \"num_return_sequences\": 1,\n",
        "        \"num_keep_best\": 1,\n",
        "    }\n",
        "\n",
        "    def _ids_to_captions(all_ids):\n",
        "        captions = []\n",
        "        for ids in all_ids:\n",
        "            c = tokenizer.decode(ids.tolist(), skip_special_tokens=True)\n",
        "            captions.append(c)\n",
        "        return captions\n",
        "\n",
        "    if args.sc_baseline_type == 'greedy':\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            greedy_res_raw, _ = model(**inputs)\n",
        "            greedy_res_raw.squeeze_(1)  # batch_size * max_len\n",
        "        greedy_res = _ids_to_captions(greedy_res_raw)\n",
        "    else:\n",
        "        greedy_res = None\n",
        "\n",
        "    model.train()\n",
        "    inputs['do_sample'] = True\n",
        "    inputs['num_return_sequences'] = args.sc_train_sample_n\n",
        "    sample_res_raw, sample_logprobs = model(**inputs)\n",
        "    sample_res_raw.squeeze_(1)\n",
        "    sample_logprobs.squeeze_(1)\n",
        "    assert sample_logprobs.requires_grad == True\n",
        "    assert sample_res_raw.requires_grad == False\n",
        "    sample_res = _ids_to_captions(sample_res_raw)\n",
        "\n",
        "    gt_res = [train_dataloader.dataset.get_captions_by_key(k) for k in img_keys]\n",
        "    loss = scst_criterion(gt_res, greedy_res, sample_res, sample_logprobs)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def  get_predict_file(output_dir, yaml_file, args):\n",
        "    cc = ['pred']\n",
        "    # make sure it works with/without / in end of the path.\n",
        "    data = op.basename(op.join(args.data_dir, '')[:-1])\n",
        "    split = op.basename(yaml_file)\n",
        "    assert split.endswith('.yaml')\n",
        "    split = split[:-5]\n",
        "    cc.append(data)\n",
        "    cc.append(split)\n",
        "    cc.append('beam{}'.format(args.num_beams))\n",
        "    cc.append('max{}'.format(args.max_gen_length))\n",
        "    if args.add_od_labels:\n",
        "        cc.append('odlabels')\n",
        "    if args.num_keep_best != 1:\n",
        "        cc.append('best{}'.format(args.num_keep_best))\n",
        "    if args.use_cbs:\n",
        "        cc.append('cbs{}'.format(args.min_constraints_to_satisfy))\n",
        "    if args.output_hidden_states:\n",
        "        cc.append('hidden')\n",
        "    return op.join(output_dir, '{}.tsv'.format('.'.join(cc)))\n",
        "\n",
        "\n",
        "def get_evaluate_file(predict_file):\n",
        "    assert predict_file.endswith('.tsv')\n",
        "    fpath = op.splitext(predict_file)[0]\n",
        "    return fpath + '.eval.json'\n",
        "\n",
        "\n",
        "def get_evaluate_method(predict_file):\n",
        "    if 'nocaps' in op.basename(predict_file):\n",
        "        return 'nocaps'\n",
        "    else:\n",
        "        return 'coco'\n",
        "\n",
        "\n",
        "def evaluate(args, val_dataloader, model, tokenizer, output_dir):\n",
        "    predict_file = get_predict_file(output_dir,\n",
        "            val_dataloader.dataset.yaml_file, args)\n",
        "    test(args, val_dataloader, model, tokenizer, predict_file)\n",
        "\n",
        "    if get_world_size() > 1:\n",
        "        torch.distributed.barrier()\n",
        "    evaluate_file = get_evaluate_file(predict_file)\n",
        "    if is_main_process():\n",
        "        caption_file = val_dataloader.dataset.get_caption_file_in_coco_format()\n",
        "        data = val_dataloader.dataset.yaml_file.split('/')[-2]\n",
        "        if 'nocaps' not in data:\n",
        "            result = evaluate_on_coco_caption(predict_file, caption_file, outfile=evaluate_file)\n",
        "            logger.info('evaluation result: {}'.format(str(result)))\n",
        "            logger.info('evaluation result saved to {}'.format(evaluate_file))\n",
        "    if get_world_size() > 1:\n",
        "        torch.distributed.barrier()\n",
        "    return evaluate_file\n",
        "\n",
        "\n",
        "def test(args, test_dataloader, model, tokenizer, predict_file):\n",
        "    cls_token_id, sep_token_id, pad_token_id, mask_token_id, period_token_id = \\\n",
        "        tokenizer.convert_tokens_to_ids([tokenizer.cls_token, tokenizer.sep_token, \n",
        "        tokenizer.pad_token, tokenizer.mask_token, '.'])\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        cache_file = predict_file\n",
        "    else:\n",
        "        cache_file = op.splitext(predict_file)[0] + '_{}_{}'.format(get_rank(), \n",
        "                world_size) + op.splitext(predict_file)[1]\n",
        "\n",
        "    model.eval()\n",
        "    inputs_param = {'is_decode': True,\n",
        "        'do_sample': False,\n",
        "        'bos_token_id': cls_token_id,\n",
        "        'pad_token_id': pad_token_id,\n",
        "        'eos_token_ids': [sep_token_id],\n",
        "        'mask_token_id': mask_token_id,\n",
        "        # for adding od labels\n",
        "        'add_od_labels': args.add_od_labels, 'od_labels_start_posid': args.max_seq_a_length,\n",
        "\n",
        "        # hyperparameters of beam search\n",
        "        'max_length': args.max_gen_length,\n",
        "        'num_beams': args.num_beams,\n",
        "        \"temperature\": args.temperature,\n",
        "        \"top_k\": args.top_k,\n",
        "        \"top_p\": args.top_p,\n",
        "        \"repetition_penalty\": args.repetition_penalty,\n",
        "        \"length_penalty\": args.length_penalty,\n",
        "        \"num_return_sequences\": args.num_return_sequences,\n",
        "        \"num_keep_best\": args.num_keep_best,\n",
        "    }\n",
        "    if args.use_cbs:\n",
        "        inputs_param.update({'use_cbs': True,\n",
        "            'min_constraints_to_satisfy': args.min_constraints_to_satisfy,\n",
        "        })\n",
        "    def gen_rows():\n",
        "        time_meter = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step, (img_keys, batch) in tqdm(enumerate(test_dataloader)):\n",
        "                batch = tuple(t.to(args.device) for t in batch)\n",
        "                inputs = {\n",
        "                    'input_ids': batch[0], 'attention_mask': batch[1],\n",
        "                    'token_type_ids': batch[2], 'img_feats': batch[3],\n",
        "                    'masked_pos': batch[4],\n",
        "                }\n",
        "                if args.use_cbs:\n",
        "                    inputs.update({\n",
        "                        'fsm': batch[5],\n",
        "                        'num_constraints': batch[6],\n",
        "                    })\n",
        "                inputs.update(inputs_param)\n",
        "                tic = time.time()\n",
        "                # captions, logprobs\n",
        "                outputs = model(**inputs)\n",
        "                time_meter += time.time() - tic\n",
        "                all_caps = outputs[0]  # batch_size * num_keep_best * max_len\n",
        "                all_confs = torch.exp(outputs[1])\n",
        "\n",
        "                for img_key, caps, confs in zip(img_keys, all_caps, all_confs):\n",
        "                    res = []\n",
        "                    for cap, conf in zip(caps, confs):\n",
        "                        cap = tokenizer.decode(cap.tolist(), skip_special_tokens=True)\n",
        "                        res.append({'caption': cap, 'conf': conf.item()})\n",
        "                    if isinstance(img_key, torch.Tensor):\n",
        "                        img_key = img_key.item()\n",
        "                    yield img_key, json.dumps(res)\n",
        "\n",
        "        logger.info(\"Inference model computing time: {} seconds per batch\".format(time_meter / (step+1)))\n",
        "\n",
        "    tsv_writer(gen_rows(), cache_file)\n",
        "    if world_size > 1:\n",
        "        torch.distributed.barrier()\n",
        "    if world_size > 1 and is_main_process():\n",
        "        cache_files = [op.splitext(predict_file)[0] + '_{}_{}'.format(i, world_size) + \\\n",
        "            op.splitext(predict_file)[1] for i in range(world_size)]\n",
        "        concat_tsv_files(cache_files, predict_file)\n",
        "        delete_tsv_files(cache_files)\n",
        "        reorder_tsv_keys(predict_file, test_dataloader.dataset.image_keys, predict_file)\n",
        "    if world_size > 1:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "\n",
        "def restore_training_settings(args):\n",
        "    if args.do_train:\n",
        "        if not args.scst:\n",
        "            return args\n",
        "        checkpoint = args.model_name_or_path\n",
        "    else:\n",
        "        assert args.do_test or args.do_eval\n",
        "        checkpoint = args.eval_model_dir\n",
        "    # restore training settings, check hasattr for backward compatibility\n",
        "    print(op.join(checkpoint, 'training_args.bin'))\n",
        "    train_args = torch.load(op.join(checkpoint, 'training_args.bin'))\n",
        "\n",
        "    \n",
        "\n",
        "    if hasattr(train_args, 'max_seq_a_length'):\n",
        "        if hasattr(train_args, 'scst') and train_args.scst:\n",
        "            max_od_labels_len = train_args.max_seq_length - train_args.max_gen_length\n",
        "        else:\n",
        "            max_od_labels_len = train_args.max_seq_length - train_args.max_seq_a_length\n",
        "        max_seq_length = args.max_gen_length + max_od_labels_len\n",
        "        args.max_seq_length = max_seq_length\n",
        "        logger.warning('Override max_seq_length to {} = max_gen_length:{} + od_labels_len:{}'.format(\n",
        "                max_seq_length, args.max_gen_length, max_od_labels_len))\n",
        "\n",
        "\n",
        "    override_params = ['max_seq_a_length', 'do_lower_case', 'add_od_labels',\n",
        "            'max_img_seq_length']\n",
        "    for param in override_params:\n",
        "        if hasattr(train_args, param):\n",
        "            train_v = getattr(train_args, param)\n",
        "            test_v = getattr(args, param)\n",
        "            if train_v != test_v:\n",
        "                logger.warning('Override {} with train args: {} -> {}'.format(param,\n",
        "                    test_v, train_v))\n",
        "                setattr(args, param, train_v)\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not dist.is_available():\n",
        "        return 1\n",
        "    if not dist.is_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not dist.is_available():\n",
        "        return 0\n",
        "    if not dist.is_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def synchronize():\n",
        "    \"\"\"\n",
        "    Helper function to synchronize (barrier) among all processes when\n",
        "    using distributed training\n",
        "    \"\"\"\n",
        "    if not dist.is_available():\n",
        "        return\n",
        "    if not dist.is_initialized():\n",
        "        return\n",
        "    world_size = dist.get_world_size()\n",
        "    if world_size == 1:\n",
        "        return\n",
        "    dist.barrier()\n",
        "\n",
        "\n",
        "def ensure_init_process_group(local_rank=None, port=12345):\n",
        "    # init with env\n",
        "    world_size = int(os.environ['WORLD_SIZE']) if 'WORLD_SIZE' in os.environ else 1\n",
        "    if world_size > 1 and not dist.is_initialized():\n",
        "        assert local_rank is not None\n",
        "        print(\"Init distributed training on local rank {}\".format(local_rank))\n",
        "        torch.cuda.set_device(local_rank)\n",
        "        dist.init_process_group(\n",
        "            backend='nccl', init_method='env://'\n",
        "        )\n",
        "    return local_rank\n",
        "\n",
        "class Arguments(object):\n",
        "  pass\n",
        "\n",
        "def main():\n",
        "    args = Arguments()\n",
        "    args.data_dir = 'datasets/coco_caption' # The input data dir with all required files.\n",
        "    args.train_yaml = 'train.yaml' # yaml file for training.\n",
        "    args.test_yaml = 'test.yaml' # yaml file for testing.\n",
        "    args.val_yaml = 'val.yaml' # yaml file used for validation during training.\n",
        "    args.model_name_or_path = None # Path to pre-trained model or model type.\n",
        "    args.output_dir = 'output/'# The output directory to save checkpoint and test results.\n",
        "    args.loss_type = 'sfmx'# Loss function types: support kl, x2, sfmx\n",
        "    args.config_name = \"\", # Pretrained config name or path if not the same as model_name.\n",
        "    args.tokenizer_name = \"\" # Pretrained tokenizer name or path if not the same as model_name.\n",
        "    args.max_seq_length = 70 # The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
        "    args.max_seq_a_length = 40 #The maximum sequence length for caption.\n",
        "    args.do_train = False # Whether to run training.\n",
        "    args.do_test = False # Whether to run inference.\n",
        "    args.do_eval = False # Whether to run evaluation\n",
        "    args.do_lower_case = False # Set this flag if you are using an uncased model.\n",
        "    args.mask_prob = 0.15 # Probability to mask input sentence during training\n",
        "    args.max_masked_tokens = 3 # The max number of masked tokens per sentence.\n",
        "    args.add_od_labels = False # Whether to add object detection labels or not\n",
        "    args.drop_out = 0.1 # Drop out in BERT\n",
        "\n",
        "    args.max_img_seq_length = 50 # The maximum total input image sequence length.\")\n",
        "    args.img_feature_dim = 2054 # The Image Feature Dimension.\")\n",
        "    args.img_feature_type= 'frcnn' #Image feature type.\")\n",
        "    args.tie_weights = False # Whether to tie decoding weights to that of encoding\")\n",
        "    args.freeze_embedding = False # Whether to freeze word embeddings in Bert\")\n",
        "    args.label_smoothing = 0\n",
        "    args.drop_worst_ratio = 0\n",
        "    args.drop_worst_after = 0\n",
        "    args.per_gpu_train_batch_size = 64 # Batch size per GPU/CPU for training.\")\n",
        "    args.per_gpu_eval_batch_size = 64 # Batch size per GPU/CPU for evaluation.\")\n",
        "    args.output_mode = 'classification' # output mode, support classification or regression.\")\n",
        "    args.num_labels = 2 # num_labels is 2 for classification and 1 for regression.\")\n",
        "    args.gradient_accumulation_steps = 1 # Number of updates steps to accumulate before backward.\")\n",
        "    args.learning_rate = 3e-5 # The initial lr.\")\n",
        "    args.weight_decay = 0.05 # Weight deay.\")\n",
        "    args.adam_epsilon = 1e-8 # Epsilon for Adam.\")\n",
        "    args.max_grad_norm = 1.0 # Max gradient norm.\")\n",
        "    args.warmup_steps = 0 # Linear warmup.\")\n",
        "    args.scheduler ='linear' # constant or linear or\")\n",
        "    args.num_workers = 4 # Workers in dataloader.\")\n",
        "    args.num_train_epochs = 40 # Total number of training epochs to perform.\")\n",
        "\n",
        "    args.max_steps = -1 # \"Total number of training steps. Override num_train_epochs.\")\n",
        "    args.logging_steps = 20 # \"Log every X steps.\")\n",
        "\n",
        "    args.save_steps = -1 # \"Save checkpoint every X steps. Will also perform evaluatin.\")\n",
        "    args.evaluate_during_training = False # Run evaluation during training at each save_steps.\")\n",
        "    args.no_cuda = False # Avoid using CUDA.\")\n",
        "    args.local_rank = 0 # For distributed training.\")\n",
        "    args.seed = 88 # random seed for initialization.\")\n",
        "    # for self-critical sequence training\n",
        "    args.scst = False # Self-critical sequence training')\n",
        "    args.sc_train_sample_n = 5 # \"number of sampled captions for sc training\")\n",
        "    args.sc_baseline_type = 'greedy' # \"baseline tyep of REINFORCE algorithm\")\n",
        "    args.sc_beam_size = 1 # beam size for scst training\")\n",
        "    args.cider_cached_tokens = 'coco-train-words.p' #path to cached cPickle file used to calculate CIDEr scores\")\n",
        "    # for generation\n",
        "    args.eval_model_dir = '' # \"Model directory for evaluation.\")\n",
        "    args.max_gen_length = 20 # \"max length of generated sentences\")\n",
        "    args.output_hidden_states = False # \"Turn on for fast decoding\")\n",
        "    args.num_return_sequences = 1 # repeating times per image\")\n",
        "    args.num_beams = 1 # beam search width\")\n",
        "    args.num_keep_best = 1 # number of hypotheses to keep in beam search\")\n",
        "    args.temperature = 1 # \"temperature in softmax for sampling\")\n",
        "    args.top_k = 0 # \"filter distribution for sampling\")\n",
        "    args.top_p = 1 # filter distribution for sampling\")\n",
        "    args.repetition_penalty = 1 # \"repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)\")\n",
        "    args.length_penalty = 1 # beam search length penalty\")\n",
        "    # for Constrained Beam Search\n",
        "    args.use_cbs = False # Use constrained beam search for decoding')\n",
        "    args.min_constraints_to_satisfy = 2 # minimum number of constraints to satisfy\")\n",
        "\n",
        "    args.overfit = False\n",
        "\n",
        "\n",
        "    global logger\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    local_rank = ensure_init_process_group(local_rank=args.local_rank)\n",
        "    args.local_rank = local_rank\n",
        "    args.num_gpus = get_world_size()\n",
        "    args.distributed = args.num_gpus > 1\n",
        "    args.device = torch.device('cuda')\n",
        "    args.data_dir = \"/content/drive/MyDrive/OscarPlus/datasets/coco_caption\"\n",
        "\n",
        "    # Setup custom arguments at here, this is for python notebook compatability\n",
        "    args.do_test = True\n",
        "    args.do_eval = True\n",
        "    args.test_yaml = \"test.yaml\"\n",
        "    args.num_beams = 5\n",
        "    args.max_gen_length = 20\n",
        "    args.eval_model_dir = \"/content/drive/MyDrive/OscarPlus/output/checkpoint3/checkpoint-9-80000\"\n",
        "    args.add_od_labels = True\n",
        "\n",
        "    # use overfit dataset\n",
        "    # args.overfit = True\n",
        "\n",
        "    # fine-tune setup arguments\n",
        "\n",
        "    # args.model_name_or_path = \"/content/drive/MyDrive/OscarPlus/weights/checkpoint0\"\n",
        "    # args.do_train = True\n",
        "    # args.do_lower_case = True\n",
        "    # # bug\n",
        "    # args.evaluate_during_training = False\n",
        "    # args.num_train_epochs = 30\n",
        "    # args.save_steps = 2000\n",
        "    # args.output_dir = \"/content/drive/MyDrive/OscarPlus/output/checkpoint3\"\n",
        "    # args.data_dir = \"/content/drive/MyDrive/OscarPlus/datasets/coco_caption\"\n",
        "    # args.train_yaml = 'train.yaml' \n",
        "    # args.test_yaml = 'test.yaml' \n",
        "    # args.val_yaml = 'val.yaml'\n",
        "\n",
        "    print(type(args.model_name_or_path))\n",
        "\n",
        "    synchronize()\n",
        "\n",
        "    output_dir = args.output_dir\n",
        "    mkdir(output_dir)\n",
        "\n",
        "    logger = setup_logger(\"vlpretrain\", output_dir, args.local_rank)\n",
        "    logger.warning(\"Device: %s, n_gpu: %s\", args.device, args.num_gpus)\n",
        "    set_seed(args.seed, args.num_gpus)\n",
        "    args = restore_training_settings(args)\n",
        "\n",
        "    # ying!\n",
        "    args.add_od_labels = True\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config_class, model_class, tokenizer_class = BertConfig, BertForImageCaptioning, BertTokenizer\n",
        "    if args.do_train:\n",
        "        assert args.model_name_or_path is not None\n",
        "        config = config_class.from_pretrained(args.model_name_or_path, num_labels=args.num_labels, finetuning_task='image_captioning')\n",
        "        if args.scst:\n",
        "            # avoid using too much memory\n",
        "            config.output_hidden_states = True\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name \\\n",
        "                else args.model_name_or_path, do_lower_case=args.do_lower_case)\n",
        "        config.img_feature_dim = args.img_feature_dim\n",
        "        config.img_feature_type = args.img_feature_type\n",
        "        config.hidden_dropout_prob = args.drop_out\n",
        "        config.loss_type = args.loss_type\n",
        "        config.tie_weights = args.tie_weights\n",
        "        config.freeze_embedding = args.freeze_embedding\n",
        "        config.label_smoothing = args.label_smoothing\n",
        "        config.drop_worst_ratio = args.drop_worst_ratio\n",
        "        config.drop_worst_after = args.drop_worst_after\n",
        "        model = model_class.from_pretrained(args.model_name_or_path,\n",
        "                from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n",
        "    else:\n",
        "        checkpoint = args.eval_model_dir\n",
        "        assert op.isdir(checkpoint)\n",
        "        config = config_class.from_pretrained(checkpoint)\n",
        "        config.output_hidden_states = args.output_hidden_states\n",
        "        tokenizer = tokenizer_class.from_pretrained(checkpoint)\n",
        "        logger.info(\"Evaluate the following checkpoint: %s\", checkpoint)\n",
        "        model = model_class.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "    if args.no_cuda:\n",
        "        pass\n",
        "    else:\n",
        "        model.to(args.device)\n",
        "        \n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "    if args.do_train:\n",
        "        train_dataloader = make_data_loader(args, args.train_yaml, tokenizer,\n",
        "            args.distributed, is_train=True)\n",
        "        val_dataloader = None\n",
        "        if args.evaluate_during_training:\n",
        "            val_dataloader = make_data_loader(args, args.val_yaml, tokenizer,\n",
        "                args.distributed, is_train=False)\n",
        "        last_checkpoint = train(args, train_dataloader, val_dataloader, model, tokenizer)\n",
        "\n",
        "        # test the last checkpoint after training\n",
        "        if args.do_test:\n",
        "            logger.info(\"Evaluate on dataset: \" + args.test_yaml)\n",
        "            test_dataloader = make_data_loader(args, args.test_yaml, \n",
        "                tokenizer, args.distributed, is_train=False)\n",
        "            evaluate(args, test_dataloader, model, tokenizer, last_checkpoint)\n",
        "\n",
        "    # inference and evaluation\n",
        "    elif args.do_test or args.do_eval:\n",
        "        logger.info(\"Evaluate on dataset: \" + args.test_yaml)\n",
        "        test_dataloader = make_data_loader(args, args.test_yaml,\n",
        "            tokenizer, args.distributed, is_train=False)\n",
        "\n",
        "        if not args.do_eval:\n",
        "            predict_file = get_predict_file(checkpoint, test_dataloader.dataset.yaml_file, args)\n",
        "            test(args, test_dataloader, model, tokenizer, predict_file)\n",
        "            logger.info(\"Prediction results saved to: {}\".format(predict_file))\n",
        "        else:\n",
        "            try:\n",
        "              evaluate_file = evaluate(args, test_dataloader, model, tokenizer,\n",
        "                    checkpoint)\n",
        "            except BrokenPipeError:\n",
        "              pass\n",
        "            logger.info(\"Evaluation results saved to: {}\".format(evaluate_file))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOYff8a1JcKR",
        "outputId": "802969b9-bb86-442d-cc04-494ec011bab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 11 05:43:13 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkT3U3wxSUHl",
        "outputId": "994297d4-065a-45a1-a55f-24fb2d97a828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added_tokens.json\n",
            "config.json\n",
            "pred.coco_caption.test.beam5.max20.odlabels_coco_format.json\n",
            "pred.coco_caption.test.beam5.max20.odlabels.eval.json\n",
            "pred.coco_caption.test.beam5.max20.odlabels.tsv\n",
            "pytorch_model.bin\n",
            "special_tokens_map.json\n",
            "training_args.bin\n",
            "vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/OscarPlus/weights/checkpoint0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ank1s00lkjd_",
        "outputId": "a495869a-81ae-4ab9-9944-621b68a6a8e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                       Version\n",
            "----------------------------- --------------\n",
            "absl-py                       0.12.0\n",
            "alabaster                     0.7.12\n",
            "albumentations                0.1.12\n",
            "altair                        4.1.0\n",
            "anytree                       2.8.0\n",
            "appdirs                       1.4.4\n",
            "argcomplete                   1.12.3\n",
            "argon2-cffi                   21.1.0\n",
            "arviz                         0.11.4\n",
            "astor                         0.8.1\n",
            "astropy                       4.3.1\n",
            "astunparse                    1.6.3\n",
            "atari-py                      0.2.9\n",
            "atomicwrites                  1.4.0\n",
            "attrs                         21.2.0\n",
            "audioread                     2.1.9\n",
            "autograd                      1.3\n",
            "Babel                         2.9.1\n",
            "backcall                      0.2.0\n",
            "beautifulsoup4                4.6.3\n",
            "bleach                        4.1.0\n",
            "blis                          0.4.1\n",
            "bokeh                         2.3.3\n",
            "boto3                         1.20.21\n",
            "botocore                      1.23.21\n",
            "Bottleneck                    1.3.2\n",
            "branca                        0.4.2\n",
            "bs4                           0.0.1\n",
            "CacheControl                  0.12.10\n",
            "cached-property               1.5.2\n",
            "cachetools                    4.2.4\n",
            "catalogue                     1.0.0\n",
            "certifi                       2021.10.8\n",
            "cffi                          1.15.0\n",
            "cftime                        1.5.1.1\n",
            "chardet                       3.0.4\n",
            "charset-normalizer            2.0.8\n",
            "click                         7.1.2\n",
            "cloudpickle                   1.3.0\n",
            "cmake                         3.12.0\n",
            "cmdstanpy                     0.9.5\n",
            "colorcet                      2.0.6\n",
            "colorlover                    0.3.0\n",
            "community                     1.0.0b1\n",
            "contextlib2                   0.5.5\n",
            "convertdate                   2.3.2\n",
            "coverage                      3.7.1\n",
            "coveralls                     0.5\n",
            "crcmod                        1.7\n",
            "cufflinks                     0.17.3\n",
            "cupy-cuda111                  9.4.0\n",
            "cvxopt                        1.2.7\n",
            "cvxpy                         1.0.31\n",
            "cycler                        0.11.0\n",
            "cymem                         2.0.6\n",
            "Cython                        0.29.24\n",
            "daft                          0.0.4\n",
            "dask                          2.12.0\n",
            "datascience                   0.10.6\n",
            "debugpy                       1.0.0\n",
            "decorator                     4.4.2\n",
            "defusedxml                    0.7.1\n",
            "descartes                     1.1.0\n",
            "dill                          0.3.4\n",
            "distributed                   1.25.3\n",
            "dlib                          19.18.0\n",
            "dm-tree                       0.1.6\n",
            "docopt                        0.6.2\n",
            "docutils                      0.17.1\n",
            "dopamine-rl                   1.0.5\n",
            "earthengine-api               0.1.290\n",
            "easydict                      1.9\n",
            "ecos                          2.0.7.post1\n",
            "editdistance                  0.5.3\n",
            "en-core-web-sm                2.2.5\n",
            "entrypoints                   0.3\n",
            "ephem                         4.1\n",
            "et-xmlfile                    1.1.0\n",
            "fa2                           0.3.5\n",
            "fastai                        1.0.61\n",
            "fastdtw                       0.3.4\n",
            "fastprogress                  1.0.0\n",
            "fastrlock                     0.8\n",
            "fbprophet                     0.7.1\n",
            "feather-format                0.4.1\n",
            "filelock                      3.4.0\n",
            "firebase-admin                4.4.0\n",
            "fix-yahoo-finance             0.0.22\n",
            "Flask                         1.1.4\n",
            "flatbuffers                   2.0\n",
            "folium                        0.8.3\n",
            "future                        0.16.0\n",
            "gast                          0.4.0\n",
            "GDAL                          2.2.2\n",
            "gdown                         3.6.4\n",
            "gensim                        3.6.0\n",
            "geographiclib                 1.52\n",
            "geopy                         1.17.0\n",
            "gin-config                    0.5.0\n",
            "glob2                         0.7\n",
            "google                        2.0.3\n",
            "google-api-core               1.26.3\n",
            "google-api-python-client      1.12.8\n",
            "google-auth                   1.35.0\n",
            "google-auth-httplib2          0.0.4\n",
            "google-auth-oauthlib          0.4.6\n",
            "google-cloud-bigquery         1.21.0\n",
            "google-cloud-bigquery-storage 1.1.0\n",
            "google-cloud-core             1.0.3\n",
            "google-cloud-datastore        1.8.0\n",
            "google-cloud-firestore        1.7.0\n",
            "google-cloud-language         1.2.0\n",
            "google-cloud-storage          1.18.1\n",
            "google-cloud-translate        1.5.0\n",
            "google-colab                  1.0.0\n",
            "google-pasta                  0.2.0\n",
            "google-resumable-media        0.4.1\n",
            "googleapis-common-protos      1.53.0\n",
            "googledrivedownloader         0.4\n",
            "graphviz                      0.10.1\n",
            "greenlet                      1.1.2\n",
            "grpcio                        1.42.0\n",
            "gspread                       3.0.1\n",
            "gspread-dataframe             3.0.8\n",
            "gym                           0.17.3\n",
            "h5py                          3.1.0\n",
            "HeapDict                      1.0.1\n",
            "hijri-converter               2.2.2\n",
            "holidays                      0.10.5.2\n",
            "holoviews                     1.14.6\n",
            "html5lib                      1.0.1\n",
            "httpimport                    0.5.18\n",
            "httplib2                      0.17.4\n",
            "httplib2shim                  0.0.3\n",
            "humanize                      0.5.1\n",
            "hyperopt                      0.1.2\n",
            "ideep4py                      2.0.0.post3\n",
            "idna                          2.10\n",
            "imageio                       2.4.1\n",
            "imagesize                     1.3.0\n",
            "imbalanced-learn              0.8.1\n",
            "imblearn                      0.0\n",
            "imgaug                        0.2.9\n",
            "importlib-metadata            4.8.2\n",
            "importlib-resources           5.4.0\n",
            "imutils                       0.5.4\n",
            "inflect                       2.1.0\n",
            "iniconfig                     1.1.1\n",
            "intel-openmp                  2021.4.0\n",
            "intervaltree                  2.1.0\n",
            "ipykernel                     4.10.1\n",
            "ipython                       5.5.0\n",
            "ipython-genutils              0.2.0\n",
            "ipython-sql                   0.3.9\n",
            "ipywidgets                    7.6.5\n",
            "itsdangerous                  1.1.0\n",
            "jax                           0.2.25\n",
            "jaxlib                        0.1.71+cuda111\n",
            "jdcal                         1.4.1\n",
            "jedi                          0.18.1\n",
            "jieba                         0.42.1\n",
            "Jinja2                        2.11.3\n",
            "jmespath                      0.10.0\n",
            "joblib                        1.1.0\n",
            "jpeg4py                       0.1.4\n",
            "jsonschema                    2.6.0\n",
            "jupyter                       1.0.0\n",
            "jupyter-client                5.3.5\n",
            "jupyter-console               5.2.0\n",
            "jupyter-core                  4.9.1\n",
            "jupyterlab-pygments           0.1.2\n",
            "jupyterlab-widgets            1.0.2\n",
            "kaggle                        1.5.12\n",
            "kapre                         0.3.6\n",
            "keras                         2.7.0\n",
            "Keras-Preprocessing           1.1.2\n",
            "keras-vis                     0.4.1\n",
            "kiwisolver                    1.3.2\n",
            "korean-lunar-calendar         0.2.1\n",
            "libclang                      12.0.0\n",
            "librosa                       0.8.1\n",
            "lightgbm                      2.2.3\n",
            "llvmlite                      0.34.0\n",
            "lmdb                          0.99\n",
            "LunarCalendar                 0.0.9\n",
            "lxml                          4.2.6\n",
            "Markdown                      3.3.6\n",
            "MarkupSafe                    2.0.1\n",
            "matplotlib                    3.2.2\n",
            "matplotlib-inline             0.1.3\n",
            "matplotlib-venn               0.11.6\n",
            "missingno                     0.5.0\n",
            "mistune                       0.8.4\n",
            "mizani                        0.6.0\n",
            "mkl                           2019.0\n",
            "mlxtend                       0.14.0\n",
            "more-itertools                8.12.0\n",
            "moviepy                       0.2.3.5\n",
            "mpmath                        1.2.1\n",
            "msgpack                       1.0.3\n",
            "multiprocess                  0.70.12.2\n",
            "multitasking                  0.0.10\n",
            "murmurhash                    1.0.6\n",
            "music21                       5.5.0\n",
            "natsort                       5.5.0\n",
            "nbclient                      0.5.9\n",
            "nbconvert                     5.6.1\n",
            "nbformat                      5.1.3\n",
            "nest-asyncio                  1.5.4\n",
            "netCDF4                       1.5.8\n",
            "networkx                      2.6.3\n",
            "nibabel                       3.0.2\n",
            "nltk                          3.2.5\n",
            "notebook                      5.3.1\n",
            "numba                         0.51.2\n",
            "numexpr                       2.7.3\n",
            "numpy                         1.19.5\n",
            "nvidia-ml-py3                 7.352.0\n",
            "oauth2client                  4.1.3\n",
            "oauthlib                      3.1.1\n",
            "okgrade                       0.4.3\n",
            "opencv-contrib-python         4.1.2.30\n",
            "opencv-python                 4.1.2.30\n",
            "openpyxl                      2.5.9\n",
            "opt-einsum                    3.3.0\n",
            "osqp                          0.6.2.post0\n",
            "packaging                     21.3\n",
            "palettable                    3.3.0\n",
            "pandas                        1.1.5\n",
            "pandas-datareader             0.9.0\n",
            "pandas-gbq                    0.13.3\n",
            "pandas-profiling              1.4.1\n",
            "pandocfilters                 1.5.0\n",
            "panel                         0.12.1\n",
            "param                         1.12.0\n",
            "parso                         0.8.3\n",
            "pathlib                       1.0.1\n",
            "patsy                         0.5.2\n",
            "pep517                        0.12.0\n",
            "pexpect                       4.8.0\n",
            "pickleshare                   0.7.5\n",
            "Pillow                        7.1.2\n",
            "pip                           21.1.3\n",
            "pip-tools                     6.2.0\n",
            "plac                          1.1.3\n",
            "plotly                        4.4.1\n",
            "plotnine                      0.6.0\n",
            "pluggy                        0.7.1\n",
            "pooch                         1.5.2\n",
            "portpicker                    1.3.9\n",
            "prefetch-generator            1.0.1\n",
            "preshed                       3.0.6\n",
            "prettytable                   2.4.0\n",
            "progressbar2                  3.38.0\n",
            "prometheus-client             0.12.0\n",
            "promise                       2.3\n",
            "prompt-toolkit                1.0.18\n",
            "protobuf                      3.17.3\n",
            "psutil                        5.4.8\n",
            "psycopg2                      2.7.6.1\n",
            "ptyprocess                    0.7.0\n",
            "py                            1.11.0\n",
            "pyarrow                       3.0.0\n",
            "pyasn1                        0.4.8\n",
            "pyasn1-modules                0.2.8\n",
            "pycocotools                   2.0.3\n",
            "pycparser                     2.21\n",
            "pyct                          0.4.8\n",
            "pydata-google-auth            1.2.0\n",
            "pydot                         1.3.0\n",
            "pydot-ng                      2.0.0\n",
            "pydotplus                     2.0.2\n",
            "PyDrive                       1.3.1\n",
            "pyemd                         0.5.1\n",
            "pyerfa                        2.0.0.1\n",
            "pyglet                        1.5.0\n",
            "Pygments                      2.6.1\n",
            "pygobject                     3.26.1\n",
            "pymc3                         3.11.4\n",
            "PyMeeus                       0.5.11\n",
            "pymongo                       3.12.1\n",
            "pymystem3                     0.2.0\n",
            "PyOpenGL                      3.1.5\n",
            "pyparsing                     3.0.6\n",
            "pyrsistent                    0.18.0\n",
            "pysndfile                     1.3.8\n",
            "PySocks                       1.7.1\n",
            "pystan                        2.19.1.1\n",
            "pytest                        3.6.4\n",
            "python-apt                    0.0.0\n",
            "python-chess                  0.23.11\n",
            "python-dateutil               2.8.2\n",
            "python-louvain                0.15\n",
            "python-slugify                5.0.2\n",
            "python-utils                  2.5.6\n",
            "pytz                          2018.9\n",
            "pyviz-comms                   2.1.0\n",
            "PyWavelets                    1.2.0\n",
            "PyYAML                        3.13\n",
            "pyzmq                         22.3.0\n",
            "qdldl                         0.1.5.post0\n",
            "qtconsole                     5.2.1\n",
            "QtPy                          1.11.2\n",
            "regex                         2019.12.20\n",
            "requests                      2.23.0\n",
            "requests-oauthlib             1.3.0\n",
            "resampy                       0.2.2\n",
            "retrying                      1.3.3\n",
            "rpy2                          3.4.5\n",
            "rsa                           4.8\n",
            "s3transfer                    0.5.0\n",
            "scikit-image                  0.18.3\n",
            "scikit-learn                  1.0.1\n",
            "scipy                         1.4.1\n",
            "screen-resolution-extra       0.0.0\n",
            "scs                           2.1.4\n",
            "seaborn                       0.11.2\n",
            "semver                        2.13.0\n",
            "Send2Trash                    1.8.0\n",
            "setuptools                    57.4.0\n",
            "setuptools-git                1.2\n",
            "Shapely                       1.8.0\n",
            "simplegeneric                 0.8.1\n",
            "six                           1.15.0\n",
            "sklearn                       0.0\n",
            "sklearn-pandas                1.8.0\n",
            "smart-open                    5.2.1\n",
            "snowballstemmer               2.2.0\n",
            "sortedcontainers              2.4.0\n",
            "SoundFile                     0.10.3.post1\n",
            "spacy                         2.2.4\n",
            "Sphinx                        1.8.6\n",
            "sphinxcontrib-serializinghtml 1.1.5\n",
            "sphinxcontrib-websupport      1.2.4\n",
            "SQLAlchemy                    1.4.27\n",
            "sqlparse                      0.4.2\n",
            "srsly                         1.0.5\n",
            "statsmodels                   0.10.2\n",
            "sympy                         1.7.1\n",
            "tables                        3.4.4\n",
            "tabulate                      0.8.9\n",
            "tblib                         1.7.0\n",
            "tensorboard                   2.7.0\n",
            "tensorboard-data-server       0.6.1\n",
            "tensorboard-plugin-wit        1.8.0\n",
            "tensorflow                    2.7.0\n",
            "tensorflow-datasets           4.0.1\n",
            "tensorflow-estimator          2.7.0\n",
            "tensorflow-gcs-config         2.7.0\n",
            "tensorflow-hub                0.12.0\n",
            "tensorflow-io-gcs-filesystem  0.22.0\n",
            "tensorflow-metadata           1.4.0\n",
            "tensorflow-probability        0.15.0\n",
            "termcolor                     1.1.0\n",
            "terminado                     0.12.1\n",
            "testpath                      0.5.0\n",
            "text-unidecode                1.3\n",
            "textblob                      0.15.3\n",
            "Theano-PyMC                   1.1.2\n",
            "thinc                         7.4.0\n",
            "threadpoolctl                 3.0.0\n",
            "tifffile                      2021.11.2\n",
            "toml                          0.10.2\n",
            "tomli                         1.2.2\n",
            "toolz                         0.11.2\n",
            "torch                         1.10.0+cu111\n",
            "torchaudio                    0.10.0+cu111\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.11.0\n",
            "torchvision                   0.11.1+cu111\n",
            "tornado                       5.1.1\n",
            "tqdm                          4.62.3\n",
            "traitlets                     5.1.1\n",
            "tweepy                        3.10.0\n",
            "typeguard                     2.7.1\n",
            "typing-extensions             3.10.0.2\n",
            "tzlocal                       1.5.1\n",
            "uritemplate                   3.0.1\n",
            "urllib3                       1.25.11\n",
            "vega-datasets                 0.9.0\n",
            "wasabi                        0.8.2\n",
            "wcwidth                       0.2.5\n",
            "webencodings                  0.5.1\n",
            "Werkzeug                      1.0.1\n",
            "wheel                         0.37.0\n",
            "widgetsnbextension            3.5.2\n",
            "wordcloud                     1.5.0\n",
            "wrapt                         1.13.3\n",
            "xarray                        0.18.2\n",
            "xgboost                       0.90\n",
            "xkit                          0.0.0\n",
            "xlrd                          1.1.0\n",
            "xlwt                          1.3.0\n",
            "yellowbrick                   1.3.post1\n",
            "zict                          2.0.0\n",
            "zipp                          3.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PnL0sYeOMZ0"
      },
      "source": [
        "# Pretrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrGCLt7wOP32",
        "outputId": "7b16a0d6-d4ea-4af0-d9e8-2b86daffc128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/10/2021 08:16:56 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n",
            "12/10/2021 08:16:56 - INFO - __main__ -  -> Recovering model from /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000300\n",
            "12/10/2021 08:16:56 - INFO - transformers.pytorch_transformers.modeling_utils - loading configuration file /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000300/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last checkpoint dir:  /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/10/2021 08:16:57 - INFO - transformers.pytorch_transformers.modeling_utils - Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"img_feature_dim\": 2054,\n",
            "  \"img_feature_type\": \"faster_r-cnn\",\n",
            "  \"img_layer_norm_eps\": 1e-12,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_bert\": true,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_contrast_classes\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_img_layernorm\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/10/2021 08:16:57 - INFO - root - loading weights file /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000300/pytorch_model.bin\n",
            "12/10/2021 08:16:59 - INFO - oscar.modeling.modeling_bert - BertImgModel Image Dimension: 2054\n",
            "12/10/2021 08:17:06 - INFO - __main__ - Total Parameters: 111686973\n",
            "12/10/2021 08:17:15 - INFO - __main__ - Training/evaluation parameters <__main__.Arguments object at 0x7fce620b23d0>\n",
            "12/10/2021 08:17:16 - INFO - __main__ - Load BERT optimizer from /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000300\n",
            "12/10/2021 08:17:29 - INFO - oscar.datasets.build - Train with 64 images per GPU\n",
            "12/10/2021 08:17:30 - INFO - transformers.pytorch_transformers.tokenization_utils - Model name '/content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000300' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000300' is a path or url to a directory containing tokenizer files.\n",
            "12/10/2021 08:17:30 - INFO - transformers.pytorch_transformers.tokenization_utils - loading file /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000300/added_tokens.json\n",
            "12/10/2021 08:17:30 - INFO - transformers.pytorch_transformers.tokenization_utils - loading file /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000300/special_tokens_map.json\n",
            "12/10/2021 08:17:30 - INFO - transformers.pytorch_transformers.tokenization_utils - loading file /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2000300/vocab.txt\n",
            "12/10/2021 08:17:31 - INFO - root - Datasets: coco\n",
            "12/10/2021 08:17:31 - INFO - root - Open image label file /content/drive/MyDrive/OscarPlus/datasets/pretrain/labels/coco/predictions_gt.tsv, time: 0.17134642601013184\n",
            "12/10/2021 08:17:33 - INFO - root - Load img label offset map: /content/drive/MyDrive/OscarPlus/datasets/pretrain/labels/coco/imageid2idx.json, time: 1.8507113456726074\n",
            "12/10/2021 08:17:33 - INFO - root - * Loading dataset coco\n",
            "12/10/2021 08:17:33 - INFO - root - Open dataset /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.tsv, time: 0.2689971923828125\n",
            "12/10/2021 08:17:33 - INFO - root - Info: loading img features using 2.2986743450164795 secs\n",
            "12/10/2021 08:17:33 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/coco.lineidx\n",
            "  0%|          | 0/3691045 [00:00<?, ?it/s]12/10/2021 08:17:36 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/labels/coco/predictions_gt.lineidx\n",
            " 15%|█▌        | 558863/3691045 [01:52<08:11, 6371.23it/s]12/10/2021 08:19:28 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/labels/coco/QA_fileB.lineidx\n",
            "100%|██████████| 3691045/3691045 [05:06<00:00, 12045.66it/s] \n",
            "12/10/2021 08:22:42 - INFO - root - Max_tokens: 184\n",
            "12/10/2021 08:22:42 - INFO - root - Total docs - Corpus_lines: 1588501-3177002\n",
            "12/10/2021 08:22:42 - INFO - root - Total QA docs - Corpus_lines: 1029500\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "12/10/2021 08:22:42 - INFO - __main__ - ***** Running training *****\n",
            "12/10/2021 08:22:42 - INFO - __main__ -  Num examples = 1588501\n",
            "12/10/2021 08:22:42 - INFO - __main__ -   Instantaneous batch size = 64\n",
            "12/10/2021 08:22:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "12/10/2021 08:22:42 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "12/10/2021 08:22:42 - INFO - __main__ -   Total optimization steps = 2050000\n",
            "12/10/2021 08:22:42 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "12/10/2021 08:22:42 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "12/10/2021 08:22:43 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "12/10/2021 08:22:43 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "12/10/2021 08:22:43 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "12/10/2021 08:22:43 - INFO - root - loading lineidx: /content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.lineidx\n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "12/10/2021 08:22:45 - INFO - root - *** Example ***\n",
            "12/10/2021 08:22:45 - INFO - root - *** Example ***\n",
            "12/10/2021 08:22:45 - INFO - root - guid: 0\n",
            "12/10/2021 08:22:45 - INFO - root - *** Example ***\n",
            "12/10/2021 08:22:45 - INFO - root - guid: 0\n",
            "12/10/2021 08:22:45 - INFO - root - guid: 0\n",
            "12/10/2021 08:22:45 - INFO - root - tokens: [CLS] [MASK] man [MASK] a paddle board on top of [MASK] wave . [SEP] brown [SEP]\n",
            "12/10/2021 08:22:45 - INFO - root - tokens: [CLS] some yellow plants in a [MASK] green bucket [SEP] pot ##ted plant vase [MASK] furniture [MASK] table concrete [MASK] [SEP]\n",
            "12/10/2021 08:22:45 - INFO - root - tokens: [CLS] what is the zebra sniffing ? [SEP] ground [SEP]\n",
            "12/10/2021 08:22:45 - INFO - root - input_ids: 101 103 2158 103 1037 20890 2604 2006 2327 1997 103 4400 1012 102 2829 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - input_ids: 101 2070 3756 4264 1999 1037 103 2665 13610 102 8962 3064 3269 18781 103 7390 103 2795 5509 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - input_ids: 101 2054 2003 1996 29145 27646 1029 102 2598 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - LM label: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "12/10/2021 08:22:45 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - Is next sentence label: 0 \n",
            "12/10/2021 08:22:45 - INFO - root - LM label: [-1, -1, -1, -1, -1, -1, 2235, -1, -1, -1, -1, -1, -1, -1, 6847, -1, 2422, -1, -1, 2813, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "12/10/2021 08:22:45 - INFO - root - LM label: [-1, 1037, -1, 5559, -1, -1, -1, -1, -1, -1, 1037, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "12/10/2021 08:22:45 - INFO - root - Is next sentence label: 0 \n",
            "12/10/2021 08:22:45 - INFO - root - Is next sentence label: 1 \n",
            "12/10/2021 08:22:45 - INFO - root - *** Example ***\n",
            "12/10/2021 08:22:45 - INFO - root - guid: 0\n",
            "12/10/2021 08:22:45 - INFO - root - tokens: [CLS] what is the enables holding ? [SEP] person surf ##board clothes sea [SEP]\n",
            "12/10/2021 08:22:45 - INFO - root - input_ids: 101 2054 2003 1996 12939 3173 1029 102 2711 14175 6277 4253 2712 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - LM label: [-1, -1, -1, -1, 2158, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "12/10/2021 08:22:45 - INFO - root - Is next sentence label: 2 \n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "/content/drive/MyDrive/OscarPlus/oscar/datasets/oscar_tsv.py:572: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  feat = torch.from_numpy(feat)\n",
            "12/10/2021 08:22:45 - INFO - root - *** Example ***\n",
            "12/10/2021 08:22:45 - INFO - root - *** Example ***\n",
            "12/10/2021 08:22:45 - INFO - root - guid: 0\n",
            "12/10/2021 08:22:45 - INFO - root - guid: 0\n",
            "12/10/2021 08:22:45 - INFO - root - tokens: [CLS] what type of vehicle is [MASK] ? [SEP] [MASK] cart [SEP]\n",
            "12/10/2021 08:22:45 - INFO - root - input_ids: 101 2054 2828 1997 4316 2003 103 1029 102 103 11122 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - tokens: [CLS] two small bears stand next to each other . [SEP] [MASK] bear floor [SEP]\n",
            "12/10/2021 08:22:45 - INFO - root - input_ids: 101 2048 2235 6468 3233 2279 2000 2169 2060 1012 102 103 4562 2723 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/10/2021 08:22:45 - INFO - root - LM label: [-1, -1, -1, -1, -1, -1, 2023, -1, -1, 4744, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "12/10/2021 08:22:45 - INFO - root - LM label: [-1, -1, -1, -1, 3233, -1, -1, -1, -1, -1, -1, 4562, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "12/10/2021 08:22:45 - INFO - root - Is next sentence label: 0 \n",
            "12/10/2021 08:22:45 - INFO - root - Is next sentence label: 0 \n",
            "/content/drive/MyDrive/OscarPlus/transformers/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "12/10/2021 08:40:15 - INFO - __main__ - eta: 2 days, 8:13:57  iter: 2000500  max mem: 10787\n",
            "    batch_metrics - loss: 1.6983 (1.5518)\n",
            "    time_info     - compute: 0.7842 (4.0896)  data: 0.0019 (3.3118)  compute1: 0.7095 (0.7163)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 08:48:20 - INFO - __main__ - eta: 1 day, 1:19:13  iter: 2001000  max mem: 10787\n",
            "    batch_metrics - loss: 1.7159 (1.5139)\n",
            "    time_info     - compute: 0.7697 (1.8603)  data: 0.0022 (1.0558)  compute1: 0.7138 (0.7204)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 08:48:20 - INFO - __main__ - PROGRESS: 97.6098%\n",
            "12/10/2021 08:48:20 - INFO - __main__ - EVALERR: 1.513864517211914%\n",
            "12/10/2021 08:48:26 - INFO - __main__ - Saving model checkpoint 2001000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2001000\n",
            "12/10/2021 08:55:10 - INFO - __main__ - eta: 19:13:10  iter: 2001500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4319 (1.5209)\n",
            "    epoch_metrics - ex_cnt: 44800.0000 (44800.0000)  loss: 1.5139 (1.5139)\n",
            "    time_info     - compute: 0.7792 (1.4266)  data: 0.0022 (0.6230)  compute1: 0.7128 (0.7216)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 09:01:45 - INFO - __main__ - eta: 16:31:42  iter: 2002000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4169 (1.5199)\n",
            "    epoch_metrics - ex_cnt: 44800.0000 (44800.0000)  loss: 1.5139 (1.5139)\n",
            "    time_info     - compute: 0.7800 (1.2396)  data: 0.0024 (0.4409)  compute1: 0.7095 (0.7214)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 09:01:45 - INFO - __main__ - PROGRESS: 97.6585%\n",
            "12/10/2021 09:01:45 - INFO - __main__ - EVALERR: 1.5240638256072998%\n",
            "12/10/2021 09:01:52 - INFO - __main__ - Saving model checkpoint 2002000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2002000\n",
            "12/10/2021 09:08:37 - INFO - __main__ - eta: 15:06:39  iter: 2002500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4093 (1.5145)\n",
            "    epoch_metrics - ex_cnt: 44800.0000 (54400.0000)  loss: 1.5139 (1.5190)\n",
            "    time_info     - compute: 0.7835 (1.1452)  data: 0.0028 (0.3449)  compute1: 0.7126 (0.7221)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 09:15:17 - INFO - __main__ - eta: 14:06:54  iter: 2003000  max mem: 10787\n",
            "    batch_metrics - loss: 1.3743 (1.5122)\n",
            "    epoch_metrics - ex_cnt: 44800.0000 (54400.0000)  loss: 1.5139 (1.5190)\n",
            "    time_info     - compute: 0.7860 (1.0812)  data: 0.0031 (0.2818)  compute1: 0.7159 (0.7221)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 09:15:17 - INFO - __main__ - PROGRESS: 97.7073%\n",
            "12/10/2021 09:15:17 - INFO - __main__ - EVALERR: 1.4990874528884888%\n",
            "12/10/2021 09:15:23 - INFO - __main__ - Saving model checkpoint 2003000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2003000\n",
            "12/10/2021 09:22:03 - INFO - __main__ - eta: 13:25:27  iter: 2003500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4213 (1.5098)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (57600.0000)  loss: 1.5139 (1.5123)\n",
            "    time_info     - compute: 0.7970 (1.0393)  data: 0.0028 (0.2405)  compute1: 0.7166 (0.7215)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 09:28:47 - INFO - __main__ - eta: 12:52:47  iter: 2004000  max mem: 10787\n",
            "    batch_metrics - loss: 1.3850 (1.5140)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (57600.0000)  loss: 1.5139 (1.5123)\n",
            "    time_info     - compute: 0.7966 (1.0080)  data: 0.0026 (0.2085)  compute1: 0.7080 (0.7220)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 09:28:47 - INFO - __main__ - PROGRESS: 97.7561%\n",
            "12/10/2021 09:28:47 - INFO - __main__ - EVALERR: 1.5187636613845825%\n",
            "12/10/2021 09:28:55 - INFO - __main__ - Saving model checkpoint 2004000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2004000\n",
            "12/10/2021 09:35:42 - INFO - __main__ - eta: 12:28:16  iter: 2004500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4355 (1.5136)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (59200.0000)  loss: 1.5139 (1.5139)\n",
            "    time_info     - compute: 0.7950 (0.9867)  data: 0.0025 (0.1862)  compute1: 0.7140 (0.7224)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 09:42:21 - INFO - __main__ - eta: 12:05:05  iter: 2005000  max mem: 10787\n",
            "    batch_metrics - loss: 1.6155 (1.5128)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (59200.0000)  loss: 1.5139 (1.5139)\n",
            "    time_info     - compute: 0.7938 (0.9668)  data: 0.0029 (0.1668)  compute1: 0.7129 (0.7226)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 09:42:21 - INFO - __main__ - PROGRESS: 97.8049%\n",
            "12/10/2021 09:42:21 - INFO - __main__ - EVALERR: 1.5087589025497437%\n",
            "12/10/2021 09:42:28 - INFO - __main__ - Saving model checkpoint 2005000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2005000\n",
            "12/10/2021 09:49:16 - INFO - __main__ - eta: 11:47:12  iter: 2005500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4306 (1.5106)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (60160.0000)  loss: 1.5139 (1.5129)\n",
            "    time_info     - compute: 0.7989 (0.9535)  data: 0.0025 (0.1525)  compute1: 0.7181 (0.7226)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 09:56:00 - INFO - __main__ - eta: 11:29:51  iter: 2006000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5194 (1.5102)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (60160.0000)  loss: 1.5139 (1.5129)\n",
            "    time_info     - compute: 0.8231 (0.9407)  data: 0.0032 (0.1395)  compute1: 0.7236 (0.7229)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 09:56:00 - INFO - __main__ - PROGRESS: 97.8537%\n",
            "12/10/2021 09:56:00 - INFO - __main__ - EVALERR: 1.4978634119033813%\n",
            "12/10/2021 09:56:07 - INFO - __main__ - Saving model checkpoint 2006000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2006000\n",
            "12/10/2021 10:02:55 - INFO - __main__ - eta: 11:15:33  iter: 2006500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4886 (1.5113)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (60800.0000)  loss: 1.5088 (1.5104)\n",
            "    time_info     - compute: 0.7815 (0.9318)  data: 0.0026 (0.1297)  compute1: 0.7218 (0.7233)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 10:09:41 - INFO - __main__ - eta: 11:01:25  iter: 2007000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4565 (1.5129)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (60800.0000)  loss: 1.5088 (1.5104)\n",
            "    time_info     - compute: 0.8085 (0.9229)  data: 0.0027 (0.1204)  compute1: 0.7128 (0.7235)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 10:09:41 - INFO - __main__ - PROGRESS: 97.9024%\n",
            "12/10/2021 10:09:41 - INFO - __main__ - EVALERR: 1.5281840562820435%\n",
            "12/10/2021 10:09:48 - INFO - __main__ - Saving model checkpoint 2007000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2007000\n",
            "12/10/2021 10:16:38 - INFO - __main__ - eta: 10:49:21  iter: 2007500  max mem: 10787\n",
            "    batch_metrics - loss: 1.3436 (1.5122)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (61257.1429)  loss: 1.5139 (1.5129)\n",
            "    time_info     - compute: 0.7992 (0.9167)  data: 0.0028 (0.1133)  compute1: 0.7175 (0.7239)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 10:23:26 - INFO - __main__ - eta: 10:37:06  iter: 2008000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5249 (1.5138)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (61257.1429)  loss: 1.5139 (1.5129)\n",
            "    time_info     - compute: 0.8176 (0.9102)  data: 0.0028 (0.1063)  compute1: 0.7187 (0.7241)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 10:23:26 - INFO - __main__ - PROGRESS: 97.9512%\n",
            "12/10/2021 10:23:26 - INFO - __main__ - EVALERR: 1.5199637413024902%\n",
            "12/10/2021 10:23:33 - INFO - __main__ - Saving model checkpoint 2008000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2008000\n",
            "12/10/2021 10:30:23 - INFO - __main__ - eta: 10:26:20  iter: 2008500  max mem: 10787\n",
            "    batch_metrics - loss: 1.2350 (1.5137)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (61600.0000)  loss: 1.5139 (1.5138)\n",
            "    time_info     - compute: 0.8171 (0.9056)  data: 0.0029 (0.1010)  compute1: 0.7196 (0.7245)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 10:37:06 - INFO - __main__ - eta: 10:14:51  iter: 2009000  max mem: 10787\n",
            "    batch_metrics - loss: 1.2871 (1.5159)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (61600.0000)  loss: 1.5139 (1.5138)\n",
            "    time_info     - compute: 0.7905 (0.8998)  data: 0.0028 (0.0955)  compute1: 0.7148 (0.7242)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 10:37:06 - INFO - __main__ - PROGRESS: 98.0%\n",
            "12/10/2021 10:37:06 - INFO - __main__ - EVALERR: 1.5316073894500732%\n",
            "12/10/2021 10:37:13 - INFO - __main__ - Saving model checkpoint 2009000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2009000\n",
            "12/10/2021 10:44:06 - INFO - __main__ - eta: 10:05:08  iter: 2009500  max mem: 10787\n",
            "    batch_metrics - loss: 1.5475 (1.5163)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (61866.6667)  loss: 1.5188 (1.5158)\n",
            "    time_info     - compute: 0.8205 (0.8965)  data: 0.0029 (0.0913)  compute1: 0.7196 (0.7246)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 10:50:53 - INFO - __main__ - eta: 9:54:54  iter: 2010000  max mem: 10787\n",
            "    batch_metrics - loss: 1.3564 (1.5153)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (61866.6667)  loss: 1.5188 (1.5158)\n",
            "    time_info     - compute: 0.8231 (0.8924)  data: 0.0040 (0.0869)  compute1: 0.7190 (0.7248)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 10:50:54 - INFO - __main__ - PROGRESS: 98.0488%\n",
            "12/10/2021 10:50:54 - INFO - __main__ - EVALERR: 1.5108898878097534%\n",
            "12/10/2021 10:51:01 - INFO - __main__ - Saving model checkpoint 2010000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2010000\n",
            "12/10/2021 10:57:50 - INFO - __main__ - eta: 9:45:34  iter: 2010500  max mem: 10787\n",
            "    batch_metrics - loss: 1.5351 (1.5152)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62080.0000)  loss: 1.5139 (1.5153)\n",
            "    time_info     - compute: 0.8102 (0.8895)  data: 0.0032 (0.0836)  compute1: 0.7199 (0.7249)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 11:04:40 - INFO - __main__ - eta: 9:36:03  iter: 2011000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4488 (1.5144)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62080.0000)  loss: 1.5139 (1.5153)\n",
            "    time_info     - compute: 0.7949 (0.8863)  data: 0.0033 (0.0799)  compute1: 0.7164 (0.7251)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 11:04:41 - INFO - __main__ - PROGRESS: 98.0976%\n",
            "12/10/2021 11:04:41 - INFO - __main__ - EVALERR: 1.5050891637802124%\n",
            "12/10/2021 11:04:48 - INFO - __main__ - Saving model checkpoint 2011000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2011000\n",
            "12/10/2021 11:11:40 - INFO - __main__ - eta: 9:27:19  iter: 2011500  max mem: 10787\n",
            "    batch_metrics - loss: 1.5733 (1.5144)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62254.5455)  loss: 1.5139 (1.5144)\n",
            "    time_info     - compute: 0.8097 (0.8841)  data: 0.0028 (0.0772)  compute1: 0.7179 (0.7252)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 11:18:31 - INFO - __main__ - eta: 9:18:16  iter: 2012000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5097 (1.5145)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62254.5455)  loss: 1.5139 (1.5144)\n",
            "    time_info     - compute: 0.8238 (0.8815)  data: 0.0034 (0.0741)  compute1: 0.7299 (0.7253)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 11:18:31 - INFO - __main__ - PROGRESS: 98.1463%\n",
            "12/10/2021 11:18:31 - INFO - __main__ - EVALERR: 1.5152225494384766%\n",
            "12/10/2021 11:18:38 - INFO - __main__ - Saving model checkpoint 2012000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2012000\n",
            "12/10/2021 11:25:30 - INFO - __main__ - eta: 9:09:50  iter: 2012500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4946 (1.5147)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62400.0000)  loss: 1.5139 (1.5144)\n",
            "    time_info     - compute: 0.8143 (0.8797)  data: 0.0029 (0.0719)  compute1: 0.7090 (0.7254)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 11:32:24 - INFO - __main__ - eta: 9:01:13  iter: 2013000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5499 (1.5140)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62400.0000)  loss: 1.5139 (1.5144)\n",
            "    time_info     - compute: 0.8023 (0.8777)  data: 0.0032 (0.0693)  compute1: 0.7216 (0.7257)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 11:32:24 - INFO - __main__ - PROGRESS: 98.1951%\n",
            "12/10/2021 11:32:24 - INFO - __main__ - EVALERR: 1.5086839199066162%\n",
            "12/10/2021 11:32:31 - INFO - __main__ - Saving model checkpoint 2013000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2013000\n",
            "12/10/2021 11:39:24 - INFO - __main__ - eta: 8:53:04  iter: 2013500  max mem: 10787\n",
            "    batch_metrics - loss: 1.5434 (1.5145)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62523.0769)  loss: 1.5139 (1.5140)\n",
            "    time_info     - compute: 0.8196 (0.8763)  data: 0.0029 (0.0674)  compute1: 0.7212 (0.7259)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 11:46:14 - INFO - __main__ - eta: 8:44:31  iter: 2014000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5459 (1.5153)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62523.0769)  loss: 1.5139 (1.5140)\n",
            "    time_info     - compute: 0.8270 (0.8742)  data: 0.0030 (0.0651)  compute1: 0.7284 (0.7260)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 11:46:14 - INFO - __main__ - PROGRESS: 98.2439%\n",
            "12/10/2021 11:46:14 - INFO - __main__ - EVALERR: 1.5310436487197876%\n",
            "12/10/2021 11:46:22 - INFO - __main__ - Saving model checkpoint 2014000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2014000\n",
            "12/10/2021 11:53:13 - INFO - __main__ - eta: 8:36:29  iter: 2014500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4707 (1.5150)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62628.5714)  loss: 1.5139 (1.5152)\n",
            "    time_info     - compute: 0.8050 (0.8729)  data: 0.0028 (0.0635)  compute1: 0.7129 (0.7262)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 12:00:05 - INFO - __main__ - eta: 8:28:12  iter: 2015000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5295 (1.5153)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62628.5714)  loss: 1.5139 (1.5152)\n",
            "    time_info     - compute: 0.8385 (0.8712)  data: 0.0045 (0.0615)  compute1: 0.7250 (0.7263)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 12:00:05 - INFO - __main__ - PROGRESS: 98.2927%\n",
            "12/10/2021 12:00:05 - INFO - __main__ - EVALERR: 1.516219973564148%\n",
            "12/10/2021 12:00:12 - INFO - __main__ - Saving model checkpoint 2015000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2015000\n",
            "12/10/2021 12:07:07 - INFO - __main__ - eta: 8:20:26  iter: 2015500  max mem: 10787\n",
            "    batch_metrics - loss: 1.6164 (1.5147)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62720.0000)  loss: 1.5152 (1.5153)\n",
            "    time_info     - compute: 0.8258 (0.8703)  data: 0.0034 (0.0601)  compute1: 0.7209 (0.7265)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 12:13:58 - INFO - __main__ - eta: 8:12:18  iter: 2016000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4552 (1.5148)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62720.0000)  loss: 1.5152 (1.5153)\n",
            "    time_info     - compute: 0.8083 (0.8688)  data: 0.0029 (0.0584)  compute1: 0.7148 (0.7266)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 12:13:58 - INFO - __main__ - PROGRESS: 98.3415%\n",
            "12/10/2021 12:13:58 - INFO - __main__ - EVALERR: 1.5069445371627808%\n",
            "12/10/2021 12:14:05 - INFO - __main__ - Saving model checkpoint 2016000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2016000\n",
            "12/10/2021 12:20:59 - INFO - __main__ - eta: 8:04:38  iter: 2016500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4248 (1.5156)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62800.0000)  loss: 1.5139 (1.5148)\n",
            "    time_info     - compute: 0.8205 (0.8680)  data: 0.0033 (0.0572)  compute1: 0.7191 (0.7266)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 12:27:52 - INFO - __main__ - eta: 7:56:41  iter: 2017000  max mem: 10787\n",
            "    batch_metrics - loss: 1.3507 (1.5151)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62800.0000)  loss: 1.5139 (1.5148)\n",
            "    time_info     - compute: 0.8393 (0.8667)  data: 0.0030 (0.0556)  compute1: 0.7231 (0.7267)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 12:27:52 - INFO - __main__ - PROGRESS: 98.3902%\n",
            "12/10/2021 12:27:52 - INFO - __main__ - EVALERR: 1.5208613872528076%\n",
            "12/10/2021 12:27:59 - INFO - __main__ - Saving model checkpoint 2017000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2017000\n",
            "12/10/2021 12:34:54 - INFO - __main__ - eta: 7:49:07  iter: 2017500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4074 (1.5148)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62870.5882)  loss: 1.5152 (1.5151)\n",
            "    time_info     - compute: 0.8102 (0.8661)  data: 0.0032 (0.0546)  compute1: 0.7181 (0.7269)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 12:41:45 - INFO - __main__ - eta: 7:41:14  iter: 2018000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5594 (1.5151)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62870.5882)  loss: 1.5152 (1.5151)\n",
            "    time_info     - compute: 0.8161 (0.8648)  data: 0.0032 (0.0532)  compute1: 0.7174 (0.7270)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 12:41:45 - INFO - __main__ - PROGRESS: 98.439%\n",
            "12/10/2021 12:41:45 - INFO - __main__ - EVALERR: 1.514449119567871%\n",
            "12/10/2021 12:41:53 - INFO - __main__ - Saving model checkpoint 2018000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2018000\n",
            "12/10/2021 12:48:47 - INFO - __main__ - eta: 7:33:44  iter: 2018500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4123 (1.5158)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62933.3333)  loss: 1.5144 (1.5151)\n",
            "    time_info     - compute: 0.8241 (0.8643)  data: 0.0037 (0.0523)  compute1: 0.7191 (0.7270)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 12:55:38 - INFO - __main__ - eta: 7:25:57  iter: 2019000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4038 (1.5158)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62933.3333)  loss: 1.5144 (1.5151)\n",
            "    time_info     - compute: 0.8172 (0.8631)  data: 0.0033 (0.0510)  compute1: 0.7190 (0.7271)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 12:55:38 - INFO - __main__ - PROGRESS: 98.4878%\n",
            "12/10/2021 12:55:38 - INFO - __main__ - EVALERR: 1.5286102294921875%\n",
            "12/10/2021 12:55:46 - INFO - __main__ - Saving model checkpoint 2019000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2019000\n",
            "12/10/2021 13:02:40 - INFO - __main__ - eta: 7:18:29  iter: 2019500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4831 (1.5162)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62989.4737)  loss: 1.5152 (1.5158)\n",
            "    time_info     - compute: 0.8292 (0.8626)  data: 0.0033 (0.0502)  compute1: 0.7213 (0.7272)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 13:09:31 - INFO - __main__ - eta: 7:10:47  iter: 2020000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4588 (1.5162)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (62989.4737)  loss: 1.5152 (1.5158)\n",
            "    time_info     - compute: 0.8141 (0.8616)  data: 0.0028 (0.0490)  compute1: 0.7232 (0.7273)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 13:09:31 - INFO - __main__ - PROGRESS: 98.5366%\n",
            "12/10/2021 13:09:31 - INFO - __main__ - EVALERR: 1.522363305091858%\n",
            "12/10/2021 13:09:39 - INFO - __main__ - Saving model checkpoint 2020000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2020000\n",
            "12/10/2021 13:16:33 - INFO - __main__ - eta: 7:03:23  iter: 2020500  max mem: 10787\n",
            "    batch_metrics - loss: 1.2472 (1.5157)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63040.0000)  loss: 1.5152 (1.5161)\n",
            "    time_info     - compute: 0.8195 (0.8611)  data: 0.0027 (0.0483)  compute1: 0.7178 (0.7274)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 13:23:24 - INFO - __main__ - eta: 6:55:46  iter: 2021000  max mem: 10787\n",
            "    batch_metrics - loss: 1.2783 (1.5158)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63040.0000)  loss: 1.5152 (1.5161)\n",
            "    time_info     - compute: 0.8122 (0.8602)  data: 0.0032 (0.0473)  compute1: 0.7159 (0.7274)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 13:23:24 - INFO - __main__ - PROGRESS: 98.5854%\n",
            "12/10/2021 13:23:24 - INFO - __main__ - EVALERR: 1.5080829858779907%\n",
            "12/10/2021 13:23:32 - INFO - __main__ - Saving model checkpoint 2021000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2021000\n",
            "12/10/2021 13:30:27 - INFO - __main__ - eta: 6:48:27  iter: 2021500  max mem: 10787\n",
            "    batch_metrics - loss: 1.3777 (1.5154)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63085.7143)  loss: 1.5152 (1.5157)\n",
            "    time_info     - compute: 0.7903 (0.8599)  data: 0.0028 (0.0467)  compute1: 0.7210 (0.7276)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 13:37:19 - INFO - __main__ - eta: 6:40:53  iter: 2022000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4339 (1.5155)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63085.7143)  loss: 1.5152 (1.5157)\n",
            "    time_info     - compute: 0.7961 (0.8591)  data: 0.0025 (0.0457)  compute1: 0.7189 (0.7276)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 13:37:19 - INFO - __main__ - PROGRESS: 98.6341%\n",
            "12/10/2021 13:37:19 - INFO - __main__ - EVALERR: 1.509199619293213%\n",
            "12/10/2021 13:37:27 - INFO - __main__ - Saving model checkpoint 2022000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2022000\n",
            "12/10/2021 13:44:22 - INFO - __main__ - eta: 6:33:36  iter: 2022500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4338 (1.5148)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63127.2727)  loss: 1.5144 (1.5154)\n",
            "    time_info     - compute: 0.8075 (0.8588)  data: 0.0043 (0.0451)  compute1: 0.7230 (0.7277)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 13:51:13 - INFO - __main__ - eta: 6:26:04  iter: 2023000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4945 (1.5146)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63127.2727)  loss: 1.5144 (1.5154)\n",
            "    time_info     - compute: 0.8084 (0.8580)  data: 0.0034 (0.0442)  compute1: 0.7160 (0.7277)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 13:51:13 - INFO - __main__ - PROGRESS: 98.6829%\n",
            "12/10/2021 13:51:13 - INFO - __main__ - EVALERR: 1.495935082435608%\n",
            "12/10/2021 13:51:21 - INFO - __main__ - Saving model checkpoint 2023000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2023000\n",
            "12/10/2021 13:58:16 - INFO - __main__ - eta: 6:18:48  iter: 2023500  max mem: 10787\n",
            "    batch_metrics - loss: 1.7144 (1.5152)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63165.2174)  loss: 1.5144 (1.5146)\n",
            "    time_info     - compute: 0.8100 (0.8577)  data: 0.0026 (0.0437)  compute1: 0.7231 (0.7278)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 14:05:09 - INFO - __main__ - eta: 6:11:22  iter: 2024000  max mem: 10787\n",
            "    batch_metrics - loss: 1.3861 (1.5152)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63165.2174)  loss: 1.5144 (1.5146)\n",
            "    time_info     - compute: 0.8104 (0.8570)  data: 0.0031 (0.0430)  compute1: 0.7256 (0.7277)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 14:05:09 - INFO - __main__ - PROGRESS: 98.7317%\n",
            "12/10/2021 14:05:09 - INFO - __main__ - EVALERR: 1.528649091720581%\n",
            "12/10/2021 14:05:16 - INFO - __main__ - Saving model checkpoint 2024000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2024000\n",
            "12/10/2021 14:12:10 - INFO - __main__ - eta: 6:04:06  iter: 2024500  max mem: 10787\n",
            "    batch_metrics - loss: 1.5287 (1.5153)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63200.0000)  loss: 1.5144 (1.5152)\n",
            "    time_info     - compute: 0.7901 (0.8567)  data: 0.0025 (0.0425)  compute1: 0.7153 (0.7278)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 14:19:01 - INFO - __main__ - eta: 5:56:40  iter: 2025000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4180 (1.5154)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63200.0000)  loss: 1.5144 (1.5152)\n",
            "    time_info     - compute: 0.8051 (0.8560)  data: 0.0027 (0.0417)  compute1: 0.7187 (0.7279)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 14:19:01 - INFO - __main__ - PROGRESS: 98.7805%\n",
            "12/10/2021 14:19:01 - INFO - __main__ - EVALERR: 1.5189143419265747%\n",
            "12/10/2021 14:19:08 - INFO - __main__ - Saving model checkpoint 2025000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2025000\n",
            "12/10/2021 14:25:59 - INFO - __main__ - eta: 5:49:22  iter: 2025500  max mem: 10787\n",
            "    batch_metrics - loss: 1.2422 (1.5149)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63232.0000)  loss: 1.5152 (1.5153)\n",
            "    time_info     - compute: 0.7916 (0.8556)  data: 0.0027 (0.0413)  compute1: 0.7178 (0.7278)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 14:32:45 - INFO - __main__ - eta: 5:41:54  iter: 2026000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5623 (1.5150)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63232.0000)  loss: 1.5152 (1.5153)\n",
            "    time_info     - compute: 0.8032 (0.8548)  data: 0.0028 (0.0406)  compute1: 0.7148 (0.7278)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 14:32:45 - INFO - __main__ - PROGRESS: 98.8293%\n",
            "12/10/2021 14:32:45 - INFO - __main__ - EVALERR: 1.5054712295532227%\n",
            "12/10/2021 14:32:52 - INFO - __main__ - Saving model checkpoint 2026000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2026000\n",
            "12/10/2021 14:39:42 - INFO - __main__ - eta: 5:34:37  iter: 2026500  max mem: 10787\n",
            "    batch_metrics - loss: 1.3266 (1.5145)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63259.8846)  loss: 1.5152 (1.5150)\n",
            "    time_info     - compute: 0.8338 (0.8544)  data: 0.0028 (0.0402)  compute1: 0.7335 (0.7278)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 14:46:33 - INFO - __main__ - eta: 5:27:16  iter: 2027000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5174 (1.5147)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63259.8846)  loss: 1.5152 (1.5150)\n",
            "    time_info     - compute: 0.7987 (0.8538)  data: 0.0028 (0.0395)  compute1: 0.7231 (0.7279)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 14:46:33 - INFO - __main__ - PROGRESS: 98.878%\n",
            "12/10/2021 14:46:33 - INFO - __main__ - EVALERR: 1.5067148208618164%\n",
            "12/10/2021 14:46:41 - INFO - __main__ - Saving model checkpoint 2027000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2027000\n",
            "12/10/2021 14:53:33 - INFO - __main__ - eta: 5:20:04  iter: 2027500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4874 (1.5146)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63287.2963)  loss: 1.5144 (1.5146)\n",
            "    time_info     - compute: 0.8137 (0.8535)  data: 0.0027 (0.0391)  compute1: 0.7131 (0.7280)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 15:00:23 - INFO - __main__ - eta: 5:12:44  iter: 2028000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4496 (1.5147)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63287.2963)  loss: 1.5144 (1.5146)\n",
            "    time_info     - compute: 0.8157 (0.8529)  data: 0.0027 (0.0385)  compute1: 0.7259 (0.7280)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 15:00:23 - INFO - __main__ - PROGRESS: 98.9268%\n",
            "12/10/2021 15:00:23 - INFO - __main__ - EVALERR: 1.5155547857284546%\n",
            "12/10/2021 15:00:31 - INFO - __main__ - Saving model checkpoint 2028000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2028000\n",
            "12/10/2021 15:07:25 - INFO - __main__ - eta: 5:05:34  iter: 2028500  max mem: 10787\n",
            "    batch_metrics - loss: 1.6023 (1.5145)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63312.7500)  loss: 1.5144 (1.5147)\n",
            "    time_info     - compute: 0.8044 (0.8528)  data: 0.0027 (0.0382)  compute1: 0.7190 (0.7281)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 15:14:14 - INFO - __main__ - eta: 4:58:14  iter: 2029000  max mem: 10787\n",
            "    batch_metrics - loss: 1.6863 (1.5147)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63312.7500)  loss: 1.5144 (1.5147)\n",
            "    time_info     - compute: 0.8154 (0.8521)  data: 0.0030 (0.0376)  compute1: 0.7207 (0.7281)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 15:14:14 - INFO - __main__ - PROGRESS: 98.9756%\n",
            "12/10/2021 15:14:14 - INFO - __main__ - EVALERR: 1.514884352684021%\n",
            "12/10/2021 15:14:21 - INFO - __main__ - Saving model checkpoint 2029000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2029000\n",
            "12/10/2021 15:21:13 - INFO - __main__ - eta: 4:51:03  iter: 2029500  max mem: 10787\n",
            "    batch_metrics - loss: 1.3764 (1.5144)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63336.4483)  loss: 1.5144 (1.5147)\n",
            "    time_info     - compute: 0.8204 (0.8519)  data: 0.0031 (0.0373)  compute1: 0.7230 (0.7281)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 15:28:03 - INFO - __main__ - eta: 4:43:47  iter: 2030000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4314 (1.5140)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63336.4483)  loss: 1.5144 (1.5147)\n",
            "    time_info     - compute: 0.8038 (0.8514)  data: 0.0026 (0.0367)  compute1: 0.7218 (0.7281)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 15:28:03 - INFO - __main__ - PROGRESS: 99.0244%\n",
            "12/10/2021 15:28:03 - INFO - __main__ - EVALERR: 1.4945095777511597%\n",
            "12/10/2021 15:28:10 - INFO - __main__ - Saving model checkpoint 2030000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2030000\n",
            "12/10/2021 15:35:03 - INFO - __main__ - eta: 4:36:37  iter: 2030500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4700 (1.5140)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63358.5667)  loss: 1.5144 (1.5140)\n",
            "    time_info     - compute: 0.8147 (0.8512)  data: 0.0030 (0.0364)  compute1: 0.7246 (0.7282)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 15:41:53 - INFO - __main__ - eta: 4:29:22  iter: 2031000  max mem: 10787\n",
            "    batch_metrics - loss: 1.6645 (1.5143)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63358.5667)  loss: 1.5144 (1.5140)\n",
            "    time_info     - compute: 0.8200 (0.8507)  data: 0.0031 (0.0359)  compute1: 0.7166 (0.7282)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 15:41:53 - INFO - __main__ - PROGRESS: 99.0732%\n",
            "12/10/2021 15:41:53 - INFO - __main__ - EVALERR: 1.5228132009506226%\n",
            "12/10/2021 15:42:00 - INFO - __main__ - Saving model checkpoint 2031000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2031000\n",
            "12/10/2021 15:48:54 - INFO - __main__ - eta: 4:22:14  iter: 2031500  max mem: 10787\n",
            "    batch_metrics - loss: 1.3313 (1.5147)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63379.2581)  loss: 1.5149 (1.5143)\n",
            "    time_info     - compute: 0.8076 (0.8505)  data: 0.0029 (0.0357)  compute1: 0.7226 (0.7283)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 15:55:41 - INFO - __main__ - eta: 4:14:59  iter: 2032000  max mem: 10787\n",
            "    batch_metrics - loss: 1.7978 (1.5150)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63379.2581)  loss: 1.5149 (1.5143)\n",
            "    time_info     - compute: 0.8136 (0.8500)  data: 0.0033 (0.0352)  compute1: 0.7167 (0.7283)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 15:55:42 - INFO - __main__ - PROGRESS: 99.122%\n",
            "12/10/2021 15:55:42 - INFO - __main__ - EVALERR: 1.537442684173584%\n",
            "12/10/2021 15:55:49 - INFO - __main__ - Saving model checkpoint 2032000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2032000\n",
            "12/10/2021 16:02:42 - INFO - __main__ - eta: 4:07:52  iter: 2032500  max mem: 10787\n",
            "    batch_metrics - loss: 1.3067 (1.5152)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63398.6562)  loss: 1.5149 (1.5150)\n",
            "    time_info     - compute: 0.8173 (0.8498)  data: 0.0029 (0.0349)  compute1: 0.7185 (0.7283)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 16:09:34 - INFO - __main__ - eta: 4:00:40  iter: 2033000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4945 (1.5155)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63398.6562)  loss: 1.5149 (1.5150)\n",
            "    time_info     - compute: 0.8159 (0.8494)  data: 0.0025 (0.0345)  compute1: 0.7155 (0.7283)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 16:09:34 - INFO - __main__ - PROGRESS: 99.1707%\n",
            "12/10/2021 16:09:34 - INFO - __main__ - EVALERR: 1.5289853811264038%\n",
            "12/10/2021 16:09:42 - INFO - __main__ - Saving model checkpoint 2033000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2033000\n",
            "12/10/2021 16:16:36 - INFO - __main__ - eta: 3:53:34  iter: 2033500  max mem: 10787\n",
            "    batch_metrics - loss: 1.5643 (1.5154)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63416.8788)  loss: 1.5156 (1.5154)\n",
            "    time_info     - compute: 0.8139 (0.8493)  data: 0.0029 (0.0343)  compute1: 0.7242 (0.7284)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 16:23:31 - INFO - __main__ - eta: 3:46:24  iter: 2034000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4190 (1.5154)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63416.8788)  loss: 1.5156 (1.5154)\n",
            "    time_info     - compute: 0.8314 (0.8491)  data: 0.0040 (0.0338)  compute1: 0.7166 (0.7284)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 16:23:31 - INFO - __main__ - PROGRESS: 99.2195%\n",
            "12/10/2021 16:23:31 - INFO - __main__ - EVALERR: 1.5126241445541382%\n",
            "12/10/2021 16:23:38 - INFO - __main__ - Saving model checkpoint 2034000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2034000\n",
            "12/10/2021 16:30:41 - INFO - __main__ - eta: 3:39:23  iter: 2034500  max mem: 10787\n",
            "    batch_metrics - loss: 1.5479 (1.5155)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63434.0294)  loss: 1.5149 (1.5154)\n",
            "    time_info     - compute: 0.8181 (0.8492)  data: 0.0026 (0.0336)  compute1: 0.7218 (0.7285)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 16:37:38 - INFO - __main__ - eta: 3:32:15  iter: 2035000  max mem: 10787\n",
            "    batch_metrics - loss: 1.2089 (1.5151)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63434.0294)  loss: 1.5149 (1.5154)\n",
            "    time_info     - compute: 0.8191 (0.8490)  data: 0.0031 (0.0332)  compute1: 0.7185 (0.7286)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 16:37:38 - INFO - __main__ - PROGRESS: 99.2683%\n",
            "12/10/2021 16:37:38 - INFO - __main__ - EVALERR: 1.5056356191635132%\n",
            "12/10/2021 16:37:45 - INFO - __main__ - Saving model checkpoint 2035000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2035000\n",
            "12/10/2021 16:44:46 - INFO - __main__ - eta: 3:25:12  iter: 2035500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4308 (1.5148)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63450.2000)  loss: 1.5144 (1.5151)\n",
            "    time_info     - compute: 0.8131 (0.8491)  data: 0.0029 (0.0330)  compute1: 0.7235 (0.7287)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 16:51:45 - INFO - __main__ - eta: 3:18:05  iter: 2036000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4028 (1.5147)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63450.2000)  loss: 1.5144 (1.5151)\n",
            "    time_info     - compute: 0.8048 (0.8489)  data: 0.0038 (0.0326)  compute1: 0.7237 (0.7288)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 16:51:45 - INFO - __main__ - PROGRESS: 99.3171%\n",
            "12/10/2021 16:51:45 - INFO - __main__ - EVALERR: 1.5011154413223267%\n",
            "12/10/2021 16:51:52 - INFO - __main__ - Saving model checkpoint 2036000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2036000\n",
            "12/10/2021 16:58:51 - INFO - __main__ - eta: 3:11:01  iter: 2036500  max mem: 10787\n",
            "    batch_metrics - loss: 1.6483 (1.5149)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63465.4722)  loss: 1.5144 (1.5147)\n",
            "    time_info     - compute: 0.9362 (0.8490)  data: 0.0051 (0.0324)  compute1: 0.7368 (0.7289)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 17:05:50 - INFO - __main__ - eta: 3:03:54  iter: 2037000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5159 (1.5149)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63465.4722)  loss: 1.5144 (1.5147)\n",
            "    time_info     - compute: 0.8230 (0.8488)  data: 0.0030 (0.0321)  compute1: 0.7189 (0.7289)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 17:05:50 - INFO - __main__ - PROGRESS: 99.3659%\n",
            "12/10/2021 17:05:50 - INFO - __main__ - EVALERR: 1.520247459411621%\n",
            "12/10/2021 17:05:59 - INFO - __main__ - Saving model checkpoint 2037000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2037000\n",
            "12/10/2021 17:13:06 - INFO - __main__ - eta: 2:56:54  iter: 2037500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4809 (1.5142)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63479.9189)  loss: 1.5144 (1.5148)\n",
            "    time_info     - compute: 0.8143 (0.8491)  data: 0.0030 (0.0319)  compute1: 0.7169 (0.7290)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 17:20:00 - INFO - __main__ - eta: 2:49:46  iter: 2038000  max mem: 10787\n",
            "    batch_metrics - loss: 1.3225 (1.5141)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63479.9189)  loss: 1.5144 (1.5148)\n",
            "    time_info     - compute: 0.8140 (0.8489)  data: 0.0033 (0.0316)  compute1: 0.7243 (0.7290)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 17:20:00 - INFO - __main__ - PROGRESS: 99.4146%\n",
            "12/10/2021 17:20:00 - INFO - __main__ - EVALERR: 1.4860402345657349%\n",
            "12/10/2021 17:20:08 - INFO - __main__ - Saving model checkpoint 2038000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2038000\n",
            "12/10/2021 17:27:10 - INFO - __main__ - eta: 2:42:43  iter: 2038500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4355 (1.5141)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63493.6053)  loss: 1.5126 (1.5141)\n",
            "    time_info     - compute: 0.8248 (0.8490)  data: 0.0032 (0.0314)  compute1: 0.7275 (0.7291)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 17:34:03 - INFO - __main__ - eta: 2:35:35  iter: 2039000  max mem: 10787\n",
            "    batch_metrics - loss: 1.3462 (1.5141)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63493.6053)  loss: 1.5126 (1.5141)\n",
            "    time_info     - compute: 0.8117 (0.8487)  data: 0.0032 (0.0311)  compute1: 0.7180 (0.7291)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 17:34:03 - INFO - __main__ - PROGRESS: 99.4634%\n",
            "12/10/2021 17:34:03 - INFO - __main__ - EVALERR: 1.5156219005584717%\n",
            "12/10/2021 17:34:11 - INFO - __main__ - Saving model checkpoint 2039000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2039000\n",
            "12/10/2021 17:41:09 - INFO - __main__ - eta: 2:28:31  iter: 2039500  max mem: 10787\n",
            "    batch_metrics - loss: 1.7310 (1.5143)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63506.5897)  loss: 1.5126 (1.5141)\n",
            "    time_info     - compute: 0.8226 (0.8488)  data: 0.0036 (0.0310)  compute1: 0.7261 (0.7292)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 17:48:06 - INFO - __main__ - eta: 2:21:25  iter: 2040000  max mem: 10787\n",
            "    batch_metrics - loss: 1.3756 (1.5143)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63506.5897)  loss: 1.5126 (1.5141)\n",
            "    time_info     - compute: 0.8189 (0.8486)  data: 0.0034 (0.0306)  compute1: 0.7223 (0.7292)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 17:48:06 - INFO - __main__ - PROGRESS: 99.5122%\n",
            "12/10/2021 17:48:06 - INFO - __main__ - EVALERR: 1.520071268081665%\n",
            "12/10/2021 17:48:13 - INFO - __main__ - Saving model checkpoint 2040000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2040000\n",
            "12/10/2021 17:55:13 - INFO - __main__ - eta: 2:14:22  iter: 2040500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4220 (1.5144)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63518.9250)  loss: 1.5126 (1.5143)\n",
            "    time_info     - compute: 0.8247 (0.8486)  data: 0.0028 (0.0305)  compute1: 0.7262 (0.7293)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 18:02:10 - INFO - __main__ - eta: 2:07:16  iter: 2041000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4492 (1.5145)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63518.9250)  loss: 1.5126 (1.5143)\n",
            "    time_info     - compute: 0.8353 (0.8485)  data: 0.0033 (0.0302)  compute1: 0.7227 (0.7294)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 18:02:10 - INFO - __main__ - PROGRESS: 99.561%\n",
            "12/10/2021 18:02:10 - INFO - __main__ - EVALERR: 1.522333025932312%\n",
            "12/10/2021 18:02:18 - INFO - __main__ - Saving model checkpoint 2041000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2041000\n",
            "12/10/2021 18:09:19 - INFO - __main__ - eta: 2:00:12  iter: 2041500  max mem: 10787\n",
            "    batch_metrics - loss: 1.3532 (1.5141)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63530.6585)  loss: 1.5149 (1.5145)\n",
            "    time_info     - compute: 0.8436 (0.8486)  data: 0.0043 (0.0301)  compute1: 0.7298 (0.7295)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 18:16:15 - INFO - __main__ - eta: 1:53:06  iter: 2042000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4403 (1.5137)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63530.6585)  loss: 1.5149 (1.5145)\n",
            "    time_info     - compute: 0.8441 (0.8484)  data: 0.0049 (0.0298)  compute1: 0.7234 (0.7295)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 18:16:15 - INFO - __main__ - PROGRESS: 99.6098%\n",
            "12/10/2021 18:16:15 - INFO - __main__ - EVALERR: 1.483713984489441%\n",
            "12/10/2021 18:16:22 - INFO - __main__ - Saving model checkpoint 2042000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2042000\n",
            "12/10/2021 18:23:19 - INFO - __main__ - eta: 1:46:02  iter: 2042500  max mem: 10787\n",
            "    batch_metrics - loss: 1.6051 (1.5139)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63541.8333)  loss: 1.5149 (1.5137)\n",
            "    time_info     - compute: 0.8046 (0.8484)  data: 0.0032 (0.0297)  compute1: 0.7188 (0.7296)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 18:30:16 - INFO - __main__ - eta: 1:38:57  iter: 2043000  max mem: 10787\n",
            "    batch_metrics - loss: 1.3699 (1.5138)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63541.8333)  loss: 1.5149 (1.5137)\n",
            "    time_info     - compute: 0.8250 (0.8482)  data: 0.0035 (0.0294)  compute1: 0.7243 (0.7296)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 18:30:16 - INFO - __main__ - PROGRESS: 99.6585%\n",
            "12/10/2021 18:30:16 - INFO - __main__ - EVALERR: 1.5153064727783203%\n",
            "12/10/2021 18:30:24 - INFO - __main__ - Saving model checkpoint 2043000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2043000\n",
            "12/10/2021 18:37:23 - INFO - __main__ - eta: 1:31:53  iter: 2043500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4179 (1.5140)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63552.4884)  loss: 1.5153 (1.5138)\n",
            "    time_info     - compute: 0.8184 (0.8483)  data: 0.0028 (0.0293)  compute1: 0.7191 (0.7297)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 18:44:16 - INFO - __main__ - eta: 1:24:48  iter: 2044000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5091 (1.5139)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63552.4884)  loss: 1.5153 (1.5138)\n",
            "    time_info     - compute: 0.8125 (0.8480)  data: 0.0033 (0.0290)  compute1: 0.7358 (0.7296)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 18:44:16 - INFO - __main__ - PROGRESS: 99.7073%\n",
            "12/10/2021 18:44:16 - INFO - __main__ - EVALERR: 1.5211255550384521%\n",
            "12/10/2021 18:44:23 - INFO - __main__ - Saving model checkpoint 2044000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2044000\n",
            "12/10/2021 18:51:18 - INFO - __main__ - eta: 1:17:43  iter: 2044500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4994 (1.5141)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63562.6591)  loss: 1.5153 (1.5139)\n",
            "    time_info     - compute: 0.8064 (0.8480)  data: 0.0027 (0.0289)  compute1: 0.7127 (0.7297)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 18:58:12 - INFO - __main__ - eta: 1:10:38  iter: 2045000  max mem: 10787\n",
            "    batch_metrics - loss: 1.3802 (1.5140)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63562.6591)  loss: 1.5153 (1.5139)\n",
            "    time_info     - compute: 0.8299 (0.8477)  data: 0.0030 (0.0286)  compute1: 0.7305 (0.7297)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 18:58:12 - INFO - __main__ - PROGRESS: 99.7561%\n",
            "12/10/2021 18:58:12 - INFO - __main__ - EVALERR: 1.5184059143066406%\n",
            "12/10/2021 18:58:20 - INFO - __main__ - Saving model checkpoint 2045000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2045000\n",
            "12/10/2021 19:05:15 - INFO - __main__ - eta: 1:03:34  iter: 2045500  max mem: 10787\n",
            "    batch_metrics - loss: 1.3992 (1.5138)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63572.3778)  loss: 1.5153 (1.5140)\n",
            "    time_info     - compute: 0.7959 (0.8477)  data: 0.0023 (0.0285)  compute1: 0.7161 (0.7297)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 19:12:06 - INFO - __main__ - eta: 0:56:29  iter: 2046000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5137 (1.5135)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63572.3778)  loss: 1.5153 (1.5140)\n",
            "    time_info     - compute: 0.8220 (0.8474)  data: 0.0029 (0.0283)  compute1: 0.7176 (0.7297)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 19:12:06 - INFO - __main__ - PROGRESS: 99.8049%\n",
            "12/10/2021 19:12:06 - INFO - __main__ - EVALERR: 1.4899123907089233%\n",
            "12/10/2021 19:12:14 - INFO - __main__ - Saving model checkpoint 2046000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2046000\n",
            "12/10/2021 19:19:08 - INFO - __main__ - eta: 0:49:25  iter: 2046500  max mem: 10787\n",
            "    batch_metrics - loss: 1.3923 (1.5135)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63581.6739)  loss: 1.5153 (1.5135)\n",
            "    time_info     - compute: 0.7991 (0.8474)  data: 0.0029 (0.0282)  compute1: 0.7185 (0.7298)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 19:26:02 - INFO - __main__ - eta: 0:42:21  iter: 2047000  max mem: 10787\n",
            "    batch_metrics - loss: 1.4651 (1.5136)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63581.6739)  loss: 1.5153 (1.5135)\n",
            "    time_info     - compute: 0.8296 (0.8472)  data: 0.0035 (0.0279)  compute1: 0.7212 (0.7298)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 19:26:02 - INFO - __main__ - PROGRESS: 99.8537%\n",
            "12/10/2021 19:26:02 - INFO - __main__ - EVALERR: 1.518169641494751%\n",
            "12/10/2021 19:26:10 - INFO - __main__ - Saving model checkpoint 2047000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2047000\n",
            "12/10/2021 19:33:09 - INFO - __main__ - eta: 0:35:18  iter: 2047500  max mem: 10787\n",
            "    batch_metrics - loss: 1.5318 (1.5134)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63590.5745)  loss: 1.5156 (1.5136)\n",
            "    time_info     - compute: 0.8097 (0.8473)  data: 0.0027 (0.0278)  compute1: 0.7176 (0.7298)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 19:40:02 - INFO - __main__ - eta: 0:28:14  iter: 2048000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5490 (1.5135)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63590.5745)  loss: 1.5156 (1.5136)\n",
            "    time_info     - compute: 0.8332 (0.8470)  data: 0.0038 (0.0276)  compute1: 0.7234 (0.7299)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 19:40:02 - INFO - __main__ - PROGRESS: 99.9024%\n",
            "12/10/2021 19:40:02 - INFO - __main__ - EVALERR: 1.5080435276031494%\n",
            "12/10/2021 19:40:09 - INFO - __main__ - Saving model checkpoint 2048000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2048000\n",
            "12/10/2021 19:47:08 - INFO - __main__ - eta: 0:21:10  iter: 2048500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4620 (1.5136)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63599.1042)  loss: 1.5153 (1.5135)\n",
            "    time_info     - compute: 0.8071 (0.8471)  data: 0.0030 (0.0275)  compute1: 0.7236 (0.7299)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 19:54:03 - INFO - __main__ - eta: 0:14:06  iter: 2049000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5403 (1.5136)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63599.1042)  loss: 1.5153 (1.5135)\n",
            "    time_info     - compute: 0.7990 (0.8469)  data: 0.0031 (0.0273)  compute1: 0.7195 (0.7299)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 19:54:03 - INFO - __main__ - PROGRESS: 99.9512%\n",
            "12/10/2021 19:54:03 - INFO - __main__ - EVALERR: 1.520783543586731%\n",
            "12/10/2021 19:54:11 - INFO - __main__ - Saving model checkpoint 2049000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2049000\n",
            "12/10/2021 20:01:07 - INFO - __main__ - eta: 0:07:03  iter: 2049500  max mem: 10787\n",
            "    batch_metrics - loss: 1.4112 (1.5139)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63607.2857)  loss: 1.5156 (1.5136)\n",
            "    time_info     - compute: 0.8345 (0.8469)  data: 0.0033 (0.0272)  compute1: 0.7320 (0.7299)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 20:08:02 - INFO - __main__ - eta: 0:00:00  iter: 2050000  max mem: 10787\n",
            "    batch_metrics - loss: 1.5073 (1.5142)\n",
            "    epoch_metrics - ex_cnt: 64000.0000 (63607.2857)  loss: 1.5156 (1.5136)\n",
            "    time_info     - compute: 0.7349 (0.8468)  data: 0.0018 (0.0270)  compute1: 0.7046 (0.7299)  compute2: 0.0000 (0.0000)\n",
            "    params        - bert_lr: 0.000000\n",
            "12/10/2021 20:08:02 - INFO - __main__ - PROGRESS: 100.0%\n",
            "12/10/2021 20:08:02 - INFO - __main__ - EVALERR: 1.5388435125350952%\n",
            "12/10/2021 20:08:09 - INFO - __main__ - Saving model checkpoint 2050000 to /content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2050000\n",
            "12/10/2021 20:08:10 - INFO - __main__ - Total training time: 11:41:32.764686 (0.0205 s / it)\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from oscar.modeling.modeling_bert import BertImgForPreTraining\n",
        "from transformers.pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n",
        "                                  BertTokenizer)\n",
        "\n",
        "from oscar.datasets.build import make_data_loader\n",
        "\n",
        "from transformers.pytorch_transformers import AdamW, WarmupLinearSchedule\n",
        "from oscar.utils.misc import mkdir, get_rank\n",
        "from oscar.utils.metric_logger import TensorboardLogger\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (BertConfig,)), ())\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertImgForPreTraining, BertTokenizer),\n",
        "}\n",
        "\n",
        "\n",
        "\"\"\" ****** Pretraining ****** \"\"\"\n",
        "\n",
        "class Arguments(object):\n",
        "  pass\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = Arguments()\n",
        "\n",
        "\n",
        "    ## Required parameters\n",
        "    args.data_dir = None # The input data dir. Should contain the .yaml files for the task.\n",
        "    args.dataset_file = None # The training dataset yaml file.\n",
        "    args.extra_dataset_file = None # The extra training dataset yaml file.\n",
        "    args.bert_model = None # Bert pre-trained model selected in the list: bert-base-uncased, \n",
        "                          #   \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"\n",
        "    args.output_dir = None # REQUIRED! The output directory where the model checkpoints will be written.\n",
        "\n",
        "    # image chunks\n",
        "    args.chunk_start_id = -1 # Image Chunk Start ID \n",
        "    args.chunk_end_id = -1 # Image Chunk End ID\n",
        "\n",
        "    ## Image parameters\n",
        "    args.max_img_seq_length = 50 # The maximum total input image sequence length.\n",
        "    args.img_feature_dim = 2054 # The Image Feature Dimension.\n",
        "    args.img_feature_type = 'faster_r-cnn' # faster_r-cnn or mask_r-cnn\n",
        "    args.use_layernorm = False # use_layernorm\n",
        "\n",
        "    args.drop_out = 0.1 # Drop out for BERT.\n",
        "\n",
        "    args.use_b = 1 # \"use_b\"\n",
        "    args.textb_sample_mode = 0  #\"0: sample from both texta&textb, \"\n",
        "                           #  \"1: sample from textb, \"\n",
        "                           #  \"2: sample from QA answers\"\n",
        "    args.extra_textb_sample_mode =1 \n",
        "    args.texta_false_prob = 0.0 # the probality that we sample wrong texta, should in [0.0, 0.5]\n",
        "\n",
        "    args.model_name_or_path = None # REQUIRED! Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(\n",
        "                            # ALL_MODELS))\n",
        "    args.config_name = \"\"  # Pretrained config name or path if not the same as model_name\n",
        "    args.tokenizer_name = \"\" # Pretrained tokenizer name or path if not the same as model_name\n",
        "    args.cache_dir = \"\" # Where do you want to store the pre-trained models downloaded from s3\n",
        "\n",
        "    args.max_seq_length = 35 # The maximum total input sequence length after WordPiece tokenization.\n",
        "                             #\"Sequences longer than this will be truncated, and sequences shorter than this will be padded.\")\n",
        "    args.do_train = True # Whether to run training.\n",
        "    args.learning_rate = 5e-5 # The initial learning rate for Adam.\n",
        "    args.max_iters = 2000000 # Maximal number of training iterations.\n",
        "    args.train_batch_size = 1024 # Batch size for training.\n",
        "    args.num_workers = 6  # Number of workers for dataset.\n",
        "    args.adam_epsilon = 1e-8 # Epsilon for Adam optimizer.\n",
        "    args.optim = 'adamw' # The optimizer used for Bert, [adamw, lamb], default: adamw\"\n",
        "    args.max_grad_norm = -1.0 # Max gradient norm.\n",
        "    args.warmup_steps = 0 # Linear warmup over warmup_steps.\n",
        "    args.no_cuda = False # Whether not to use CUDA when available\n",
        "    args.on_memory = True # Whether to load train samples into memory or use disk\n",
        "    args.do_lower_case = True # Whether to lower case the input text. True for uncased models, False for cased models.\n",
        "    args.local_rank = -1 # local_rank for distributed training on gpus\n",
        "    args.seed = 42 # random seed for initialization\n",
        "    args.gradient_accumulation_steps = 1 # Number of updates steps to accumualte before performing a backward/update pass.\n",
        "\n",
        "    args.from_scratch = True # train from scratch\n",
        "    args.use_img_layernorm = 0 # Normalize image features with bertlayernorm\n",
        "    args.img_layer_norm_eps = 1e-12 # The eps in image feature laynorm layer\n",
        "    # distributed\n",
        "    args.gpu_ids = '-1'\n",
        "    args.mask_loss_for_unmatched = 1 # masked language model loss for unmatched triplets\n",
        "    args.extra_loss_weight = 0.0 # the loss weight for the extra train data batch (should be in [0,1])\n",
        "    args.use_gtlabels = 1 # use groundtruth labels for text b or not\n",
        "\n",
        "    # logging\n",
        "    args.ckpt_period = 10000 # Period for saving checkpoint\n",
        "    args.log_period = 100 # Period for saving logging info\n",
        "\n",
        "    # arguments for pretrain\n",
        "\n",
        "    args.max_grad_norm = 10\n",
        "    args.gradient_accumulation_steps = 1\n",
        "    args.use_img_layernorm = True\n",
        "    args.output_dir = '/content/drive/MyDrive/OscarPlus/output/pretrain/test' \n",
        "    args.bert_model = 'bert'\n",
        "    args.model_name_or_path = 'bert-base-uncased'\n",
        "    # args.model_name_or_path = '/content/drive/MyDrive/OscarPlus/weights/pretrain/pretrained_base/checkpoint-2000000'\n",
        "    args.do_lower_case = True\n",
        "    args.learning_rate = 5e-05\n",
        "    args.warmup_steps = 0\n",
        "    args.do_train = True\n",
        "    args.max_seq_length = 35\n",
        "    args.on_memory = True\n",
        "    args.max_img_seq_length = 50\n",
        "    args.img_feature_dim = 2054\n",
        "    args.drop_out = 0.1\n",
        "    args.train_batch_size = 64\n",
        "    args.ckpt_period = 1000\n",
        "    args.max_iters = 2050000\n",
        "    args.log_period = 500\n",
        "    args.data_dir = '/content/drive/MyDrive/OscarPlus/datasets/pretrain'\n",
        "    args.dataset_file = '/content/drive/MyDrive/OscarPlus/datasets/pretrain/coco.yaml'\n",
        "    args.from_scratch = False\n",
        "    args.textb_sample_mode = 1\n",
        "    args.texta_false_prob = 0.25\n",
        "\n",
        "\n",
        "    if args.gpu_ids != '-1':\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_ids\n",
        "\n",
        "    args.num_gpus = int(\n",
        "        os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1\n",
        "    args.distributed = args.num_gpus > 1\n",
        "    print(args.distributed)\n",
        "\n",
        "    if args.gpu_ids != '-1':\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_ids\n",
        "\n",
        "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n",
        "        logger.info(\"Output Directory Exists.\")\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(\n",
        "            backend='nccl', init_method=\"env://\"\n",
        "        )\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
        "        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s\",\n",
        "        args.local_rank, device, args.n_gpu, bool(args.local_rank != -1)\n",
        "    )\n",
        "\n",
        "    if args.gradient_accumulation_steps < 1:\n",
        "        raise ValueError(\n",
        "            \"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
        "                args.gradient_accumulation_steps))\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    if not args.do_train:\n",
        "        raise ValueError(\n",
        "            \"Training is currently the only implemented execution option. Please set `do_train`.\")\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        mkdir(args.output_dir)\n",
        "\n",
        "    last_checkpoint_dir = None\n",
        "    arguments = {\"iteration\": 0}\n",
        "    if os.path.exists(args.output_dir):\n",
        "        save_file = os.path.join(args.output_dir, \"last_checkpoint\")\n",
        "        try:\n",
        "            with open(save_file, \"r\") as f:\n",
        "                last_saved = f.read()\n",
        "                last_saved = last_saved.strip()\n",
        "        except IOError:\n",
        "            # if file doesn't exist, maybe because it has just been\n",
        "            # deleted by a separate process\n",
        "            last_saved = \"\"\n",
        "        if last_saved:\n",
        "            folder_name = os.path.splitext(last_saved.split('/')[0])[0] # in the form of checkpoint-00001 or checkpoint-00001/pytorch_model.bin\n",
        "            last_checkpoint_dir = os.path.join(args.output_dir, folder_name)\n",
        "            arguments[\"iteration\"] = int(folder_name.split('-')[-1])\n",
        "            assert os.path.isfile(os.path.join(last_checkpoint_dir, WEIGHTS_NAME)), \"Last_checkpoint detected, but file not found!\"\n",
        "\n",
        "    # # define ckpt path\n",
        "    # last_checkpoint_dir = '/content/drive/MyDrive/OscarPlus/weights/pretrain/pretrained_base/checkpoint-2000000'\n",
        "    print(\"Last checkpoint dir: \", last_checkpoint_dir)\n",
        "\n",
        "    # model first\n",
        "    if get_rank() != 0:\n",
        "        torch.distributed.barrier()\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.bert_model]\n",
        "    if last_checkpoint_dir is not None:  # recovery\n",
        "        args.model_name_or_path = last_checkpoint_dir\n",
        "        logger.info(\" -> Recovering model from {}\".format(last_checkpoint_dir))\n",
        "\n",
        "    config = config_class.from_pretrained(\n",
        "        args.config_name if args.config_name else args.model_name_or_path,\n",
        "    )\n",
        "    config.img_layer_norm_eps = args.img_layer_norm_eps\n",
        "    config.use_img_layernorm = args.use_img_layernorm\n",
        "\n",
        "    # discrete code\n",
        "    config.img_feature_dim = args.img_feature_dim\n",
        "    config.img_feature_type = args.img_feature_type\n",
        "    config.hidden_dropout_prob = args.drop_out\n",
        "    if args.texta_false_prob < 0.5 and (args.texta_false_prob > 0 or not args.use_b):\n",
        "        args.num_contrast_classes = 3\n",
        "    else:\n",
        "        args.num_contrast_classes = 2\n",
        "    config.num_contrast_classes = args.num_contrast_classes\n",
        "\n",
        "    # Prepare model\n",
        "    # model = BertForPreTraining.from_pretrained(args.bert_model)\n",
        "    load_num = 0\n",
        "    while load_num < 10:\n",
        "        try:\n",
        "            model = BertImgForPreTraining.from_pretrained(\n",
        "                args.model_name_or_path,\n",
        "                from_tf=bool('.ckpt' in args.model_name_or_path),\n",
        "                config=config)\n",
        "            break\n",
        "        except:\n",
        "            load_num += 1\n",
        "\n",
        "    # train from scratch\n",
        "    if args.from_scratch:\n",
        "        if last_checkpoint_dir is None:\n",
        "            logger.info(\"Training from scratch ... \")\n",
        "            model.apply(model.init_weights)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    logger.info(\n",
        "        'Total Parameters: {}'.format(total_params))\n",
        "\n",
        "    for key, val in vars(config).items():\n",
        "        setattr(args, key, val)\n",
        "\n",
        "    if get_rank() == 0 and args.local_rank != -1:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    tb_log_dir = os.path.join(args.output_dir, 'train_logs')\n",
        "    meters = TensorboardLogger(\n",
        "        log_dir=tb_log_dir,\n",
        "        delimiter=\"  \",\n",
        "    )\n",
        "\n",
        "    # Prepare optimizer\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if\n",
        "                    not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if\n",
        "                    any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                              lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = WarmupLinearSchedule(optimizer,\n",
        "                                     warmup_steps=args.warmup_steps,\n",
        "                                     t_total=args.max_iters)\n",
        "\n",
        "    if arguments['iteration'] > 0 and os.path.isfile(os.path.join(last_checkpoint_dir, 'optimizer.pth')):  # recovery\n",
        "        logger.info(\n",
        "            \"Load BERT optimizer from {}\".format(last_checkpoint_dir))\n",
        "        optimizer_to_load = torch.load(\n",
        "            os.path.join(last_checkpoint_dir, 'optimizer.pth'),\n",
        "            map_location=torch.device(\"cpu\"))\n",
        "        optimizer.load_state_dict(optimizer_to_load.pop(\"optimizer\"))\n",
        "        scheduler.load_state_dict(optimizer_to_load.pop(\"scheduler\"))\n",
        "\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank,\n",
        "            find_unused_parameters=True)\n",
        "    elif args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # train_examples = None\n",
        "    train_dataloaders = make_data_loader(\n",
        "        args, is_distributed=args.distributed, arguments=arguments\n",
        "    )\n",
        "\n",
        "    if isinstance(train_dataloaders, list):\n",
        "        train_dataloader = train_dataloaders[0]\n",
        "    else:\n",
        "        train_dataloader = train_dataloaders\n",
        "    train_dataloader_extra = [None] * len(train_dataloader)\n",
        "    if isinstance(train_dataloaders, list) and len(train_dataloaders) > 1:\n",
        "        logger.info(\"Having two train dataloaders!\")\n",
        "        train_dataloader_extra = train_dataloaders[1]\n",
        "    tokenizer = train_dataloader.dataset.tokenizer\n",
        "\n",
        "    # torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    max_iter = len(train_dataloader)\n",
        "    start_iter = arguments[\"iteration\"]\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\" Num examples = {}\".format(len(train_dataloader.dataset)))\n",
        "    logger.info(\"  Instantaneous batch size = %d\",\n",
        "                args.train_batch_size // args.gradient_accumulation_steps)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\",\n",
        "                args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\",\n",
        "                max_iter // args.gradient_accumulation_steps)\n",
        "\n",
        "    log_json = {}\n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    clock_started = False\n",
        "    # Every args.ckpt_period, report train_score and save model\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, (batch, batch_extra) in enumerate(zip(train_dataloader, train_dataloader_extra), start_iter):\n",
        "        if not clock_started:\n",
        "            start_training_time = time.time()\n",
        "            end = time.time()\n",
        "            clock_started = True\n",
        "\n",
        "        def data_process(mini_batch):\n",
        "            images, targets, qa_inds = \\\n",
        "                mini_batch[0], mini_batch[1], mini_batch[2]\n",
        "            targets_transposed = list(zip(*targets))\n",
        "            input_ids = torch.stack(targets_transposed[0]).to(args.device, non_blocking=True)\n",
        "            input_mask = torch.stack(targets_transposed[1]).to(args.device, non_blocking=True)\n",
        "            segment_ids = torch.stack(targets_transposed[2]).to(args.device, non_blocking=True)\n",
        "            lm_label_ids = torch.stack(targets_transposed[3]).to(args.device, non_blocking=True)\n",
        "            is_next = torch.stack(targets_transposed[4]).to(args.device, non_blocking=True)\n",
        "            is_img_match = torch.stack(targets_transposed[5]).to(args.device, non_blocking=True)\n",
        "\n",
        "            return images, input_ids, input_mask, segment_ids, lm_label_ids, is_next\n",
        "\n",
        "        images1, input_ids1, input_mask1, segment_ids1, lm_label_ids1, is_next1 \\\n",
        "            = data_process(batch)\n",
        "        if batch_extra is not None:\n",
        "            images2, input_ids2, input_mask2, segment_ids2, lm_label_ids2, is_next2 \\\n",
        "                = data_process(batch_extra)\n",
        "\n",
        "        data_time = time.time() - end\n",
        "\n",
        "        def forward_backward(images, input_ids, input_mask, segment_ids,\n",
        "                             lm_label_ids, is_next, loss_weight=1.0):\n",
        "            # feature as input\n",
        "            image_features = torch.stack(images).to(args.device, non_blocking=True)\n",
        "\n",
        "            outputs = model(input_ids, segment_ids, input_mask,\n",
        "                            lm_label_ids, is_next, img_feats=image_features)\n",
        "\n",
        "            loss = loss_weight * outputs[0]\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu.\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            return loss.item(), input_ids.size(0)\n",
        "\n",
        "        start1 = time.time()\n",
        "        loss1, nb_tr_example1 = forward_backward(\n",
        "            images1, input_ids1, input_mask1,\n",
        "            segment_ids1, lm_label_ids1, is_next1,\n",
        "            loss_weight=1.0-args.extra_loss_weight\n",
        "        )\n",
        "        tr_loss += loss1\n",
        "        nb_tr_examples += nb_tr_example1\n",
        "        compute_time1 = time.time() - start1\n",
        "\n",
        "        loss2, nb_tr_example2 = 0.0, 0\n",
        "        compute_time2 = 0.0\n",
        "        if batch_extra is not None:\n",
        "            start2 = time.time()\n",
        "            loss2, nb_tr_example2 = forward_backward(\n",
        "                images2, input_ids2, input_mask2,\n",
        "                segment_ids2, lm_label_ids2, is_next2,\n",
        "                loss_weight=args.extra_loss_weight\n",
        "            )\n",
        "            tr_loss += loss2\n",
        "            nb_tr_examples += nb_tr_example2\n",
        "            compute_time2 = time.time() - start2\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        arguments[\"iteration\"] = step + 1\n",
        "\n",
        "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "            # do gradient clipping\n",
        "            if args.max_grad_norm > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "            # do the optimization steps\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Update learning rate schedule\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time = time.time() - end\n",
        "            end = time.time()\n",
        "            metrics_to_log = {\n",
        "                'time_info': {'compute': batch_time, 'data': data_time,\n",
        "                              'compute1': compute_time1,\n",
        "                              'compute2': compute_time2},\n",
        "                'batch_metrics': {'loss': loss1+loss2}\n",
        "            }\n",
        "            params_to_log = {'params': {'bert_lr': optimizer.param_groups[0][\"lr\"]}}\n",
        "            meters.update_metrics(metrics_to_log)\n",
        "            meters.update_params(params_to_log)\n",
        "\n",
        "            if args.log_period > 0 and (step + 1) % args.log_period == 0:\n",
        "                avg_time = meters.meters['time_info']['compute'].global_avg\n",
        "                eta_seconds = avg_time * (max_iter - step - 1)\n",
        "                eta_string = str(\n",
        "                    datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                logger.info(\n",
        "                    meters.delimiter.join(\n",
        "                        [\n",
        "                            \"eta: {eta}\",\n",
        "                            \"iter: {iter}\",\n",
        "                            \"max mem: {memory:.0f}\",\n",
        "                        ]\n",
        "                    ).format(\n",
        "                        eta=eta_string,\n",
        "                        iter=step + 1,\n",
        "                        memory=torch.cuda.max_memory_allocated() / 1024.0 / 1024.0,\n",
        "                    ) + \"\\n    \" + meters.get_logs(step + 1)\n",
        "                )\n",
        "\n",
        "        if (step + 1) == max_iter or (step + 1) % args.ckpt_period == 0:  # Save a trained model\n",
        "            log_json[step+1] = tr_loss\n",
        "            train_metrics_total = torch.Tensor([tr_loss, nb_tr_examples, nb_tr_steps]).to(args.device)\n",
        "            if args.distributed:\n",
        "              torch.distributed.all_reduce(train_metrics_total)\n",
        "            # reset metrics\n",
        "            tr_loss = 0\n",
        "            nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "            if get_rank() == 0:\n",
        "                # report metrics\n",
        "                train_score_gathered = train_metrics_total[0] / \\\n",
        "                                       train_metrics_total[2]\n",
        "                logger.info(\"PROGRESS: {}%\".format(\n",
        "                    round(100 * (step + 1) / max_iter, 4)))\n",
        "                logger.info(\n",
        "                    \"EVALERR: {}%\".format(train_score_gathered))\n",
        "                meters.update_metrics(\n",
        "                    {\n",
        "                        'epoch_metrics': {'ex_cnt': train_metrics_total[1],\n",
        "                                          'loss': train_score_gathered}\n",
        "                    }\n",
        "                )\n",
        "                with open(os.path.join(args.output_dir, 'loss_logs.json'),\n",
        "                          'w') as fp:\n",
        "                    json.dump(log_json, fp)\n",
        "\n",
        "                # save checkpoint\n",
        "                output_dir = os.path.join(args.output_dir,\n",
        "                                          'checkpoint-{:07d}'.format(\n",
        "                                              step + 1))\n",
        "                if not os.path.exists(output_dir):\n",
        "                    os.makedirs(output_dir)\n",
        "                model_to_save = model.module if hasattr(\n",
        "                    model,\n",
        "                    'module') else model  # Take care of distributed/parallel training\n",
        "                optimizer_to_save = {\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"scheduler\": scheduler.state_dict()}\n",
        "\n",
        "                save_num = 0\n",
        "                while save_num < 10:\n",
        "                    try:\n",
        "                        model_to_save.save_pretrained(output_dir)\n",
        "                        torch.save(args, os.path.join(output_dir,\n",
        "                                                      'training_args.bin'))\n",
        "                        tokenizer.save_pretrained(output_dir)\n",
        "                        torch.save(optimizer_to_save,\n",
        "                                   os.path.join(output_dir,\n",
        "                                                'optimizer.pth'))\n",
        "                        save_file = os.path.join(args.output_dir, \"last_checkpoint\")\n",
        "                        with open(save_file, \"w\") as f:\n",
        "                            f.write('checkpoint-{:07d}/pytorch_model.bin'.format(step + 1))\n",
        "                        break\n",
        "                    except:\n",
        "                        save_num += 1\n",
        "                logger.info(\n",
        "                    \"Saving model checkpoint {0} to {1}\".format(\n",
        "                        step + 1, output_dir))\n",
        "\n",
        "    if clock_started:\n",
        "        total_training_time = time.time() - start_training_time\n",
        "    else:\n",
        "        total_training_time = 0.0\n",
        "    total_time_str = str(datetime.timedelta(seconds=total_training_time))\n",
        "    logger.info(\n",
        "        \"Total training time: {} ({:.4f} s / it)\".format(\n",
        "            total_time_str, total_training_time / max_iter\n",
        "        )\n",
        "    )\n",
        "    # close the tb logger\n",
        "    meters.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFb_jurgRLU-"
      },
      "outputs": [],
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=1 oscar/run_oscarplus_pretrain.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Text Retrieval"
      ],
      "metadata": {
        "id": "2Pqc9oOTBO_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Text Retrieval"
      ],
      "metadata": {
        "id": "M4I7mFZUBUJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2020 Microsoft Corporation. Licensed under the MIT license. \n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import argparse\n",
        "import os\n",
        "import base64\n",
        "import os.path as op\n",
        "import random, json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "from oscar.utils.tsv_file import TSVFile\n",
        "from oscar.utils.logger import setup_logger\n",
        "from oscar.utils.misc import mkdir, set_seed\n",
        "from oscar.modeling.modeling_bert import ImageBertForSequenceClassification\n",
        "from transformers.pytorch_transformers import BertTokenizer, BertConfig \n",
        "from transformers.pytorch_transformers import AdamW, WarmupLinearSchedule, WarmupConstantSchedule\n",
        "\n",
        "\n",
        "class RetrievalDataset(Dataset):\n",
        "    \"\"\" Image/Text Retrieval Dataset\"\"\"\n",
        "    def __init__(self, tokenizer, args, split='train', is_train=True):\n",
        "        \"\"\"\n",
        "        tokenizer: tokenizer to process caption text.\n",
        "        args: configureation parameters including max_seq_length, etc.\n",
        "        split: used to infer the data used for training or testing. \n",
        "             All files are in .pt format of a dictionary with image keys and \n",
        "             image features (pytorch tensors), captions (list of str, support multiple\n",
        "             captions per image), labels (list of dictionary or str of all labels),\n",
        "\n",
        "        \"\"\"\n",
        "        super(RetrievalDataset, self).__init__()\n",
        "        self.img_file = args.img_feat_file\n",
        "        caption_file = op.join(args.data_dir, '{}_captions.pt'.format(split))\n",
        "        self.img_tsv = TSVFile(self.img_file)\n",
        "        self.captions = torch.load(caption_file)\n",
        "        self.img_keys = list(self.captions.keys())  # img_id as int\n",
        "        if not type(self.captions[self.img_keys[0]]) == list:\n",
        "            self.captions = {k: json.loads(self.captions[k]) for k in self.img_keys}\n",
        "\n",
        "        # get the image image_id to index map\n",
        "        imgid2idx_file = op.join(op.dirname(self.img_file), 'imageid2idx.json')\n",
        "        self.image_id2idx = json.load(open(imgid2idx_file))  # img_id as string\n",
        "        \n",
        "        if args.add_od_labels:\n",
        "            label_data_dir = op.dirname(self.img_file)\n",
        "            label_file = os.path.join(label_data_dir, \"predictions.tsv\")\n",
        "            self.label_tsv = TSVFile(label_file)\n",
        "            self.labels = {}\n",
        "            for line_no in range(self.label_tsv.num_rows()):\n",
        "                row = self.label_tsv.seek(line_no)\n",
        "                image_id = row[0]\n",
        "                if int(image_id) in self.img_keys:\n",
        "                    results = json.loads(row[1])\n",
        "                    objects = results['objects'] if type(\n",
        "                        results) == dict else results\n",
        "                    self.labels[int(image_id)] = {\n",
        "                        \"image_h\": results[\"image_h\"] if type(\n",
        "                            results) == dict else 600,\n",
        "                        \"image_w\": results[\"image_w\"] if type(\n",
        "                            results) == dict else 800,\n",
        "                        \"class\": [cur_d['class'] for cur_d in objects],\n",
        "                        \"boxes\": np.array([cur_d['rect'] for cur_d in objects],\n",
        "                                          dtype=np.float32)\n",
        "                    }\n",
        "            self.label_tsv._fp.close()\n",
        "            self.label_tsv._fp = None\n",
        "\n",
        "        if is_train:\n",
        "            self.num_captions_per_img = args.num_captions_per_img_train\n",
        "        else:\n",
        "            self.num_captions_per_img = args.num_captions_per_img_val\n",
        "            if args.eval_img_keys_file:\n",
        "                # select a subset of image keys for evaluation. eg. COCO 1k and 5k\n",
        "                # eval_img_keys_file is a list of image keys saved in tsv file\n",
        "                with open(op.join(args.data_dir, args.eval_img_keys_file), 'r') as f:\n",
        "                    img_keys = f.readlines()\n",
        "                self.img_keys = [int(k.strip()) for k in img_keys]\n",
        "                self.captions = {k: self.captions[k] for k in self.img_keys}\n",
        "                if args.add_od_labels:\n",
        "                    self.labels = {k: self.labels[k] for k in self.img_keys}\n",
        "\n",
        "            if args.eval_caption_index_file:\n",
        "                # hard negative image/caption indexs for retrieval re-rank setting.\n",
        "                # useful for mini val set to monitor the performance during training.\n",
        "                # However, it cannot be used together with cross image evaluation.\n",
        "                self.has_caption_indexs = True\n",
        "                assert not args.cross_image_eval \n",
        "                caption_index_file = op.join(args.data_dir, args.eval_caption_index_file)\n",
        "                self.caption_indexs = torch.load(caption_index_file)\n",
        "                if not type(self.caption_indexs[self.img_keys[0]]) == list:\n",
        "                    self.caption_indexs = {k: json.loads(self.caption_indexs[k]) for k in self.img_keys}\n",
        "            else:\n",
        "                self.has_caption_indexs = False\n",
        "        self.is_train = is_train\n",
        "        self.output_mode = args.output_mode\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = args.max_seq_length\n",
        "        self.max_img_seq_len = args.max_img_seq_length\n",
        "        self.args = args\n",
        "\n",
        "    def get_image_caption_index(self, index):\n",
        "        # return img_idx to access features and [img_key, cap_idx] to access caption\n",
        "        if not self.is_train and self.args.cross_image_eval:\n",
        "            img_idx = index // (self.num_captions_per_img * len(self.img_keys))\n",
        "            cap_idx = index % (self.num_captions_per_img * len(self.img_keys))\n",
        "            img_idx1 = cap_idx // self.num_captions_per_img\n",
        "            cap_idx1 = cap_idx % self.num_captions_per_img\n",
        "            return img_idx, [self.img_keys[img_idx1], cap_idx1]\n",
        "        if not self.is_train and self.has_caption_indexs:\n",
        "            img_idx = index // self.num_captions_per_img\n",
        "            cap_idx = index % self.num_captions_per_img\n",
        "            img_key1, cap_idx1 = self.caption_indexs[self.img_keys[img_idx]][cap_idx]\n",
        "            return img_idx, [img_key1, cap_idx1]\n",
        "        img_idx = index // self.num_captions_per_img\n",
        "        cap_idx = index % self.num_captions_per_img\n",
        "        return img_idx, [self.img_keys[img_idx], cap_idx]\n",
        "\n",
        "    def get_label(self, index):\n",
        "        img_idx, cap_idx = self.get_image_caption_index(index)\n",
        "        return 1 if self.img_keys[img_idx] == cap_idx[0] else 0\n",
        "\n",
        "    def get_od_labels(self, img_key):\n",
        "        if self.args.add_od_labels:\n",
        "            if type(self.labels[img_key]) == str:\n",
        "                od_labels = self.labels[img_key]\n",
        "            else:\n",
        "                od_labels = ' '.join(self.labels[img_key]['class'])\n",
        "            return od_labels\n",
        "\n",
        "    def tensorize_example(self, text_a, img_feat, text_b=None, \n",
        "            cls_token_segment_id=0, pad_token_segment_id=0,\n",
        "            sequence_a_segment_id=0, sequence_b_segment_id=1):\n",
        "        tokens_a = self.tokenizer.tokenize(text_a)\n",
        "        if len(tokens_a) > self.args.max_seq_length - 2:\n",
        "            tokens_a = tokens_a[:(self.args.max_seq_length - 2)]\n",
        "\n",
        "        tokens = [self.tokenizer.cls_token] + tokens_a + [self.tokenizer.sep_token]\n",
        "        segment_ids = [cls_token_segment_id] + [sequence_a_segment_id] * (len(tokens_a) + 1)\n",
        "        seq_a_len = len(tokens)\n",
        "        if text_b:\n",
        "            tokens_b = self.tokenizer.tokenize(text_b)\n",
        "            if len(tokens_b) > self.max_seq_len - len(tokens) - 1:\n",
        "                tokens_b = tokens_b[: (self.max_seq_len - len(tokens) - 1)]\n",
        "            tokens += tokens_b + [self.tokenizer.sep_token]\n",
        "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "        seq_len = len(tokens)\n",
        "        seq_padding_len = self.max_seq_len - seq_len\n",
        "        tokens += [self.tokenizer.pad_token] * seq_padding_len\n",
        "        segment_ids += [pad_token_segment_id] * seq_padding_len\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # image features\n",
        "        img_len = img_feat.shape[0]\n",
        "        if img_len > self.max_img_seq_len:\n",
        "            img_feat = img_feat[0 : self.max_img_seq_len, :]\n",
        "            img_len = img_feat.shape[0]\n",
        "            img_padding_len = 0\n",
        "        else:\n",
        "            img_padding_len = self.max_img_seq_len - img_len\n",
        "            padding_matrix = torch.zeros((img_padding_len, img_feat.shape[1]))\n",
        "            img_feat = torch.cat((img_feat, padding_matrix), 0)\n",
        "\n",
        "        # generate attention_mask\n",
        "        att_mask_type = self.args.att_mask_type\n",
        "        if att_mask_type == \"CLR\":\n",
        "            attention_mask = [1] * seq_len + [0] * seq_padding_len + \\\n",
        "                             [1] * img_len + [0] * img_padding_len\n",
        "        else:\n",
        "            # use 2D mask to represent the attention\n",
        "            max_len = self.max_seq_len + self.max_img_seq_len\n",
        "            attention_mask = torch.zeros((max_len, max_len), dtype=torch.long)\n",
        "            # full attention of C-C, L-L, R-R\n",
        "            c_start, c_end = 0, seq_a_len\n",
        "            l_start, l_end = seq_a_len, seq_len\n",
        "            r_start, r_end = self.max_seq_len, self.max_seq_len + img_len\n",
        "            attention_mask[c_start : c_end, c_start : c_end] = 1\n",
        "            attention_mask[l_start : l_end, l_start : l_end] = 1\n",
        "            attention_mask[r_start : r_end, r_start : r_end] = 1\n",
        "            if att_mask_type == 'CL':\n",
        "                attention_mask[c_start : c_end, l_start : l_end] = 1\n",
        "                attention_mask[l_start : l_end, c_start : c_end] = 1\n",
        "            elif att_mask_type == 'CR':\n",
        "                attention_mask[c_start : c_end, r_start : r_end] = 1\n",
        "                attention_mask[r_start : r_end, c_start : c_end] = 1\n",
        "            elif att_mask_type == 'LR':\n",
        "                attention_mask[l_start : l_end, r_start : r_end] = 1\n",
        "                attention_mask[r_start : r_end, l_start : l_end] = 1\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported attention mask type {}\".format(att_mask_type))\n",
        "        \n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
        "        segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
        "        return (input_ids, attention_mask, segment_ids, img_feat)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_train:\n",
        "            img_idx, cap_idxs = self.get_image_caption_index(index)\n",
        "            img_key = self.img_keys[img_idx]\n",
        "            feature = self.get_image(img_key)\n",
        "            caption = self.captions[cap_idxs[0]][cap_idxs[1]]\n",
        "            od_labels = self.get_od_labels(img_key)\n",
        "            example = self.tensorize_example(caption, feature, text_b=od_labels)\n",
        "\n",
        "            # select a negative pair\n",
        "            neg_img_indexs = list(range(0, img_idx)) + list(range(img_idx + 1, len(self.img_keys)))\n",
        "            img_idx_neg = random.choice(neg_img_indexs)\n",
        "            if random.random() <= 0.5:\n",
        "                # randomly select a negative caption from a different image.\n",
        "                cap_idx_neg = random.randint(0, self.num_captions_per_img - 1)\n",
        "                caption_neg = self.captions[self.img_keys[img_idx_neg]][cap_idx_neg]\n",
        "                example_neg = self.tensorize_example(caption_neg, feature, text_b=od_labels)\n",
        "            else:\n",
        "                # randomly select a negative image \n",
        "                feature_neg = self.get_image(self.img_keys[img_idx_neg])\n",
        "                od_labels_neg = self.get_od_labels(self.img_keys[img_idx_neg])\n",
        "                example_neg = self.tensorize_example(caption, feature_neg, text_b=od_labels_neg)\n",
        "\n",
        "            example_pair = tuple(list(example) + [1] + list(example_neg) + [0])\n",
        "            return index, example_pair\n",
        "        else:\n",
        "            img_idx, cap_idxs = self.get_image_caption_index(index)\n",
        "            img_key = self.img_keys[img_idx]\n",
        "            feature = self.get_image(img_key)\n",
        "            caption = self.captions[cap_idxs[0]][cap_idxs[1]]\n",
        "            od_labels = self.get_od_labels(img_key)\n",
        "            example = self.tensorize_example(caption, feature, text_b=od_labels)\n",
        "            label = 1 if img_key == cap_idxs[0] else 0\n",
        "            return index, tuple(list(example) + [label])\n",
        "\n",
        "    def get_image(self, image_id):\n",
        "        image_idx = self.image_id2idx[str(image_id)]\n",
        "        row = self.img_tsv.seek(image_idx)\n",
        "        num_boxes = int(row[1])\n",
        "        features = np.frombuffer(base64.b64decode(row[-1]),\n",
        "                                 dtype=np.float32).reshape((num_boxes, -1))\n",
        "        t_features = torch.from_numpy(features)\n",
        "        return t_features\n",
        "\n",
        "    def __len__(self):\n",
        "        if not self.is_train and self.args.cross_image_eval:\n",
        "            return len(self.img_keys) ** 2 * self.num_captions_per_img\n",
        "        return len(self.img_keys) * self.num_captions_per_img\n",
        "\n",
        "\n",
        "def compute_score_with_logits(logits, labels):\n",
        "    if logits.shape[1] > 1:\n",
        "        logits = torch.max(logits, 1)[1].data # argmax\n",
        "        scores = logits == labels \n",
        "    else:\n",
        "        scores = torch.zeros_like(labels).cuda()\n",
        "        for i, (logit, label) in enumerate(zip(logits, labels)):\n",
        "            logit_ = torch.sigmoid(logit)\n",
        "            if (logit_ >= 0.5 and label == 1) or (logit_ < 0.5 and label == 0):\n",
        "                scores[i] = 1\n",
        "    return scores\n",
        "\n",
        "\n",
        "def compute_ranks(dataset, results):\n",
        "    labels = np.array([dataset.get_label(i) for i in range(len(dataset))])\n",
        "    similarities = np.array([results[i] for i in range(len(dataset))])\n",
        "    if dataset.has_caption_indexs:\n",
        "        num_captions_per_img = dataset.num_captions_per_img\n",
        "    else:\n",
        "        num_captions_per_img = len(dataset.img_keys) * dataset.num_captions_per_img\n",
        "    labels = np.reshape(labels, [-1, num_captions_per_img])\n",
        "    similarities = np.reshape(similarities, [-1, num_captions_per_img])\n",
        "    i2t_ranks, t2i_ranks = [], []\n",
        "    for lab, sim in zip(labels, similarities):\n",
        "        inds = np.argsort(sim)[::-1]\n",
        "        rank = num_captions_per_img\n",
        "        for r, ind in enumerate(inds):\n",
        "            if lab[ind] == 1:\n",
        "                rank = r\n",
        "                break\n",
        "        i2t_ranks.append(rank)\n",
        "    if not dataset.has_caption_indexs:\n",
        "        labels = np.swapaxes(labels, 0, 1)\n",
        "        similarities = np.swapaxes(similarities, 0, 1)\n",
        "        for lab, sim in zip(labels, similarities):\n",
        "            inds = np.argsort(sim)[::-1]\n",
        "            rank = num_captions_per_img\n",
        "            for r, ind in enumerate(inds):\n",
        "                if lab[ind] == 1:\n",
        "                    rank = r\n",
        "                    break\n",
        "            t2i_ranks.append(rank)\n",
        "    return i2t_ranks, t2i_ranks\n",
        "\n",
        "\n",
        "def save_checkpoint(model, tokenizer, args, epoch, global_step):\n",
        "    checkpoint_dir = op.join(args.output_dir, 'checkpoint-{}-{}'.format(\n",
        "        epoch, global_step))\n",
        "    mkdir(checkpoint_dir)\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    save_num = 0\n",
        "    while (save_num < 10):\n",
        "        try:\n",
        "            model_to_save.save_pretrained(checkpoint_dir)\n",
        "            torch.save(args, op.join(checkpoint_dir, 'training_args.bin'))\n",
        "            tokenizer.save_pretrained(checkpoint_dir)\n",
        "            logger.info(\"Save checkpoint to {}\".format(checkpoint_dir))\n",
        "            break\n",
        "        except:\n",
        "            save_num += 1\n",
        "    if save_num == 10:\n",
        "        logger.info(\"Failed to save checkpoint after 10 trails.\")\n",
        "    return\n",
        "\n",
        "\n",
        "def train(args, train_dataset, val_dataset, model, tokenizer):\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) \n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, \n",
        "            batch_size=args.train_batch_size, num_workers=args.num_workers)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // \\\n",
        "                args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps \\\n",
        "                * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and scheduler\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not \\\n",
        "            any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if \\\n",
        "            any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    if args.scheduler == \"constant\":\n",
        "        scheduler = WarmupConstantSchedule(\n",
        "                optimizer, warmup_steps=args.warmup_steps)\n",
        "    elif args.scheduler == \"linear\":\n",
        "        scheduler = WarmupLinearSchedule(\n",
        "                optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown scheduler type: {}\".format(args.scheduler))\n",
        "\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\"  Total train batch size (w. parallel, & accumulation) = %d\",\n",
        "                   args.train_batch_size * args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step, global_loss, global_acc =0,  0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    log_json = []\n",
        "    best_score = 0\n",
        "    for epoch in range(int(args.num_train_epochs)):\n",
        "        for step, (_, batch) in enumerate(train_dataloader):\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "            inputs = {\n",
        "                'input_ids':      torch.cat((batch[0], batch[5]), dim=0),\n",
        "                'attention_mask': torch.cat((batch[1], batch[6]), dim=0),\n",
        "                'token_type_ids': torch.cat((batch[2], batch[7]), dim=0),\n",
        "                'img_feats':      torch.cat((batch[3], batch[8]), dim=0),\n",
        "                'labels':         torch.cat((batch[4], batch[9]), dim=0)\n",
        "            }\n",
        "            outputs = model(**inputs)\n",
        "            loss, logits = outputs[:2]\n",
        "            if args.n_gpu > 1: \n",
        "                loss = loss.mean() # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "            batch_score = compute_score_with_logits(logits, inputs['labels']).sum()\n",
        "            batch_acc = batch_score.item() / (args.train_batch_size * 2)\n",
        "            global_loss += loss.item()\n",
        "            global_acc += batch_acc\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                global_step += 1\n",
        "                scheduler.step()\n",
        "                optimizer.step()\n",
        "                model.zero_grad()\n",
        "                if global_step % args.logging_steps == 0:\n",
        "                    logger.info(\"Epoch: {}, global_step: {}, lr: {:.6f}, loss: {:.4f} ({:.4f}), \" \\\n",
        "                        \"score: {:.4f} ({:.4f})\".format(epoch, global_step, \n",
        "                        optimizer.param_groups[0][\"lr\"], loss, global_loss / global_step, \n",
        "                        batch_acc, global_acc / global_step)\n",
        "                    )\n",
        "\n",
        "                if (args.save_steps > 0 and global_step % args.save_steps == 0) or \\\n",
        "                        global_step == t_total:\n",
        "                    save_checkpoint(model, tokenizer, args, epoch, global_step) \n",
        "                    # evaluation\n",
        "                    if args.evaluate_during_training: \n",
        "                        logger.info(\"Perform evaluation at step: %d\" % (global_step))\n",
        "                        test_result = test(args, model, val_dataset)\n",
        "                        eval_result = evaluate(val_dataset, test_result)\n",
        "                        rank_accs = eval_result['i2t_retrieval']\n",
        "                        if rank_accs['R@1'] > best_score:\n",
        "                            best_score = rank_accs['R@1']\n",
        "                        epoch_log = {'epoch': epoch, 'global_step': global_step, \n",
        "                                     'R1': rank_accs['R@1'], 'R5': rank_accs['R@5'], \n",
        "                                     'R10': rank_accs['R@10'], 'best_R1':best_score}\n",
        "                        log_json.append(epoch_log)\n",
        "                        with open(args.output_dir + '/eval_logs.json', 'w') as fp:\n",
        "                            json.dump(log_json, fp) \n",
        "    return global_step, global_loss / global_step\n",
        "\n",
        "\n",
        "def test(args, model, eval_dataset):\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,\n",
        "            batch_size=args.eval_batch_size, num_workers=args.num_workers)\n",
        "    \n",
        "    logger.info(\"Num examples = {}\".format(len(eval_dataset)))\n",
        "    logger.info(\"Evaluation batch size = {}\".format(args.eval_batch_size))\n",
        "    model.eval()\n",
        "    results = {}\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "    for indexs, batch in tqdm(eval_dataloader):\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                'input_ids':      batch[0],\n",
        "                'attention_mask': batch[1],\n",
        "                'token_type_ids': batch[2],\n",
        "                'img_feats':      batch[3],\n",
        "                'labels':         batch[4]\n",
        "            }\n",
        "            _, logits = model(**inputs)[:2]\n",
        "            if args.num_labels == 2:\n",
        "                probs = softmax(logits)\n",
        "                result = probs[:, 1] # the confidence to be a matched pair\n",
        "            else:\n",
        "                result = logits\n",
        "            result = [_.to(torch.device(\"cpu\")) for _ in result]\n",
        "            results.update({idx.item(): res.item() for idx, res in zip(indexs, result)})\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate(eval_dataset, test_results):\n",
        "    i2t_ranks, t2i_ranks = compute_ranks(eval_dataset, test_results)\n",
        "    rank = [1, 5, 10]\n",
        "    i2t_accs = [sum([_ < r for _ in i2t_ranks]) / len(i2t_ranks) for r in rank]\n",
        "    logger.info(\"I2T Retrieval: {:.4f} @ R1, {:.4f} @ R5, {:.4f} @ R10\".format(\n",
        "                i2t_accs[0], i2t_accs[1], i2t_accs[2]))\n",
        "    eval_result = {\"i2t_retrieval\": {\"R@1\": i2t_accs[0], \"R@5\": i2t_accs[1], \"R@10\": i2t_accs[2]}}\n",
        "    if t2i_ranks:\n",
        "        t2i_accs = [sum([_ < r for _ in t2i_ranks]) / len(t2i_ranks) for r in rank]\n",
        "        logger.info(\"T2I Retrieval: {:.4f} @ R1, {:.4f} @ R5, {:.4f} @ R10\".format(\n",
        "                    t2i_accs[0], t2i_accs[1], t2i_accs[2]))\n",
        "        eval_result[\"t2i_retrieval\"] = {\"R@1\": t2i_accs[0], \"R@5\": t2i_accs[1], \"R@10\": t2i_accs[2]}\n",
        "    return eval_result\n",
        "\n",
        "\n",
        "def get_predict_file(args):\n",
        "    cc = []\n",
        "    data = op.basename(op.join(args.data_dir, '')[:-1])\n",
        "    if data != 'coco_ir':\n",
        "        cc.append(data)\n",
        "    cc.append(args.test_split)\n",
        "    if args.add_od_labels:\n",
        "        cc.append('wlabels{}'.format(args.od_label_type))\n",
        "    return op.join(args.eval_model_dir, '{}.results.pt'.format('.'.join(cc))) \n",
        "\n",
        "\n",
        "def restore_training_settings(args):\n",
        "    assert not args.do_train and (args.do_test or args.do_eval)\n",
        "    train_args = torch.load(op.join(args.eval_model_dir, 'training_args.bin'))\n",
        "    override_params = ['do_lower_case', 'img_feature_type', 'max_seq_length', \n",
        "            'max_img_seq_length', 'add_od_labels', 'od_label_type',\n",
        "            'use_img_layernorm', 'img_layer_norm_eps']\n",
        "    for param in override_params:\n",
        "        if hasattr(train_args, param):\n",
        "            train_v = getattr(train_args, param)\n",
        "            test_v = getattr(args, param)\n",
        "            if train_v != test_v:\n",
        "                logger.warning('Override {} with train args: {} -> {}'.format(param,\n",
        "                    test_v, train_v))\n",
        "                setattr(args, param, train_v)\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data_dir\", default='/content/drive/MyDrive/OscarPlus/datasets/retrieval/coco_ir', type=str, required=False,\n",
        "                        help=\"The input data dir with all required files.\")\n",
        "    parser.add_argument(\"--img_feat_file\", default='/content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.tsv', type=str, required=False,\n",
        "                        help=\"The absolute address of the image feature file.\")\n",
        "    parser.add_argument(\"--model_name_or_path\", default='/content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2050000-backup', type=str, required=False,\n",
        "                        help=\"Path to pre-trained model or model type. required for training.\")\n",
        "    parser.add_argument(\"--output_dir\", default='/content/drive/MyDrive/OscarPlus/output/retrieval', type=str, required=False,\n",
        "                        help=\"The output directory to save checkpoint and test results.\")\n",
        "    parser.add_argument(\"--loss_type\", default='sfmx', type=str, \n",
        "                        help=\"Loss function types: support kl, sfmx\")\n",
        "    parser.add_argument(\"--config_name\", default=\"\", type=str, \n",
        "                        help=\"Pretrained config name or path if not the same as model_name.\")\n",
        "    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str, \n",
        "                        help=\"Pretrained tokenizer name or path if not the same as model_name.\")\n",
        "    parser.add_argument(\"--max_seq_length\", default=70, type=int,\n",
        "                        help=\"The maximum total input sequence length after tokenization. \"\n",
        "                             \"Sequences longer than this will be truncated, \"\n",
        "                             \"sequences shorter will be padded.\"\n",
        "                             \"This number is calculated on COCO dataset\" \n",
        "                             \"If add object detection labels, the suggested length should be 70.\")\n",
        "    parser.add_argument(\"--do_train\", default=True, action='store_true', help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_test\", action='store_true', help=\"Whether to run inference.\")\n",
        "    parser.add_argument(\"--do_eval\", action='store_true', help=\"Whether to run performance valuation.\"\n",
        "                       \"do not activate if we want to inference on dataset without gt labels.\")\n",
        "    parser.add_argument(\"--test_split\", default='test', type=str, help='data split name.')\n",
        "    parser.add_argument(\"--eval_img_keys_file\", default='', type=str, \n",
        "                        help=\"image key tsv to select a subset of images for evaluation. \"\n",
        "                        \"This is useful in 5-folds evaluation. The topn index file is not \" \n",
        "                        \"needed in this case.\")\n",
        "    parser.add_argument(\"--eval_caption_index_file\", default='', type=str, \n",
        "                        help=\"index of a list of (img_key, cap_idx) for each image.\"\n",
        "                        \"this is used to perform re-rank using hard negative samples.\"\n",
        "                        \"useful for validation set to monitor the performance during training.\")\n",
        "    parser.add_argument(\"--cross_image_eval\", action='store_true', \n",
        "                        help=\"perform cross image inference, ie. each image with all texts from other images.\")\n",
        "    parser.add_argument(\"--add_od_labels\", default=True, action='store_true', \n",
        "                        help=\"Whether to add object detection labels or not.\")\n",
        "    parser.add_argument(\"--od_label_type\", default='vg', type=str, \n",
        "                        help=\"label type, support vg, gt, oid\")\n",
        "    parser.add_argument(\"--att_mask_type\", default='CLR', type=str, \n",
        "                        help=\"attention mask type, support ['CL', 'CR', 'LR', 'CLR']\"\n",
        "                        \"C: caption, L: labels, R: image regions; CLR is full attention by default.\"\n",
        "                        \"CL means attention between caption and labels.\"\n",
        "                        \"please pay attention to the order CLR, which is the default concat order.\")\n",
        "    parser.add_argument(\"--do_lower_case\", default=True, action='store_true', \n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "    parser.add_argument(\"--drop_out\", default=0.1, type=float, help=\"Drop out in BERT.\")\n",
        "    parser.add_argument(\"--max_img_seq_length\", default=70, type=int, \n",
        "                        help=\"The maximum total input image sequence length.\")\n",
        "    parser.add_argument(\"--img_feature_dim\", default=2054, type=int, \n",
        "                        help=\"The Image Feature Dimension.\")\n",
        "    parser.add_argument(\"--img_feature_type\", default='frcnn', type=str,\n",
        "                        help=\"Image feature type.\")\n",
        "    parser.add_argument(\"--use_img_layernorm\", type=int, default=1,\n",
        "                        help=\"Normalize image features with bertlayernorm\")\n",
        "    parser.add_argument(\"--img_layer_norm_eps\", default=1e-12, type=float,\n",
        "                        help=\"The eps in image feature laynorm layer\")\n",
        "    parser.add_argument(\"--per_gpu_train_batch_size\", default=32, type=int, \n",
        "                        help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\"--per_gpu_eval_batch_size\", default=32, type=int, \n",
        "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
        "    parser.add_argument(\"--output_mode\", default='classification', type=str,\n",
        "                        help=\"output mode, support classification or regression.\")\n",
        "    parser.add_argument(\"--num_labels\", default=2, type=int, \n",
        "                        help=\"num_labels is 2 for classification and 1 for regression.\")\n",
        "    parser.add_argument(\"--num_captions_per_img_train\", default=5, type=int,\n",
        "                        help=\"number of positive matched captions for each training image.\")\n",
        "    parser.add_argument(\"--num_captions_per_img_val\", default=20, type=int,\n",
        "                        help=\"number of captions for each testing image.\")\n",
        "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                        help=\"Number of updates steps to accumulate before backward.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=2e-5, type=float, help=\"The initial lr.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.05, type=float, help=\"Weight deay.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup.\")\n",
        "    parser.add_argument(\"--scheduler\", default='linear', type=str, help=\"constant or linear.\")\n",
        "    parser.add_argument(\"--num_workers\", default=4, type=int, help=\"Workers in dataloader.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", default=30, type=int, \n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--max_steps\", default=-1, type=int, \n",
        "                        help=\"Total number of training steps. Override num_train_epochs.\")\n",
        "    parser.add_argument('--logging_steps', type=int, default=20, help=\"Log every X steps.\")\n",
        "    parser.add_argument('--save_steps', type=int, default=100, \n",
        "                        help=\"Save checkpoint every X steps. Will also perform evaluatin.\")\n",
        "    parser.add_argument(\"--evaluate_during_training\", default=True, action='store_true', \n",
        "                        help=\"Run evaluation during training at each save_steps.\")\n",
        "    parser.add_argument(\"--eval_model_dir\", type=str, default='', \n",
        "                        help=\"Model directory for evaluation.\")\n",
        "    parser.add_argument(\"--no_cuda\", action='store_true', help=\"Avoid using CUDA.\")\n",
        "    parser.add_argument('--seed', type=int, default=88, help=\"random seed for initialization.\")\n",
        "    args = parser.parse_args(\"\")\n",
        "    print(args)\n",
        "\n",
        "    global logger\n",
        "    mkdir(args.output_dir)\n",
        "    logger = setup_logger(\"vlpretrain\", args.output_dir, 0)\n",
        "\n",
        "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "    set_seed(args.seed, args.n_gpu)\n",
        "    logger.warning(\"Device: %s, n_gpu: %s\", args.device, args.n_gpu)\n",
        "    logger.info('output_mode: {}, #Labels: {}'.format(args.output_mode, args.num_labels))\n",
        " \n",
        "    config_class, tokenizer_class = BertConfig, BertTokenizer\n",
        "    model_class = ImageBertForSequenceClassification\n",
        "    if args.do_train:\n",
        "        config = config_class.from_pretrained(args.config_name if args.config_name else \\\n",
        "            args.model_name_or_path, num_labels=args.num_labels, finetuning_task='ir')\n",
        "        print(config)\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name \\\n",
        "            else args.model_name_or_path, do_lower_case=args.do_lower_case)\n",
        "        config.img_feature_dim = args.img_feature_dim\n",
        "        config.img_feature_type = args.img_feature_type\n",
        "        config.hidden_dropout_prob = args.drop_out\n",
        "        config.loss_type = args.loss_type\n",
        "        config.img_layer_norm_eps = args.img_layer_norm_eps\n",
        "        config.use_img_layernorm = args.use_img_layernorm\n",
        "        model = model_class.from_pretrained(args.model_name_or_path, \n",
        "            from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n",
        "    else:\n",
        "        checkpoint = args.eval_model_dir\n",
        "        assert op.isdir(checkpoint)\n",
        "        config = config_class.from_pretrained(checkpoint)\n",
        "        tokenizer = tokenizer_class.from_pretrained(checkpoint)\n",
        "        logger.info(\"Evaluate the following checkpoint: %s\", checkpoint)\n",
        "        model = model_class.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "    model.to(args.device)\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "    if args.do_train:\n",
        "        train_dataset = RetrievalDataset(tokenizer, args, 'train', is_train=True)\n",
        "        if args.evaluate_during_training:\n",
        "            val_dataset = RetrievalDataset(tokenizer, args, 'minival', is_train=False)\n",
        "        else:\n",
        "            val_dataset = None\n",
        "        global_step, avg_loss = train(args, train_dataset, val_dataset, model, tokenizer)\n",
        "        logger.info(\"Training done: total_step = %s, avg loss = %s\", global_step, avg_loss)\n",
        "\n",
        "    # inference and evaluation\n",
        "    if args.do_test or args.do_eval:\n",
        "        args = restore_training_settings(args)\n",
        "        test_dataset = RetrievalDataset(tokenizer, args, args.test_split, is_train=False)\n",
        "        checkpoint = args.eval_model_dir\n",
        "        assert op.isdir(checkpoint)\n",
        "        logger.info(\"Evaluate the following checkpoint: %s\", checkpoint)\n",
        "        model = model_class.from_pretrained(checkpoint, config=config)\n",
        "        model.to(args.device)\n",
        "        if args.n_gpu > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "\n",
        "        pred_file = get_predict_file(args)\n",
        "        if op.isfile(pred_file):\n",
        "            logger.info(\"Prediction file exist, skip inference.\")\n",
        "            if args.do_eval:\n",
        "                test_result = torch.load(pred_file)\n",
        "        else:\n",
        "            test_result = test(args, model, test_dataset)\n",
        "            torch.save(test_result, pred_file)\n",
        "            logger.info(\"Prediction results saved to {}.\".format(pred_file))\n",
        "\n",
        "        if args.do_eval:\n",
        "            eval_result = evaluate(test_dataset, test_result)\n",
        "            result_file = op.splitext(pred_file)[0] + '.eval.json'\n",
        "            with open(result_file, 'w') as f:\n",
        "                json.dump(eval_result, f)\n",
        "            logger.info(\"Evaluation results saved to {}.\".format(result_file))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j2ZpsemxBVya",
        "outputId": "03f64d4d-52e7-4638-ae34-43db7cd9134d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(adam_epsilon=1e-08, add_od_labels=True, att_mask_type='CLR', config_name='', cross_image_eval=False, data_dir='/content/drive/MyDrive/OscarPlus/datasets/retrieval/coco_ir', do_eval=False, do_lower_case=True, do_test=False, do_train=True, drop_out=0.1, eval_caption_index_file='', eval_img_keys_file='', eval_model_dir='', evaluate_during_training=True, gradient_accumulation_steps=1, img_feat_file='/content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.tsv', img_feature_dim=2054, img_feature_type='frcnn', img_layer_norm_eps=1e-12, learning_rate=2e-05, logging_steps=20, loss_type='sfmx', max_grad_norm=1.0, max_img_seq_length=70, max_seq_length=70, max_steps=-1, model_name_or_path='/content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2050000-backup', no_cuda=False, num_captions_per_img_train=5, num_captions_per_img_val=20, num_labels=2, num_train_epochs=30, num_workers=4, od_label_type='vg', output_dir='/content/drive/MyDrive/OscarPlus/output/retrieval', output_mode='classification', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=100, scheduler='linear', seed=88, test_split='test', tokenizer_name='', use_img_layernorm=1, warmup_steps=0, weight_decay=0.05)\n",
            "2021-12-11 06:27:43,696 vlpretrain WARNING: Device: cuda, n_gpu: 1\n",
            "2021-12-11 06:27:43,696 vlpretrain WARNING: Device: cuda, n_gpu: 1\n",
            "2021-12-11 06:27:43,696 vlpretrain WARNING: Device: cuda, n_gpu: 1\n",
            "2021-12-11 06:27:43,706 vlpretrain INFO: output_mode: classification, #Labels: 2\n",
            "2021-12-11 06:27:43,706 vlpretrain INFO: output_mode: classification, #Labels: 2\n",
            "2021-12-11 06:27:43,706 vlpretrain INFO: output_mode: classification, #Labels: 2\n",
            "{\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"ir\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"img_feature_dim\": 2054,\n",
            "  \"img_feature_type\": \"faster_r-cnn\",\n",
            "  \"img_layer_norm_eps\": 1e-12,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_bert\": true,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_contrast_classes\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_img_layernorm\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "2021-12-11 06:27:48,614 vlpretrain INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, add_od_labels=True, att_mask_type='CLR', config_name='', cross_image_eval=False, data_dir='/content/drive/MyDrive/OscarPlus/datasets/retrieval/coco_ir', device=device(type='cuda'), do_eval=False, do_lower_case=True, do_test=False, do_train=True, drop_out=0.1, eval_caption_index_file='', eval_img_keys_file='', eval_model_dir='', evaluate_during_training=True, gradient_accumulation_steps=1, img_feat_file='/content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.tsv', img_feature_dim=2054, img_feature_type='frcnn', img_layer_norm_eps=1e-12, learning_rate=2e-05, logging_steps=20, loss_type='sfmx', max_grad_norm=1.0, max_img_seq_length=70, max_seq_length=70, max_steps=-1, model_name_or_path='/content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2050000-backup', n_gpu=1, no_cuda=False, num_captions_per_img_train=5, num_captions_per_img_val=20, num_labels=2, num_train_epochs=30, num_workers=4, od_label_type='vg', output_dir='/content/drive/MyDrive/OscarPlus/output/retrieval', output_mode='classification', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=100, scheduler='linear', seed=88, test_split='test', tokenizer_name='', use_img_layernorm=1, warmup_steps=0, weight_decay=0.05)\n",
            "2021-12-11 06:27:48,614 vlpretrain INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, add_od_labels=True, att_mask_type='CLR', config_name='', cross_image_eval=False, data_dir='/content/drive/MyDrive/OscarPlus/datasets/retrieval/coco_ir', device=device(type='cuda'), do_eval=False, do_lower_case=True, do_test=False, do_train=True, drop_out=0.1, eval_caption_index_file='', eval_img_keys_file='', eval_model_dir='', evaluate_during_training=True, gradient_accumulation_steps=1, img_feat_file='/content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.tsv', img_feature_dim=2054, img_feature_type='frcnn', img_layer_norm_eps=1e-12, learning_rate=2e-05, logging_steps=20, loss_type='sfmx', max_grad_norm=1.0, max_img_seq_length=70, max_seq_length=70, max_steps=-1, model_name_or_path='/content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2050000-backup', n_gpu=1, no_cuda=False, num_captions_per_img_train=5, num_captions_per_img_val=20, num_labels=2, num_train_epochs=30, num_workers=4, od_label_type='vg', output_dir='/content/drive/MyDrive/OscarPlus/output/retrieval', output_mode='classification', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=100, scheduler='linear', seed=88, test_split='test', tokenizer_name='', use_img_layernorm=1, warmup_steps=0, weight_decay=0.05)\n",
            "2021-12-11 06:27:48,614 vlpretrain INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, add_od_labels=True, att_mask_type='CLR', config_name='', cross_image_eval=False, data_dir='/content/drive/MyDrive/OscarPlus/datasets/retrieval/coco_ir', device=device(type='cuda'), do_eval=False, do_lower_case=True, do_test=False, do_train=True, drop_out=0.1, eval_caption_index_file='', eval_img_keys_file='', eval_model_dir='', evaluate_during_training=True, gradient_accumulation_steps=1, img_feat_file='/content/drive/MyDrive/OscarPlus/datasets/pretrain/model_0060000/features.tsv', img_feature_dim=2054, img_feature_type='frcnn', img_layer_norm_eps=1e-12, learning_rate=2e-05, logging_steps=20, loss_type='sfmx', max_grad_norm=1.0, max_img_seq_length=70, max_seq_length=70, max_steps=-1, model_name_or_path='/content/drive/MyDrive/OscarPlus/output/pretrain/test/checkpoint-2050000-backup', n_gpu=1, no_cuda=False, num_captions_per_img_train=5, num_captions_per_img_val=20, num_labels=2, num_train_epochs=30, num_workers=4, od_label_type='vg', output_dir='/content/drive/MyDrive/OscarPlus/output/retrieval', output_mode='classification', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=100, scheduler='linear', seed=88, test_split='test', tokenizer_name='', use_img_layernorm=1, warmup_steps=0, weight_decay=0.05)\n",
            "2021-12-11 06:30:28,493 vlpretrain INFO: ***** Running training *****\n",
            "2021-12-11 06:30:28,493 vlpretrain INFO: ***** Running training *****\n",
            "2021-12-11 06:30:28,493 vlpretrain INFO: ***** Running training *****\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vlpretrain:***** Running training *****\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-11 06:30:28,506 vlpretrain INFO:   Num examples = 566435\n",
            "2021-12-11 06:30:28,506 vlpretrain INFO:   Num examples = 566435\n",
            "2021-12-11 06:30:28,506 vlpretrain INFO:   Num examples = 566435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vlpretrain:  Num examples = 566435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-11 06:30:28,521 vlpretrain INFO:   Num Epochs = 30\n",
            "2021-12-11 06:30:28,521 vlpretrain INFO:   Num Epochs = 30\n",
            "2021-12-11 06:30:28,521 vlpretrain INFO:   Num Epochs = 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vlpretrain:  Num Epochs = 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-11 06:30:28,534 vlpretrain INFO:   Batch size per GPU = 64\n",
            "2021-12-11 06:30:28,534 vlpretrain INFO:   Batch size per GPU = 64\n",
            "2021-12-11 06:30:28,534 vlpretrain INFO:   Batch size per GPU = 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vlpretrain:  Batch size per GPU = 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-11 06:30:28,546 vlpretrain INFO:   Total train batch size (w. parallel, & accumulation) = 64\n",
            "2021-12-11 06:30:28,546 vlpretrain INFO:   Total train batch size (w. parallel, & accumulation) = 64\n",
            "2021-12-11 06:30:28,546 vlpretrain INFO:   Total train batch size (w. parallel, & accumulation) = 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vlpretrain:  Total train batch size (w. parallel, & accumulation) = 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-11 06:30:28,559 vlpretrain INFO:   Gradient Accumulation steps = 1\n",
            "2021-12-11 06:30:28,559 vlpretrain INFO:   Gradient Accumulation steps = 1\n",
            "2021-12-11 06:30:28,559 vlpretrain INFO:   Gradient Accumulation steps = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vlpretrain:  Gradient Accumulation steps = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-11 06:30:28,572 vlpretrain INFO:   Total optimization steps = 265530\n",
            "2021-12-11 06:30:28,572 vlpretrain INFO:   Total optimization steps = 265530\n",
            "2021-12-11 06:30:28,572 vlpretrain INFO:   Total optimization steps = 265530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vlpretrain:  Total optimization steps = 265530\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:241: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:241: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:241: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:241: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-dd7c6330ecff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-dd7c6330ecff>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training done: total_step = %s, avg loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-dd7c6330ecff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataset, val_dataset, model, tokenizer)\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0;34m'labels'\u001b[0m\u001b[0;34m:\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             }\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/OscarPlus/oscar/modeling/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, position_ids, head_mask, img_feats)\u001b[0m\n\u001b[1;32m    326\u001b[0m             position_ids=None, head_mask=None, img_feats=None):\n\u001b[1;32m    327\u001b[0m         outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n\u001b[0;32m--> 328\u001b[0;31m                             attention_mask=attention_mask, head_mask=head_mask, img_feats=img_feats)\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/OscarPlus/oscar/modeling/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, head_mask, img_feats, encoder_history_states)\u001b[0m\n\u001b[1;32m    271\u001b[0m         encoder_outputs = self.encoder(embedding_output,\n\u001b[1;32m    272\u001b[0m                 \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                 encoder_history_states=encoder_history_states)\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/OscarPlus/oscar/modeling/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_history_states)\u001b[0m\n\u001b[1;32m    109\u001b[0m             layer_outputs = layer_module(\n\u001b[1;32m    110\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     history_state)\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/OscarPlus/oscar/modeling/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, history_state)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 history_state=None):\n\u001b[1;32m    141\u001b[0m         attention_outputs = self.attention(hidden_states, attention_mask,\n\u001b[0;32m--> 142\u001b[0;31m                 head_mask, history_state)\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/OscarPlus/oscar/modeling/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask, head_mask, history_state)\u001b[0m\n\u001b[1;32m     82\u001b[0m     def forward(self, input_tensor, attention_mask, head_mask=None,\n\u001b[1;32m     83\u001b[0m             history_state=None):\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/OscarPlus/oscar/modeling/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, history_state)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 15.90 GiB total capacity; 14.61 GiB already allocated; 39.75 MiB free; 14.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "oscar_retrieval.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}